#!/usr/bin/env python3
"""
çµ±åˆä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
åˆ†æ•£ã—ãŸä¸¦åˆ—å‡¦ç†è¨­å®šã‚’çµ±åˆã—ã€ä¸€å…ƒç®¡ç†ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
import time
import logging
import threading
import multiprocessing as mp
from typing import Dict, Any, List, Callable, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass
import yaml
import psutil
from contextlib import contextmanager
from functools import wraps
import queue

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™"""

    cpu_usage: float
    memory_usage: float
    active_threads: int
    active_processes: int
    timestamp: float
    execution_time: float = 0.0


@dataclass
class TaskConfig:
    """ã‚¿ã‚¹ã‚¯è¨­å®š"""

    task_type: str  # "cpu_intensive", "io_intensive", "mixed"
    max_workers: int
    timeout: Optional[int] = None
    priority: int = 1  # 1=é«˜, 2=ä¸­, 3=ä½


class UnifiedParallelProcessingSystem:
    """çµ±åˆä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config_path: str = "config_final.yaml"):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.max_workers = self._get_max_workers()
        self.current_workers = self.max_workers

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.performance_history: List[PerformanceMetrics] = []
        self.lock = threading.Lock()
        self.monitoring_active = False
        self.monitor_thread = None

        # å‹•çš„èª¿æ•´è¨­å®š
        self.auto_adjust = self.config.get("performance", {}).get("auto_adjust", True)
        self.adjustment_interval = self.config.get("performance", {}).get(
            "adjustment_interval", 30
        )
        self.min_workers = 1
        self.max_workers_limit = min(self.max_workers * 2, mp.cpu_count() * 2)

        # ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼
        self.task_queue = queue.PriorityQueue()
        self.worker_threads = []

        # çµ±è¨ˆæƒ…å ±
        self.total_tasks_executed = 0
        self.total_execution_time = 0.0
        self.successful_tasks = 0
        self.failed_tasks = 0

        logger.info("ğŸš€ çµ±åˆä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   - æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        logger.info(f"   - å‹•çš„èª¿æ•´: {'æœ‰åŠ¹' if self.auto_adjust else 'ç„¡åŠ¹'}")
        logger.info(f"   - CPUæ•°: {mp.cpu_count()}")

    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _get_max_workers(self) -> int:
        """è¨­å®šã‹ã‚‰æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—"""
        try:
            # ç’°å¢ƒåˆ¥è¨­å®šã‚’å„ªå…ˆ
            environment = self.config.get("system", {}).get("environment", "production")
            env_config = self.config.get("environments", {}).get(environment, {})

            if (
                "performance" in env_config
                and "max_workers" in env_config["performance"]
            ):
                return env_config["performance"]["max_workers"]

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return self.config.get("performance", {}).get("max_workers", 4)
        except Exception as e:
            logger.warning(f"max_workersè¨­å®šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return min(4, mp.cpu_count())

    def get_performance_metrics(self) -> PerformanceMetrics:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’å–å¾—"""
        return PerformanceMetrics(
            cpu_usage=psutil.cpu_percent(interval=1),
            memory_usage=psutil.virtual_memory().percent,
            active_threads=threading.active_count(),
            active_processes=len(psutil.pids()),
            timestamp=time.time(),
        )

    def should_adjust_workers(self) -> bool:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’èª¿æ•´ã™ã¹ãã‹åˆ¤å®š"""
        if not self.auto_adjust:
            return False

        with self.lock:
            if len(self.performance_history) < 5:
                return False

            recent_metrics = self.performance_history[-5:]
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(
                recent_metrics
            )

            # CPUä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™
            if avg_cpu > 80 and self.current_workers > self.min_workers:
                return True

            # CPUä½¿ç”¨ç‡ãŒä½ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å¢—ã‚„ã™
            if avg_cpu < 30 and self.current_workers < self.max_workers_limit:
                return True

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™
            if avg_memory > 85 and self.current_workers > self.min_workers:
                return True

        return False

    def adjust_workers(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å‹•çš„èª¿æ•´"""
        if not self.should_adjust_workers():
            return

        with self.lock:
            recent_metrics = self.performance_history[-5:]
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(
                recent_metrics
            )

            old_workers = self.current_workers

            # CPUä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if avg_cpu > 80:
                self.current_workers = max(self.min_workers, self.current_workers - 1)
            elif avg_cpu < 30:
                self.current_workers = min(
                    self.max_workers_limit, self.current_workers + 1
                )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if avg_memory > 85:
                self.current_workers = max(self.min_workers, self.current_workers - 1)

            if old_workers != self.current_workers:
                logger.info(
                    f"ğŸ”„ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´: {old_workers} â†’ {self.current_workers}"
                )
                logger.info(f"   - CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
                logger.info(f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {avg_memory:.1f}%")

    def start_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹"""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_performance, daemon=True
        )
        self.monitor_thread.start()
        logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹")

    def stop_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–åœæ­¢")

    def _monitor_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring_active:
            try:
                metrics = self.get_performance_metrics()

                with self.lock:
                    self.performance_history.append(metrics)
                    # å±¥æ­´ã‚’æœ€æ–°20ä»¶ã«åˆ¶é™
                    if len(self.performance_history) > 20:
                        self.performance_history = self.performance_history[-20:]

                # å‹•çš„èª¿æ•´
                self.adjust_workers()

                time.sleep(self.adjustment_interval)

            except Exception as e:
                logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(5)

    def get_optimal_executor(self, task_type: str = "mixed") -> tuple:
        """
        ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©ãªExecutorã‚’å–å¾—

        Args:
            task_type: "cpu_intensive", "io_intensive", "mixed"

        Returns:
            (Executor, max_workers)
        """
        with self.lock:
            current_workers = self.current_workers

        if task_type == "cpu_intensive":
            # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã¯ProcessPoolExecutor
            return ProcessPoolExecutor, min(current_workers, mp.cpu_count())
        elif task_type == "io_intensive":
            # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã¯ThreadPoolExecutor
            return ThreadPoolExecutor, current_workers * 2
        else:  # mixed
            # æ··åˆã‚¿ã‚¹ã‚¯ã¯ThreadPoolExecutor
            return ThreadPoolExecutor, current_workers

    def execute_parallel(
        self,
        tasks: List[Callable],
        task_type: str = "mixed",
        timeout: Optional[int] = None,
        priority: int = 1,
    ) -> List[Any]:
        """
        ä¸¦åˆ—å®Ÿè¡Œ

        Args:
            tasks: å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            priority: å„ªå…ˆåº¦ï¼ˆ1=é«˜, 2=ä¸­, 3=ä½ï¼‰

        Returns:
            å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not tasks:
            return []

        executor_class, max_workers = self.get_optimal_executor(task_type)

        logger.info(f"ğŸš€ ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")
        logger.info(f"   - ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        logger.info(f"   - Executor: {executor_class.__name__}")
        logger.info(f"   - å„ªå…ˆåº¦: {priority}")

        start_time = time.time()
        results = []

        try:
            with executor_class(max_workers=max_workers) as executor:
                # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
                future_to_task = {
                    executor.submit(task): i for i, task in enumerate(tasks)
                }

                # çµæœã‚’åé›†
                for future in as_completed(future_to_task, timeout=timeout):
                    try:
                        result = future.result()
                        results.append((future_to_task[future], result))
                        self.successful_tasks += 1
                    except Exception as e:
                        task_index = future_to_task[future]
                        logger.error(f"ã‚¿ã‚¹ã‚¯ {task_index} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                        results.append((task_index, None))
                        self.failed_tasks += 1

        except Exception as e:
            logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

        # çµæœã‚’å…ƒã®é †åºã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[0])
        final_results = [result for _, result in results]

        execution_time = time.time() - start_time
        self.total_tasks_executed += len(tasks)
        self.total_execution_time += execution_time

        logger.info(f"âœ… ä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
        logger.info(f"   - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        logger.info(
            f"   - æˆåŠŸç‡: {len([r for r in final_results if r is not None])}/{len(tasks)}"
        )

        return final_results

    def parallel_map(
        self,
        func: Callable,
        iterable: List[Any],
        task_type: str = "mixed",
        chunk_size: Optional[int] = None,
        priority: int = 1,
    ) -> List[Any]:
        """
        ä¸¦åˆ—ãƒãƒƒãƒ—å®Ÿè¡Œ

        Args:
            func: å®Ÿè¡Œã™ã‚‹é–¢æ•°
            iterable: ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            priority: å„ªå…ˆåº¦

        Returns:
            å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not iterable:
            return []

        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®è‡ªå‹•è¨ˆç®—
        if chunk_size is None:
            executor_class, max_workers = self.get_optimal_executor(task_type)
            chunk_size = max(1, len(iterable) // max_workers)

        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [
            iterable[i : i + chunk_size] for i in range(0, len(iterable), chunk_size)
        ]

        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°
        def process_chunk(chunk):
            return [func(item) for item in chunk]

        # ä¸¦åˆ—å®Ÿè¡Œ
        chunk_results = self.execute_parallel(
            [lambda c=chunk: process_chunk(c) for chunk in chunks],
            task_type,
            priority=priority,
        )

        # çµæœã‚’å¹³å¦åŒ–
        results = []
        for chunk_result in chunk_results:
            if chunk_result is not None:
                results.extend(chunk_result)

        return results

    def get_system_stats(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆã‚’å–å¾—"""
        with self.lock:
            recent_metrics = (
                self.performance_history[-5:] if self.performance_history else []
            )

            stats = {
                "current_workers": self.current_workers,
                "max_workers": self.max_workers,
                "auto_adjust": self.auto_adjust,
                "monitoring_active": self.monitoring_active,
                "total_tasks_executed": self.total_tasks_executed,
                "successful_tasks": self.successful_tasks,
                "failed_tasks": self.failed_tasks,
                "total_execution_time": self.total_execution_time,
                "avg_execution_time": self.total_execution_time
                / max(1, self.total_tasks_executed),
                "success_rate": self.successful_tasks
                / max(1, self.total_tasks_executed)
                * 100,
            }

            if recent_metrics:
                stats.update(
                    {
                        "avg_cpu_usage": sum(m.cpu_usage for m in recent_metrics)
                        / len(recent_metrics),
                        "avg_memory_usage": sum(m.memory_usage for m in recent_metrics)
                        / len(recent_metrics),
                        "performance_history_count": len(self.performance_history),
                    }
                )

            return stats

    def set_workers(self, workers: int):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ‰‹å‹•è¨­å®š"""
        with self.lock:
            old_workers = self.current_workers
            self.current_workers = max(1, min(workers, self.max_workers_limit))
            logger.info(
                f"ğŸ”§ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æ‰‹å‹•è¨­å®š: {old_workers} â†’ {self.current_workers}"
            )

    def enable_auto_adjust(self, enabled: bool = True):
        """è‡ªå‹•èª¿æ•´ã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’è¨­å®š"""
        self.auto_adjust = enabled
        logger.info(f"ğŸ”§ è‡ªå‹•èª¿æ•´: {'æœ‰åŠ¹' if enabled else 'ç„¡åŠ¹'}")

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.stop_monitoring()
        logger.info("ğŸ§¹ çµ±åˆä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_system = None


def get_unified_system() -> UnifiedParallelProcessingSystem:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _global_system
    if _global_system is None:
        _global_system = UnifiedParallelProcessingSystem()
        _global_system.start_monitoring()
    return _global_system


def parallel_execute_unified(
    tasks: List[Callable],
    task_type: str = "mixed",
    timeout: Optional[int] = None,
    priority: int = 1,
) -> List[Any]:
    """çµ±åˆä¸¦åˆ—å®Ÿè¡Œ"""
    system = get_unified_system()
    return system.execute_parallel(tasks, task_type, timeout, priority)


def parallel_map_unified(
    func: Callable,
    iterable: List[Any],
    task_type: str = "mixed",
    chunk_size: Optional[int] = None,
    priority: int = 1,
) -> List[Any]:
    """çµ±åˆä¸¦åˆ—ãƒãƒƒãƒ—"""
    system = get_unified_system()
    return system.parallel_map(func, iterable, task_type, chunk_size, priority)


def parallel_optimized_decorator(task_type: str = "mixed", priority: int = 1):
    """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            system = get_unified_system()
            return system.execute_parallel(
                [lambda: func(*args, **kwargs)], task_type, priority=priority
            )[0]

        return wrapper

    return decorator


@contextmanager
def parallel_context(task_type: str = "mixed", max_workers: Optional[int] = None):
    """ä¸¦åˆ—å‡¦ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    system = get_unified_system()

    if max_workers:
        old_workers = system.current_workers
        system.set_workers(max_workers)

    try:
        yield system
    finally:
        if max_workers:
            system.set_workers(old_workers)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    system = get_unified_system()

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
    def test_task(x):
        time.sleep(0.1)
        return x * 2

    # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
    tasks = [lambda x=i: test_task(x) for i in range(10)]
    results = parallel_execute_unified(tasks, task_type="cpu_intensive")
    print(f"çµæœ: {results}")

    # ä¸¦åˆ—ãƒãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
    data = list(range(20))
    results = parallel_map_unified(test_task, data, task_type="mixed")
    print(f"ãƒãƒƒãƒ—çµæœ: {results}")

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    stats = system.get_system_stats()
    print(f"ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ: {stats}")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    system.cleanup()
