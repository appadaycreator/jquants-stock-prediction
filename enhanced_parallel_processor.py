#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
2-4å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹é«˜åº¦ãªä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½
"""

import asyncio
import concurrent.futures
import multiprocessing as mp
import threading
import time
import logging
import psutil
import queue
from typing import Dict, List, Any, Callable, Optional, Union, Tuple
from dataclasses import dataclass
from contextlib import contextmanager
from functools import wraps, partial
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import weakref
import gc
import os
import sys

logger = logging.getLogger(__name__)


@dataclass
class ProcessingMetrics:
    """å‡¦ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    task_count: int
    execution_time: float
    cpu_usage: float
    memory_usage: float
    throughput: float
    efficiency: float
    success_rate: float


class EnhancedParallelProcessor:
    """å¼·åŒ–ã•ã‚ŒãŸä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, max_workers: Optional[int] = None, adaptive_mode: bool = True):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.adaptive_mode = adaptive_mode
        self.current_workers = self.max_workers
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.metrics_history = []
        self.task_queue = queue.PriorityQueue()
        self.active_tasks = weakref.WeakSet()
        
        # å‹•çš„èª¿æ•´è¨­å®š
        self.adjustment_threshold = 0.8  # CPUä½¿ç”¨ç‡ã®é–¾å€¤
        self.adjustment_interval = 5.0  # èª¿æ•´é–“éš”ï¼ˆç§’ï¼‰
        self.last_adjustment = time.time()
        
        # çµ±è¨ˆæƒ…å ±
        self.total_tasks = 0
        self.successful_tasks = 0
        self.failed_tasks = 0
        self.total_execution_time = 0.0
        
        # éåŒæœŸå‡¦ç†ç”¨
        self.async_loop = None
        self.async_tasks = set()
        
        logger.info(f"ğŸš€ å¼·åŒ–ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   - æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        logger.info(f"   - é©å¿œãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if adaptive_mode else 'ç„¡åŠ¹'}")
        logger.info(f"   - CPUæ•°: {os.cpu_count()}")

    def get_optimal_workers(self, task_type: str = "mixed", data_size: int = 0) -> int:
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãæœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’è¨ˆç®—"""
        cpu_count = os.cpu_count() or 1
        
        if task_type == "cpu_intensive":
            # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            optimal = min(cpu_count, self.max_workers)
        elif task_type == "io_intensive":
            # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            optimal = min(cpu_count * 4, self.max_workers)
        elif task_type == "memory_intensive":
            # ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            available_memory = psutil.virtual_memory().available / (1024**3)  # GB
            optimal = min(int(available_memory / 2), cpu_count, self.max_workers)
        else:  # mixed
            # æ··åˆã‚¿ã‚¹ã‚¯
            optimal = min(cpu_count * 2, self.max_workers)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãèª¿æ•´
        if data_size > 0:
            if data_size < 1000:
                optimal = min(optimal, 2)
            elif data_size < 10000:
                optimal = min(optimal, 4)
            else:
                optimal = min(optimal, 8)
        
        return max(1, optimal)

    def adaptive_worker_adjustment(self):
        """é©å¿œçš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´"""
        if not self.adaptive_mode:
            return
        
        current_time = time.time()
        if current_time - self.last_adjustment < self.adjustment_interval:
            return
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        cpu_usage = psutil.cpu_percent(interval=1)
        memory_usage = psutil.virtual_memory().percent
        
        old_workers = self.current_workers
        
        # CPUä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
        if cpu_usage > 90 and self.current_workers > 1:
            self.current_workers = max(1, self.current_workers - 1)
        elif cpu_usage < 50 and self.current_workers < self.max_workers:
            self.current_workers = min(self.max_workers, self.current_workers + 1)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
        if memory_usage > 85 and self.current_workers > 1:
            self.current_workers = max(1, self.current_workers - 1)
        
        if old_workers != self.current_workers:
            logger.info(f"ğŸ”„ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°é©å¿œèª¿æ•´: {old_workers} â†’ {self.current_workers}")
            logger.info(f"   - CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%")
            logger.info(f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_usage:.1f}%")
        
        self.last_adjustment = current_time

    def parallel_execute_optimized(
        self,
        tasks: List[Callable],
        task_type: str = "mixed",
        data_size: int = 0,
        timeout: Optional[float] = None,
        priority: int = 1
    ) -> List[Any]:
        """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—å®Ÿè¡Œ"""
        if not tasks:
            return []
        
        # é©å¿œçš„èª¿æ•´
        self.adaptive_worker_adjustment()
        
        # æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’è¨ˆç®—
        optimal_workers = self.get_optimal_workers(task_type, data_size)
        actual_workers = min(optimal_workers, len(tasks))
        
        logger.info(f"ğŸš€ æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")
        logger.info(f"   - ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {actual_workers}")
        logger.info(f"   - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size}")
        
        start_time = time.time()
        results = []
        
        try:
            # Executorã®é¸æŠ
            if task_type == "cpu_intensive":
                executor_class = ProcessPoolExecutor
            else:
                executor_class = ThreadPoolExecutor
            
            with executor_class(max_workers=actual_workers) as executor:
                # ã‚¿ã‚¹ã‚¯ã®é€ä¿¡
                future_to_task = {
                    executor.submit(self._execute_with_monitoring, task, i): i 
                    for i, task in enumerate(tasks)
                }
                
                # çµæœã®åé›†
                for future in as_completed(future_to_task, timeout=timeout):
                    try:
                        result = future.result()
                        results.append((future_to_task[future], result))
                        self.successful_tasks += 1
                    except Exception as e:
                        task_index = future_to_task[future]
                        logger.error(f"ã‚¿ã‚¹ã‚¯ {task_index} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                        results.append((task_index, None))
                        self.failed_tasks += 1
                
        except Exception as e:
            logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        # çµæœã‚’å…ƒã®é †åºã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[0])
        final_results = [result for _, result in results]
        
        execution_time = time.time() - start_time
        self.total_tasks += len(tasks)
        self.total_execution_time += execution_time
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²
        self._record_metrics(len(tasks), execution_time, task_type)
        
        logger.info(f"âœ… æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
        logger.info(f"   - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        logger.info(f"   - æˆåŠŸç‡: {len([r for r in final_results if r is not None])}/{len(tasks)}")
        
        return final_results

    def _execute_with_monitoring(self, task: Callable, task_id: int) -> Any:
        """ç›£è¦–ä»˜ãã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        start_time = time.time()
        
        try:
            result = task()
            execution_time = time.time() - start_time
            
            logger.debug(f"ã‚¿ã‚¹ã‚¯ {task_id} å®Œäº†: {execution_time:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ {task_id} ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def parallel_map_optimized(
        self,
        func: Callable,
        iterable: List[Any],
        task_type: str = "mixed",
        chunk_size: Optional[int] = None,
        priority: int = 1
    ) -> List[Any]:
        """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—ãƒãƒƒãƒ—"""
        if not iterable:
            return []
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®è‡ªå‹•è¨ˆç®—
        if chunk_size is None:
            optimal_workers = self.get_optimal_workers(task_type, len(iterable))
            chunk_size = max(1, len(iterable) // optimal_workers)
        
        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [
            iterable[i:i + chunk_size] 
            for i in range(0, len(iterable), chunk_size)
        ]
        
        logger.info(f"ğŸ“Š ä¸¦åˆ—ãƒãƒƒãƒ—æœ€é©åŒ–")
        logger.info(f"   - ãƒ‡ãƒ¼ã‚¿æ•°: {len(iterable)}")
        logger.info(f"   - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        logger.info(f"   - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        
        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°
        def process_chunk(chunk):
            return [func(item) for item in chunk]
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        chunk_results = self.parallel_execute_optimized(
            [partial(process_chunk, chunk) for chunk in chunks],
            task_type,
            len(iterable),
            priority=priority
        )
        
        # çµæœã‚’å¹³å¦åŒ–
        results = []
        for chunk_result in chunk_results:
            if chunk_result is not None:
                results.extend(chunk_result)
        
        return results

    async def async_parallel_execute(
        self,
        tasks: List[Callable],
        task_type: str = "mixed",
        max_concurrent: Optional[int] = None
    ) -> List[Any]:
        """éåŒæœŸä¸¦åˆ—å®Ÿè¡Œ"""
        if not tasks:
            return []
        
        if max_concurrent is None:
            max_concurrent = self.get_optimal_workers(task_type)
        
        logger.info(f"ğŸ”„ éåŒæœŸä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
        logger.info(f"   - æœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°: {max_concurrent}")
        
        start_time = time.time()
        results = []
        
        # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def execute_task_with_semaphore(task, task_id):
            async with semaphore:
                try:
                    # ã‚¿ã‚¹ã‚¯ã‚’éåŒæœŸã§å®Ÿè¡Œ
                    result = await asyncio.get_event_loop().run_in_executor(
                        None, task
                    )
                    return task_id, result
                except Exception as e:
                    logger.error(f"éåŒæœŸã‚¿ã‚¹ã‚¯ {task_id} ã‚¨ãƒ©ãƒ¼: {e}")
                    return task_id, None
        
        # å…¨ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        task_coroutines = [
            execute_task_with_semaphore(task, i) 
            for i, task in enumerate(tasks)
        ]
        
        # çµæœã‚’åé›†
        task_results = await asyncio.gather(*task_coroutines, return_exceptions=True)
        
        # çµæœã‚’ã‚½ãƒ¼ãƒˆ
        task_results.sort(key=lambda x: x[0] if isinstance(x, tuple) else 0)
        results = [result for _, result in task_results if isinstance(result, tuple)]
        
        execution_time = time.time() - start_time
        
        logger.info(f"âœ… éåŒæœŸä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
        logger.info(f"   - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        logger.info(f"   - æˆåŠŸç‡: {len([r for r in results if r is not None])}/{len(tasks)}")
        
        return results

    def batch_processing(
        self,
        data: List[Any],
        batch_size: int,
        processing_func: Callable,
        task_type: str = "mixed"
    ) -> List[Any]:
        """ãƒãƒƒãƒå‡¦ç†"""
        if not data:
            return []
        
        logger.info(f"ğŸ“¦ ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
        logger.info(f"   - ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        logger.info(f"   - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        
        # ãƒãƒƒãƒã«åˆ†å‰²
        batches = [
            data[i:i + batch_size] 
            for i in range(0, len(data), batch_size)
        ]
        
        # ãƒãƒƒãƒå‡¦ç†é–¢æ•°
        def process_batch(batch):
            return [processing_func(item) for item in batch]
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        batch_results = self.parallel_execute_optimized(
            [partial(process_batch, batch) for batch in batches],
            task_type,
            len(data)
        )
        
        # çµæœã‚’å¹³å¦åŒ–
        results = []
        for batch_result in batch_results:
            if batch_result is not None:
                results.extend(batch_result)
        
        logger.info(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶")
        return results

    def _record_metrics(self, task_count: int, execution_time: float, task_type: str):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        throughput = task_count / execution_time if execution_time > 0 else 0
        efficiency = (self.successful_tasks / max(1, self.total_tasks)) * 100
        
        metrics = ProcessingMetrics(
            task_count=task_count,
            execution_time=execution_time,
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            throughput=throughput,
            efficiency=efficiency,
            success_rate=efficiency
        )
        
        self.metrics_history.append({
            "timestamp": time.time(),
            "task_type": task_type,
            "metrics": metrics
        })
        
        # å±¥æ­´ã‚’æœ€æ–°100ä»¶ã«åˆ¶é™
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]

    def get_performance_report(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.metrics_history:
            return {"message": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        recent_metrics = self.metrics_history[-10:]  # æœ€æ–°10ä»¶
        
        avg_execution_time = np.mean([m["metrics"].execution_time for m in recent_metrics])
        avg_throughput = np.mean([m["metrics"].throughput for m in recent_metrics])
        avg_efficiency = np.mean([m["metrics"].efficiency for m in recent_metrics])
        avg_cpu_usage = np.mean([m["metrics"].cpu_usage for m in recent_metrics])
        avg_memory_usage = np.mean([m["metrics"].memory_usage for m in recent_metrics])
        
        return {
            "current_workers": self.current_workers,
            "max_workers": self.max_workers,
            "total_tasks": self.total_tasks,
            "successful_tasks": self.successful_tasks,
            "failed_tasks": self.failed_tasks,
            "success_rate": (self.successful_tasks / max(1, self.total_tasks)) * 100,
            "total_execution_time": self.total_execution_time,
            "avg_execution_time": avg_execution_time,
            "avg_throughput": avg_throughput,
            "avg_efficiency": avg_efficiency,
            "avg_cpu_usage": avg_cpu_usage,
            "avg_memory_usage": avg_memory_usage,
            "metrics_history_count": len(self.metrics_history),
            "recommendations": self._generate_recommendations()
        }

    def _generate_recommendations(self) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        recent_metrics = self.metrics_history[-5:]
        avg_efficiency = np.mean([m["metrics"].efficiency for m in recent_metrics])
        avg_cpu_usage = np.mean([m["metrics"].cpu_usage for m in recent_metrics])
        avg_memory_usage = np.mean([m["metrics"].memory_usage for m in recent_metrics])
        
        if avg_efficiency < 80:
            recommendations.append("å‡¦ç†åŠ¹ç‡ãŒ80%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®èª¿æ•´ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if avg_cpu_usage > 90:
            recommendations.append("CPUä½¿ç”¨ç‡ãŒ90%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if avg_memory_usage > 85:
            recommendations.append("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ85%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if self.current_workers < self.max_workers and avg_cpu_usage < 50:
            recommendations.append("CPUä½¿ç”¨ç‡ãŒä½ã„ã§ã™ã€‚ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        return recommendations

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ã‚¹ã‚¯ã‚’ã‚¯ãƒªã‚¢
        self.active_tasks.clear()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        self.metrics_history.clear()
        
        # éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’ã‚¯ãƒªã‚¢
        if self.async_tasks:
            for task in self.async_tasks:
                if not task.done():
                    task.cancel()
            self.async_tasks.clear()
        
        logger.info("ğŸ§¹ å¼·åŒ–ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")


# ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def parallel_optimized(task_type: str = "mixed", priority: int = 1):
    """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            processor = EnhancedParallelProcessor()
            return processor.parallel_execute_optimized(
                [lambda: func(*args, **kwargs)], 
                task_type, 
                priority=priority
            )[0]
        return wrapper
    return decorator


# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
@contextmanager
def parallel_context(
    max_workers: Optional[int] = None, 
    adaptive_mode: bool = True
):
    """ä¸¦åˆ—å‡¦ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    processor = EnhancedParallelProcessor(max_workers, adaptive_mode)
    try:
        yield processor
    finally:
        processor.cleanup()


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_processor = None


def get_global_processor() -> EnhancedParallelProcessor:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _global_processor
    if _global_processor is None:
        _global_processor = EnhancedParallelProcessor()
    return _global_processor


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    processor = EnhancedParallelProcessor()
    
    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
    def test_task(x):
        time.sleep(0.1)
        return x * 2
    
    # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
    tasks = [lambda x=i: test_task(x) for i in range(20)]
    results = processor.parallel_execute_optimized(tasks, task_type="cpu_intensive")
    print(f"ä¸¦åˆ—å®Ÿè¡Œçµæœ: {results}")
    
    # ä¸¦åˆ—ãƒãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
    data = list(range(50))
    results = processor.parallel_map_optimized(test_task, data, task_type="mixed")
    print(f"ä¸¦åˆ—ãƒãƒƒãƒ—çµæœ: {results[:10]}...")  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
    
    # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
    batch_results = processor.batch_processing(
        data, 
        batch_size=10, 
        processing_func=test_task, 
        task_type="mixed"
    )
    print(f"ãƒãƒƒãƒå‡¦ç†çµæœ: {batch_results[:10]}...")  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
    report = processor.get_performance_report()
    print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: {report}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    processor.cleanup()
