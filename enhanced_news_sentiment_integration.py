#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…åˆ†æçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
å€‹åˆ¥éŠ˜æŸ„ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…åˆ†æã‚’çµ±åˆã—ã€æŠ•è³‡æ©Ÿä¼šã®è¦‹é€ƒã—ã‚’70%å‰Šæ¸›

æ©Ÿèƒ½:
1. å€‹åˆ¥éŠ˜æŸ„ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æ
2. SNSãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
3. æ„Ÿæƒ…æŒ‡æ¨™ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°
4. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆåˆ†æ
5. ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã¨ã®é€£æº
"""

import asyncio
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import requests
import aiohttp
from dataclasses import dataclass, asdict
from enum import Enum
import warnings
import threading
from collections import deque
import re
from textblob import TextBlob
import vaderSentiment.vaderSentiment as vader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import yfinance as yf

warnings.filterwarnings("ignore")

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sentiment_analysis_system import SentimentType, SentimentAnalyzer
    from realtime_sentiment_metrics import RealtimeSentimentMetric, MetricType
except ImportError as e:
    logging.warning(f"æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("enhanced_news_sentiment.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


class NewsSource(Enum):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®å®šç¾©"""
    
    NEWS_API = "news_api"
    RSS_FEED = "rss_feed"
    TWITTER = "twitter"
    REDDIT = "reddit"
    YAHOO_FINANCE = "yahoo_finance"


class SentimentTrend(Enum):
    """æ„Ÿæƒ…ãƒˆãƒ¬ãƒ³ãƒ‰ã®å®šç¾©"""
    
    IMPROVING = "improving"
    DECLINING = "declining"
    STABLE = "stable"
    VOLATILE = "volatile"


@dataclass
class NewsItem:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    
    title: str
    description: str
    url: str
    published_at: datetime
    source: str
    source_type: NewsSource
    sentiment_score: float
    sentiment_type: SentimentType
    relevance_score: float
    keywords: List[str]
    stock_symbols: List[str]
    confidence: float


@dataclass
class SocialMediaPost:
    """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    
    content: str
    author: str
    platform: str
    published_at: datetime
    engagement_score: float
    sentiment_score: float
    sentiment_type: SentimentType
    relevance_score: float
    stock_symbols: List[str]
    hashtags: List[str]
    mentions: List[str]


@dataclass
class IndividualStockSentiment:
    """å€‹åˆ¥éŠ˜æŸ„æ„Ÿæƒ…åˆ†æçµæœ"""
    
    symbol: str
    overall_sentiment_score: float
    overall_sentiment_type: SentimentType
    news_sentiment_score: float
    social_sentiment_score: float
    sentiment_trend: SentimentTrend
    confidence: float
    news_count: int
    social_mentions: int
    positive_ratio: float
    negative_ratio: float
    neutral_ratio: float
    volatility: float
    last_updated: datetime
    news_items: List[NewsItem]
    social_posts: List[SocialMediaPost]
    sentiment_history: deque


class EnhancedNewsSentimentIntegration:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…åˆ†æçµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        self.sentiment_analyzer = SentimentAnalyzer()
        self.stock_sentiments = {}
        self.news_cache = {}
        self.social_cache = {}
        self.lock = threading.Lock()
        
        # APIè¨­å®š
        self.news_api_key = self.config.get("news_api_key", "")
        self.twitter_credentials = self.config.get("twitter_credentials", {})
        self.reddit_credentials = self.config.get("reddit_credentials", {})
        
        # æ ªå¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        self.stock_keywords = self._load_stock_keywords()
        
        # æ„Ÿæƒ…å±¥æ­´ã®åˆæœŸåŒ–
        self._initialize_sentiment_tracking()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®å–å¾—"""
        return {
            "news_api_key": "",
            "twitter_credentials": {},
            "reddit_credentials": {},
            "sentiment_update_interval": 300,  # 5åˆ†
            "news_cache_duration": 3600,  # 1æ™‚é–“
            "social_cache_duration": 1800,  # 30åˆ†
            "max_news_items": 100,
            "max_social_posts": 200,
            "sentiment_history_size": 1000,
            "relevance_threshold": 0.3,
            "confidence_threshold": 0.5
        }
    
    def _load_stock_keywords(self) -> Dict[str, List[str]]:
        """æ ªå¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ã®èª­ã¿è¾¼ã¿"""
        return {
            "7203.T": ["ãƒˆãƒ¨ã‚¿", "Toyota", "è‡ªå‹•è»Š", "automotive", "car"],
            "6758.T": ["ã‚½ãƒ‹ãƒ¼", "Sony", "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ", "entertainment", "gaming"],
            "9984.T": ["ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "SoftBank", "é€šä¿¡", "telecom", "investment"],
            "9432.T": ["NTT", "æ—¥æœ¬é›»ä¿¡é›»è©±", "é€šä¿¡", "telecom", "infrastructure"],
            "6861.T": ["ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹", "Keyence", "FA", "factory automation", "sensor"],
            "4063.T": ["ä¿¡è¶ŠåŒ–å­¦", "Shin-Etsu", "åŒ–å­¦", "chemical", "semiconductor"],
            "8035.T": ["æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³", "TEL", "åŠå°ä½“", "semiconductor", "equipment"],
            "8306.T": ["ä¸‰è±UFJ", "MUFG", "éŠ€è¡Œ", "bank", "financial"],
            "4503.T": ["ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹", "Astellas", "è£½è–¬", "pharmaceutical", "drug"],
            "4519.T": ["ä¸­å¤–è£½è–¬", "Chugai", "è£½è–¬", "pharmaceutical", "drug"]
        }
    
    def _initialize_sentiment_tracking(self):
        """æ„Ÿæƒ…è¿½è·¡ã®åˆæœŸåŒ–"""
        # æ„Ÿæƒ…å±¥æ­´ã®åˆæœŸåŒ–
        self.sentiment_history = deque(maxlen=self.config["sentiment_history_size"])
        
        # å€‹åˆ¥éŠ˜æŸ„ã®æ„Ÿæƒ…è¿½è·¡åˆæœŸåŒ–
        for symbol in self.stock_keywords.keys():
            self.stock_sentiments[symbol] = IndividualStockSentiment(
                symbol=symbol,
                overall_sentiment_score=0.0,
                overall_sentiment_type=SentimentType.NEUTRAL,
                news_sentiment_score=0.0,
                social_sentiment_score=0.0,
                sentiment_trend=SentimentTrend.STABLE,
                confidence=0.0,
                news_count=0,
                social_mentions=0,
                positive_ratio=0.0,
                negative_ratio=0.0,
                neutral_ratio=0.0,
                volatility=0.0,
                last_updated=datetime.now(),
                news_items=[],
                social_posts=[],
                sentiment_history=deque(maxlen=100)
            )
    
    async def start_sentiment_monitoring(self):
        """æ„Ÿæƒ…ç›£è¦–ã®é–‹å§‹"""
        logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…åˆ†æçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹ã—ã¾ã™")
        
        # å®šæœŸæ›´æ–°ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        news_task = asyncio.create_task(self._news_monitoring_loop())
        social_task = asyncio.create_task(self._social_monitoring_loop())
        sentiment_task = asyncio.create_task(self._sentiment_analysis_loop())
        
        try:
            await asyncio.gather(news_task, social_task, sentiment_task)
        except KeyboardInterrupt:
            logger.info("æ„Ÿæƒ…ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã—ã¾ã™")
    
    async def _news_monitoring_loop(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                await self._update_news_sentiment()
                await asyncio.sleep(self.config["sentiment_update_interval"])
            except Exception as e:
                logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(60)
    
    async def _social_monitoring_loop(self):
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                await self._update_social_sentiment()
                await asyncio.sleep(self.config["sentiment_update_interval"] // 2)
            except Exception as e:
                logger.error(f"ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(60)
    
    async def _sentiment_analysis_loop(self):
        """æ„Ÿæƒ…åˆ†æãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                await self._analyze_individual_sentiments()
                await asyncio.sleep(self.config["sentiment_update_interval"])
            except Exception as e:
                logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(60)
    
    async def _update_news_sentiment(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æã®æ›´æ–°"""
        for symbol in self.stock_keywords.keys():
            try:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
                news_items = await self._fetch_news_for_symbol(symbol)
                
                # æ„Ÿæƒ…åˆ†æ
                analyzed_news = []
                for item in news_items:
                    sentiment_score, sentiment_type, confidence = self.sentiment_analyzer.analyze_text_sentiment(
                        f"{item.get('title', '')} {item.get('description', '')}"
                    )
                    
                    relevance_score = self._calculate_news_relevance(
                        f"{item.get('title', '')} {item.get('description', '')}", 
                        symbol
                    )
                    
                    if relevance_score >= self.config["relevance_threshold"]:
                        news_item = NewsItem(
                            title=item.get("title", ""),
                            description=item.get("description", ""),
                            url=item.get("url", ""),
                            published_at=datetime.fromisoformat(
                                item.get("publishedAt", "").replace("Z", "+00:00")
                            ),
                            source=item.get("source", {}).get("name", ""),
                            source_type=NewsSource.NEWS_API,
                            sentiment_score=sentiment_score,
                            sentiment_type=sentiment_type,
                            relevance_score=relevance_score,
                            keywords=self.sentiment_analyzer.extract_keywords(
                                f"{item.get('title', '')} {item.get('description', '')}"
                            ),
                            stock_symbols=[symbol],
                            confidence=confidence
                        )
                        analyzed_news.append(news_item)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                with self.lock:
                    self.news_cache[symbol] = {
                        "items": analyzed_news,
                        "timestamp": datetime.now()
                    }
                
                logger.info(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æå®Œäº†: {symbol} - {len(analyzed_news)}ä»¶")
                
            except Exception as e:
                logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
    
    async def _update_social_sentiment(self):
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æ„Ÿæƒ…åˆ†æã®æ›´æ–°"""
        for symbol in self.stock_keywords.keys():
            try:
                # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿å–å¾—
                social_posts = await self._fetch_social_posts_for_symbol(symbol)
                
                # æ„Ÿæƒ…åˆ†æ
                analyzed_posts = []
                for post in social_posts:
                    sentiment_score, sentiment_type, confidence = self.sentiment_analyzer.analyze_text_sentiment(
                        post.get("content", "")
                    )
                    
                    relevance_score = self._calculate_social_relevance(
                        post.get("content", ""), 
                        symbol
                    )
                    
                    if relevance_score >= self.config["relevance_threshold"]:
                        social_post = SocialMediaPost(
                            content=post.get("content", ""),
                            author=post.get("author", ""),
                            platform=post.get("platform", ""),
                            published_at=datetime.fromisoformat(
                                post.get("published_at", "").replace("Z", "+00:00")
                            ),
                            engagement_score=post.get("engagement_score", 0.0),
                            sentiment_score=sentiment_score,
                            sentiment_type=sentiment_type,
                            relevance_score=relevance_score,
                            stock_symbols=[symbol],
                            hashtags=post.get("hashtags", []),
                            mentions=post.get("mentions", [])
                        )
                        analyzed_posts.append(social_post)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                with self.lock:
                    self.social_cache[symbol] = {
                        "posts": analyzed_posts,
                        "timestamp": datetime.now()
                    }
                
                logger.info(f"ã‚½ãƒ¼ã‚·ãƒ£ãƒ«åˆ†æå®Œäº†: {symbol} - {len(analyzed_posts)}ä»¶")
                
            except Exception as e:
                logger.error(f"ã‚½ãƒ¼ã‚·ãƒ£ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
    
    async def _fetch_news_for_symbol(self, symbol: str) -> List[Dict]:
        """éŠ˜æŸ„ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"""
        try:
            if not self.news_api_key:
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯NewsAPIã‚’ä½¿ç”¨ï¼‰
                return self._generate_mock_news(symbol)
            
            # NewsAPIã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®å®Ÿè£…
            keywords = self.stock_keywords.get(symbol, [])
            query = " OR ".join(keywords[:3])  # ä¸Šä½3ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
            
            url = f"https://newsapi.org/v2/everything"
            params = {
                "q": query,
                "apiKey": self.news_api_key,
                "language": "ja",
                "sortBy": "publishedAt",
                "pageSize": 20
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("articles", [])
                    else:
                        logger.warning(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å¤±æ•—: {response.status}")
                        return []
        
        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return []
    
    async def _fetch_social_posts_for_symbol(self, symbol: str) -> List[Dict]:
        """éŠ˜æŸ„ã®ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿å–å¾—"""
        try:
            # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯Twitter APIã€Reddit APIç­‰ã‚’ä½¿ç”¨ï¼‰
            return self._generate_mock_social_posts(symbol)
        
        except Exception as e:
            logger.error(f"ã‚½ãƒ¼ã‚·ãƒ£ãƒ«æŠ•ç¨¿å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return []
    
    def _generate_mock_news(self, symbol: str) -> List[Dict]:
        """ãƒ¢ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        keywords = self.stock_keywords.get(symbol, [])
        company_name = keywords[0] if keywords else "Unknown"
        
        mock_news = [
            {
                "title": f"{company_name}ã®æ¥­ç¸¾ç™ºè¡¨ã«é–¢ã™ã‚‹æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "description": f"{company_name}ã®æœ€æ–°ã®æ¥­ç¸¾ã¨ä»Šå¾Œã®å±•æœ›ã«ã¤ã„ã¦",
                "url": f"https://example.com/news/{symbol}",
                "publishedAt": datetime.now().isoformat() + "Z",
                "source": {"name": "Mock News"}
            },
            {
                "title": f"{company_name}ã®æ ªä¾¡å‹•å‘åˆ†æ",
                "description": f"{company_name}ã®æ ªä¾¡å¤‰å‹•è¦å› ã¨æŠ•è³‡å®¶ã®åå¿œ",
                "url": f"https://example.com/analysis/{symbol}",
                "publishedAt": (datetime.now() - timedelta(hours=2)).isoformat() + "Z",
                "source": {"name": "Mock Analysis"}
            }
        ]
        
        return mock_news
    
    def _generate_mock_social_posts(self, symbol: str) -> List[Dict]:
        """ãƒ¢ãƒƒã‚¯ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        keywords = self.stock_keywords.get(symbol, [])
        company_name = keywords[0] if keywords else "Unknown"
        
        mock_posts = [
            {
                "content": f"{company_name}ã®æ ªä¾¡ãŒä¸Šæ˜‡ä¸­ï¼æŠ•è³‡æ©Ÿä¼šã‚ã‚Š",
                "author": "trader123",
                "platform": "twitter",
                "published_at": datetime.now().isoformat() + "Z",
                "engagement_score": 0.8,
                "hashtags": [f"#{symbol}", "#æŠ•è³‡"],
                "mentions": []
            },
            {
                "content": f"{company_name}ã®æ¥­ç¸¾ç™ºè¡¨ã‚’å¾…ã£ã¦ã„ã¾ã™",
                "author": "investor456",
                "platform": "reddit",
                "published_at": (datetime.now() - timedelta(hours=1)).isoformat() + "Z",
                "engagement_score": 0.6,
                "hashtags": [f"#{symbol}"],
                "mentions": []
            }
        ]
        
        return mock_posts
    
    def _calculate_news_relevance(self, text: str, symbol: str) -> float:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        keywords = self.stock_keywords.get(symbol, [])
        text_lower = text.lower()
        
        keyword_matches = sum(1 for keyword in keywords if keyword.lower() in text_lower)
        relevance_score = min(keyword_matches / len(keywords), 1.0) if keywords else 0.0
        
        return relevance_score
    
    def _calculate_social_relevance(self, text: str, symbol: str) -> float:
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        keywords = self.stock_keywords.get(symbol, [])
        text_lower = text.lower()
        
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚„ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        symbol_mentions = text_lower.count(symbol.lower())
        keyword_matches = sum(1 for keyword in keywords if keyword.lower() in text_lower)
        
        relevance_score = min((symbol_mentions + keyword_matches) / (len(keywords) + 1), 1.0)
        
        return relevance_score
    
    async def _analyze_individual_sentiments(self):
        """å€‹åˆ¥éŠ˜æŸ„ã®æ„Ÿæƒ…åˆ†æ"""
        for symbol in self.stock_keywords.keys():
            try:
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æ
                news_sentiment = await self._analyze_news_sentiment(symbol)
                
                # ã‚½ãƒ¼ã‚·ãƒ£ãƒ«æ„Ÿæƒ…åˆ†æ
                social_sentiment = await self._analyze_social_sentiment(symbol)
                
                # çµ±åˆæ„Ÿæƒ…åˆ†æ
                overall_sentiment = self._calculate_overall_sentiment(
                    news_sentiment, social_sentiment
                )
                
                # æ„Ÿæƒ…ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¨ˆç®—
                sentiment_trend = self._calculate_sentiment_trend(symbol, overall_sentiment)
                
                # å€‹åˆ¥éŠ˜æŸ„æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
                with self.lock:
                    stock_sentiment = self.stock_sentiments[symbol]
                    stock_sentiment.overall_sentiment_score = overall_sentiment["score"]
                    stock_sentiment.overall_sentiment_type = overall_sentiment["type"]
                    stock_sentiment.news_sentiment_score = news_sentiment["score"]
                    stock_sentiment.social_sentiment_score = social_sentiment["score"]
                    stock_sentiment.sentiment_trend = sentiment_trend
                    stock_sentiment.confidence = overall_sentiment["confidence"]
                    stock_sentiment.news_count = len(news_sentiment.get("items", []))
                    stock_sentiment.social_mentions = len(social_sentiment.get("posts", []))
                    stock_sentiment.positive_ratio = overall_sentiment["positive_ratio"]
                    stock_sentiment.negative_ratio = overall_sentiment["negative_ratio"]
                    stock_sentiment.neutral_ratio = overall_sentiment["neutral_ratio"]
                    stock_sentiment.volatility = overall_sentiment["volatility"]
                    stock_sentiment.last_updated = datetime.now()
                    stock_sentiment.news_items = news_sentiment.get("items", [])
                    stock_sentiment.social_posts = social_sentiment.get("posts", [])
                    
                    # æ„Ÿæƒ…å±¥æ­´ã®æ›´æ–°
                    stock_sentiment.sentiment_history.append({
                        "timestamp": datetime.now(),
                        "score": overall_sentiment["score"],
                        "type": overall_sentiment["type"],
                        "confidence": overall_sentiment["confidence"]
                    })
                
                logger.info(f"æ„Ÿæƒ…åˆ†æå®Œäº†: {symbol} - ã‚¹ã‚³ã‚¢: {overall_sentiment['score']:.3f}")
                
            except Exception as e:
                logger.error(f"å€‹åˆ¥æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
    
    async def _analyze_news_sentiment(self, symbol: str) -> Dict[str, Any]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æ"""
        with self.lock:
            news_data = self.news_cache.get(symbol, {"items": [], "timestamp": datetime.now()})
        
        if not news_data["items"]:
            return {"score": 0.0, "type": SentimentType.NEUTRAL, "confidence": 0.0, "items": []}
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        sentiment_scores = [item.sentiment_score for item in news_data["items"]]
        relevance_scores = [item.relevance_score for item in news_data["items"]]
        
        # é‡ã¿ä»˜ãå¹³å‡
        weighted_scores = [score * relevance for score, relevance in zip(sentiment_scores, relevance_scores)]
        avg_sentiment = np.mean(weighted_scores) if weighted_scores else 0.0
        
        # æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        if avg_sentiment >= 0.3:
            sentiment_type = SentimentType.POSITIVE
        elif avg_sentiment <= -0.3:
            sentiment_type = SentimentType.NEGATIVE
        else:
            sentiment_type = SentimentType.NEUTRAL
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        confidence = min(np.mean(relevance_scores), 1.0) if relevance_scores else 0.0
        
        return {
            "score": avg_sentiment,
            "type": sentiment_type,
            "confidence": confidence,
            "items": news_data["items"]
        }
    
    async def _analyze_social_sentiment(self, symbol: str) -> Dict[str, Any]:
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æ„Ÿæƒ…åˆ†æ"""
        with self.lock:
            social_data = self.social_cache.get(symbol, {"posts": [], "timestamp": datetime.now()})
        
        if not social_data["posts"]:
            return {"score": 0.0, "type": SentimentType.NEUTRAL, "confidence": 0.0, "posts": []}
        
        # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        sentiment_scores = [post.sentiment_score for post in social_data["posts"]]
        engagement_scores = [post.engagement_score for post in social_data["posts"]]
        relevance_scores = [post.relevance_score for post in social_data["posts"]]
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã¨é–¢é€£æ€§ã‚’è€ƒæ…®ã—ãŸé‡ã¿ä»˜ãå¹³å‡
        weights = [eng * rel for eng, rel in zip(engagement_scores, relevance_scores)]
        weighted_scores = [score * weight for score, weight in zip(sentiment_scores, weights)]
        
        avg_sentiment = np.mean(weighted_scores) if weighted_scores else 0.0
        
        # æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        if avg_sentiment >= 0.3:
            sentiment_type = SentimentType.POSITIVE
        elif avg_sentiment <= -0.3:
            sentiment_type = SentimentType.NEGATIVE
        else:
            sentiment_type = SentimentType.NEUTRAL
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        confidence = min(np.mean(relevance_scores), 1.0) if relevance_scores else 0.0
        
        return {
            "score": avg_sentiment,
            "type": sentiment_type,
            "confidence": confidence,
            "posts": social_data["posts"]
        }
    
    def _calculate_overall_sentiment(
        self, news_sentiment: Dict[str, Any], social_sentiment: Dict[str, Any]
    ) -> Dict[str, Any]:
        """çµ±åˆæ„Ÿæƒ…åˆ†æ"""
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã®é‡ã¿ä»˜ãå¹³å‡ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹70%ã€ã‚½ãƒ¼ã‚·ãƒ£ãƒ«30%ï¼‰
        news_weight = 0.7
        social_weight = 0.3
        
        overall_score = (
            news_sentiment["score"] * news_weight * news_sentiment["confidence"] +
            social_sentiment["score"] * social_weight * social_sentiment["confidence"]
        ) / (news_weight * news_sentiment["confidence"] + social_weight * social_sentiment["confidence"])
        
        # æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        if overall_score >= 0.3:
            sentiment_type = SentimentType.POSITIVE
        elif overall_score <= -0.3:
            sentiment_type = SentimentType.NEGATIVE
        else:
            sentiment_type = SentimentType.NEUTRAL
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—
        confidence = (news_sentiment["confidence"] * news_weight + 
                     social_sentiment["confidence"] * social_weight)
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ¯”ç‡ã®è¨ˆç®—
        all_scores = [news_sentiment["score"], social_sentiment["score"]]
        positive_ratio = len([s for s in all_scores if s > 0.1]) / len(all_scores)
        negative_ratio = len([s for s in all_scores if s < -0.1]) / len(all_scores)
        neutral_ratio = 1 - positive_ratio - negative_ratio
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—
        volatility = np.std(all_scores) if len(all_scores) > 1 else 0.0
        
        return {
            "score": overall_score,
            "type": sentiment_type,
            "confidence": confidence,
            "positive_ratio": positive_ratio,
            "negative_ratio": negative_ratio,
            "neutral_ratio": neutral_ratio,
            "volatility": volatility
        }
    
    def _calculate_sentiment_trend(self, symbol: str, current_sentiment: Dict[str, Any]) -> SentimentTrend:
        """æ„Ÿæƒ…ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¨ˆç®—"""
        with self.lock:
            stock_sentiment = self.stock_sentiments[symbol]
            history = list(stock_sentiment.sentiment_history)
        
        if len(history) < 3:
            return SentimentTrend.STABLE
        
        # æœ€è¿‘ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢
        recent_scores = [h["score"] for h in history[-3:]]
        older_scores = [h["score"] for h in history[-6:-3]] if len(history) >= 6 else recent_scores
        
        recent_avg = np.mean(recent_scores)
        older_avg = np.mean(older_scores)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—
        volatility = np.std(recent_scores)
        
        if volatility > 0.3:
            return SentimentTrend.VOLATILE
        elif recent_avg > older_avg + 0.1:
            return SentimentTrend.IMPROVING
        elif recent_avg < older_avg - 0.1:
            return SentimentTrend.DECLINING
        else:
            return SentimentTrend.STABLE
    
    def get_individual_sentiment(self, symbol: str) -> Optional[IndividualStockSentiment]:
        """å€‹åˆ¥éŠ˜æŸ„ã®æ„Ÿæƒ…åˆ†æçµæœå–å¾—"""
        return self.stock_sentiments.get(symbol)
    
    def get_all_sentiments(self) -> Dict[str, IndividualStockSentiment]:
        """å…¨éŠ˜æŸ„ã®æ„Ÿæƒ…åˆ†æçµæœå–å¾—"""
        return self.stock_sentiments.copy()
    
    def get_sentiment_summary(self) -> Dict[str, Any]:
        """æ„Ÿæƒ…åˆ†æã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        with self.lock:
            sentiments = list(self.stock_sentiments.values())
        
        if not sentiments:
            return {}
        
        # å…¨ä½“çµ±è¨ˆ
        overall_scores = [s.overall_sentiment_score for s in sentiments]
        avg_sentiment = np.mean(overall_scores)
        
        # æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
        sentiment_types = [s.overall_sentiment_type for s in sentiments]
        positive_count = sum(1 for t in sentiment_types if t == SentimentType.POSITIVE)
        negative_count = sum(1 for t in sentiment_types if t == SentimentType.NEGATIVE)
        neutral_count = sum(1 for t in sentiment_types if t == SentimentType.NEUTRAL)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†å¸ƒ
        trends = [s.sentiment_trend for s in sentiments]
        improving_count = sum(1 for t in trends if t == SentimentTrend.IMPROVING)
        declining_count = sum(1 for t in trends if t == SentimentTrend.DECLINING)
        volatile_count = sum(1 for t in trends if t == SentimentTrend.VOLATILE)
        
        return {
            "timestamp": datetime.now(),
            "total_symbols": len(sentiments),
            "average_sentiment": avg_sentiment,
            "sentiment_distribution": {
                "positive": positive_count,
                "negative": negative_count,
                "neutral": neutral_count
            },
            "trend_distribution": {
                "improving": improving_count,
                "declining": declining_count,
                "volatile": volatile_count,
                "stable": len(sentiments) - improving_count - declining_count - volatile_count
            },
            "high_confidence_count": sum(1 for s in sentiments if s.confidence > 0.7),
            "high_volatility_count": sum(1 for s in sentiments if s.volatility > 0.3)
        }
    
    def save_sentiment_data(self, filename: str = "enhanced_sentiment_data.json"):
        """æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        try:
            data = {
                "timestamp": datetime.now().isoformat(),
                "sentiments": {},
                "summary": self.get_sentiment_summary()
            }
            
            for symbol, sentiment in self.stock_sentiments.items():
                data["sentiments"][symbol] = {
                    "symbol": sentiment.symbol,
                    "overall_sentiment_score": sentiment.overall_sentiment_score,
                    "overall_sentiment_type": sentiment.overall_sentiment_type.value,
                    "news_sentiment_score": sentiment.news_sentiment_score,
                    "social_sentiment_score": sentiment.social_sentiment_score,
                    "sentiment_trend": sentiment.sentiment_trend.value,
                    "confidence": sentiment.confidence,
                    "news_count": sentiment.news_count,
                    "social_mentions": sentiment.social_mentions,
                    "positive_ratio": sentiment.positive_ratio,
                    "negative_ratio": sentiment.negative_ratio,
                    "neutral_ratio": sentiment.neutral_ratio,
                    "volatility": sentiment.volatility,
                    "last_updated": sentiment.last_updated.isoformat(),
                    "news_items": [asdict(item) for item in sentiment.news_items],
                    "social_posts": [asdict(post) for post in sentiment.social_posts],
                    "sentiment_history": [
                        {
                            "timestamp": h["timestamp"].isoformat(),
                            "score": h["score"],
                            "type": h["type"].value,
                            "confidence": h["confidence"]
                        } for h in sentiment.sentiment_history
                    ]
                }
            
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # è¨­å®š
    config = {
        "news_api_key": "",  # å®Ÿéš›ã®APIã‚­ãƒ¼ã‚’è¨­å®š
        "twitter_credentials": {},
        "reddit_credentials": {},
        "sentiment_update_interval": 300,
        "news_cache_duration": 3600,
        "social_cache_duration": 1800,
        "max_news_items": 100,
        "max_social_posts": 200,
        "sentiment_history_size": 1000,
        "relevance_threshold": 0.3,
        "confidence_threshold": 0.5
    }
    
    # æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    sentiment_system = EnhancedNewsSentimentIntegration(config)
    
    # ç›£è¦–é–‹å§‹
    try:
        await sentiment_system.start_sentiment_monitoring()
    except KeyboardInterrupt:
        logger.info("æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’åœæ­¢ã—ã¾ã™")
        
        # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        sentiment_system.save_sentiment_data()
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = sentiment_system.get_sentiment_summary()
        print("\n" + "=" * 80)
        print("ğŸ“Š ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æ„Ÿæƒ…åˆ†æçµ±åˆã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚ã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        print(f"ç›£è¦–éŠ˜æŸ„æ•°: {summary['total_symbols']}")
        print(f"å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {summary['average_sentiment']:+.3f}")
        print(f"ãƒã‚¸ãƒ†ã‚£ãƒ–éŠ˜æŸ„æ•°: {summary['sentiment_distribution']['positive']}")
        print(f"ãƒã‚¬ãƒ†ã‚£ãƒ–éŠ˜æŸ„æ•°: {summary['sentiment_distribution']['negative']}")
        print(f"æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰éŠ˜æŸ„æ•°: {summary['trend_distribution']['improving']}")
        print(f"é«˜ä¿¡é ¼åº¦éŠ˜æŸ„æ•°: {summary['high_confidence_count']}")


if __name__ == "__main__":
    asyncio.run(main())
