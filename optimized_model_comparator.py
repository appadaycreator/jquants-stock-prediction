#!/usr/bin/env python3
"""
æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ 
é‡è¤‡è¨ˆç®—ã‚’æ’é™¤ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple, Any, Callable
from functools import lru_cache
import joblib
import hashlib
import os
from unified_parallel_processing_system import (
    execute_parallel,
    get_parallel_config,
    set_parallel_config
)
from unified_system import get_unified_system
import multiprocessing as mp
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import time
from dataclasses import dataclass
from unified_system import UnifiedSystem

logger = logging.getLogger(__name__)


@dataclass
class ModelResult:
    """ãƒ¢ãƒ‡ãƒ«çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    model_name: str
    model_type: str
    mae: float
    mse: float
    rmse: float
    r2: float
    training_time: float
    prediction_time: float
    memory_usage: float
    cross_val_scores: Optional[List[float]] = None
    feature_importance: Optional[Dict[str, float]] = None


class ModelCache:
    """ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, cache_dir: str = "model_cache"):
        self.cache_dir = cache_dir
        self.logger = logging.getLogger(__name__)
        os.makedirs(cache_dir, exist_ok=True)

    def _generate_cache_key(
        self, model_name: str, X_hash: str, y_hash: str, params: Dict
    ) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        key_string = f"{model_name}_{X_hash}_{y_hash}_{str(sorted(params.items()))}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def _generate_data_hash(self, data: np.ndarray) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        return hashlib.md5(data.tobytes()).hexdigest()

    def get_cached_model(
        self, model_name: str, X_train: np.ndarray, y_train: np.ndarray, params: Dict
    ) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        try:
            X_hash = self._generate_data_hash(X_train)
            y_hash = self._generate_data_hash(y_train)
            cache_key = self._generate_cache_key(model_name, X_hash, y_hash, params)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.joblib")

            if os.path.exists(cache_file):
                self.logger.debug(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {model_name}")
                return joblib.load(cache_file)
            return None
        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def save_model(
        self,
        model: Any,
        model_name: str,
        X_train: np.ndarray,
        y_train: np.ndarray,
        params: Dict,
    ):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            X_hash = self._generate_data_hash(X_train)
            y_hash = self._generate_data_hash(y_train)
            cache_key = self._generate_cache_key(model_name, X_hash, y_hash, params)
            cache_file = os.path.join(self.cache_dir, f"{cache_key}.joblib")

            joblib.dump(model, cache_file)
            self.logger.debug(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {model_name}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            for file in os.listdir(self.cache_dir):
                if file.endswith(".joblib"):
                    os.remove(os.path.join(self.cache_dir, file))
            self.logger.info("ğŸ§¹ ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")


class FeatureCache:
    """ç‰¹å¾´é‡è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""

    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_order = []
        self.logger = logging.getLogger(__name__)

    @lru_cache(maxsize=100)
    def get_cached_features(
        self, data_hash: str, feature_func_name: str, *args, **kwargs
    ):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç‰¹å¾´é‡ã‚’å–å¾—"""
        cache_key = f"{data_hash}_{feature_func_name}_{hash(str(args))}_{hash(str(sorted(kwargs.items())))}"

        if cache_key in self.cache:
            self.access_order.remove(cache_key)
            self.access_order.append(cache_key)
            self.logger.debug(f"ğŸ“‹ ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {feature_func_name}")
            return self.cache[cache_key]

        return None

    def save_features(
        self, data_hash: str, feature_func_name: str, result: Any, *args, **kwargs
    ):
        """ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_key = f"{data_hash}_{feature_func_name}_{hash(str(args))}_{hash(str(sorted(kwargs.items())))}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.cache) >= self.max_size:
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]

        self.cache[cache_key] = result
        self.access_order.append(cache_key)
        self.logger.debug(f"ğŸ’¾ ç‰¹å¾´é‡ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {feature_func_name}")


class OptimizedModelComparator:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚¯ãƒ©ã‚¹"""

    def __init__(self, max_workers: int = None, use_cache: bool = True):
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰max_workersã‚’èª­ã¿è¾¼ã¿
        try:
            import yaml

            with open("config_final.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            self.max_workers = max_workers or config.get("performance", {}).get(
                "max_workers", 4
            )
        except Exception:
            self.max_workers = max_workers or min(4, mp.cpu_count())
        self.use_cache = use_cache
        self.model_cache = ModelCache() if use_cache else None
        self.feature_cache = FeatureCache()
        self.system = UnifiedSystem("OptimizedModelComparator")
        self.logger = logging.getLogger(__name__)

        # è¨ˆç®—æ¸ˆã¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.metrics_cache = {}

    def compare_models_optimized(
        self,
        models_config: Dict[str, Dict],
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        feature_names: List[str] = None,
        use_parallel: bool = True,
        use_cross_validation: bool = False,
        cv_folds: int = 5,
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        self.logger.info(f"ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒé–‹å§‹ (ä¸¦åˆ—å‡¦ç†: {use_parallel})")

        start_time = time.time()
        results = []

        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
        data_hash = self._generate_data_hash(X_train, y_train)

        if use_parallel:
            results = self._compare_models_parallel(
                models_config,
                X_train,
                X_test,
                y_train,
                y_test,
                feature_names,
                data_hash,
                use_cross_validation,
                cv_folds,
            )
        else:
            results = self._compare_models_sequential(
                models_config,
                X_train,
                X_test,
                y_train,
                y_test,
                feature_names,
                data_hash,
                use_cross_validation,
                cv_folds,
            )

        processing_time = time.time() - start_time
        self.logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Œäº†: {processing_time:.2f}ç§’")

        if not results:
            self.logger.warning("âš ï¸ è©•ä¾¡ã§ããŸãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return pd.DataFrame()

        df_results = pd.DataFrame(results)
        df_results = df_results.sort_values("mae")

        # çµæœã®çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self._log_comparison_summary(df_results)

        return df_results

    def _compare_models_parallel(
        self,
        models_config: Dict,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        feature_names: List[str],
        data_hash: str,
        use_cross_validation: bool,
        cv_folds: int,
    ) -> List[ModelResult]:
        """ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        results = []

        with get_unified_system().execute_parallel(self.max_workers) as executor:
            # å„ãƒ¢ãƒ‡ãƒ«ã®å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            futures = []
            for model_name, config in models_config.items():
                future = executor.submit(
                    self._evaluate_single_model,
                    model_name,
                    config,
                    X_train,
                    X_test,
                    y_train,
                    y_test,
                    feature_names,
                    data_hash,
                    use_cross_validation,
                    cv_folds,
                )
                futures.append(future)

            # çµæœã‚’åé›†
            for future in futures:
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"âŒ ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

        return results

    def _compare_models_sequential(
        self,
        models_config: Dict,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        feature_names: List[str],
        data_hash: str,
        use_cross_validation: bool,
        cv_folds: int,
    ) -> List[ModelResult]:
        """é€æ¬¡å‡¦ç†ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ"""
        results = []

        for model_name, config in models_config.items():
            try:
                result = self._evaluate_single_model(
                    model_name,
                    config,
                    X_train,
                    X_test,
                    y_train,
                    y_test,
                    feature_names,
                    data_hash,
                    use_cross_validation,
                    cv_folds,
                )
                if result:
                    results.append(result)
            except Exception as e:
                self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ« {model_name} ã®è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return results

    def _evaluate_single_model(
        self,
        model_name: str,
        config: Dict,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        feature_names: List[str],
        data_hash: str,
        use_cross_validation: bool,
        cv_folds: int,
    ) -> Optional[ModelResult]:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
        try:
            self.logger.info(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹: {model_name}")

            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model_type = config.get("type", model_name)
            params = config.get("params", {})

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            cached_model = None
            if self.model_cache:
                cached_model = self.model_cache.get_cached_model(
                    model_name, X_train, y_train, params
                )

            if cached_model:
                model = cached_model
                self.logger.debug(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å–å¾—: {model_name}")
            else:
                # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
                model = self._create_model(model_type, params)

                # å­¦ç¿’æ™‚é–“ã‚’æ¸¬å®š
                train_start = time.time()
                model.fit(X_train, y_train)
                training_time = time.time() - train_start

                # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                if self.model_cache:
                    self.model_cache.save_model(
                        model, model_name, X_train, y_train, params
                    )
                training_time = 0.0  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ãŸå ´åˆ

            # äºˆæ¸¬æ™‚é–“ã‚’æ¸¬å®š
            pred_start = time.time()
            y_pred = model.predict(X_test)
            prediction_time = time.time() - pred_start

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰
            metrics = self._calculate_metrics_cached(
                y_test, y_pred, data_hash, model_name
            )

            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            cv_scores = None
            if use_cross_validation:
                cv_scores = self._calculate_cross_validation(
                    model, X_train, y_train, cv_folds
                )

            # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            feature_importance = None
            if feature_names and hasattr(model, "feature_importances_"):
                feature_importance = dict(
                    zip(feature_names, model.feature_importances_)
                )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
            memory_usage = self._estimate_memory_usage(model, X_train, X_test)

            result = ModelResult(
                model_name=model_name,
                model_type=model_type,
                mae=metrics["mae"],
                mse=metrics["mse"],
                rmse=metrics["rmse"],
                r2=metrics["r2"],
                training_time=training_time,
                prediction_time=prediction_time,
                memory_usage=memory_usage,
                cross_val_scores=cv_scores,
                feature_importance=feature_importance,
            )

            self.logger.info(
                f"âœ… {model_name} å®Œäº†: MAE={metrics['mae']:.4f}, æ™‚é–“={training_time:.2f}s"
            )
            return result

        except Exception as e:
            self.system.log_error(
                e,
                f"{model_name}è©•ä¾¡ã‚¨ãƒ©ãƒ¼",
                additional_info={
                    "model_type": model_type,
                    "params": params,
                    "X_train_shape": X_train.shape,
                    "y_train_shape": y_train.shape,
                },
            )
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ« {model_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _create_model(self, model_type: str, params: Dict) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        from model_factory import ModelFactory

        factory = ModelFactory()
        return factory.create_model(model_type, params)

    def _calculate_metrics_cached(
        self, y_test: np.ndarray, y_pred: np.ndarray, data_hash: str, model_name: str
    ) -> Dict[str, float]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        cache_key = f"{data_hash}_{model_name}_{hash(y_test.tobytes())}_{hash(y_pred.tobytes())}"

        if cache_key in self.metrics_cache:
            self.logger.debug(f"ğŸ“‹ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {model_name}")
            return self.metrics_cache[cache_key]

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = {
            "mae": mean_absolute_error(y_test, y_pred),
            "mse": mean_squared_error(y_test, y_pred),
            "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
            "r2": r2_score(y_test, y_pred),
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.metrics_cache[cache_key] = metrics
        return metrics

    def _calculate_cross_validation(
        self, model: Any, X: np.ndarray, y: np.ndarray, cv_folds: int
    ) -> List[float]:
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        try:
            scores = cross_val_score(
                model, X, y, cv=cv_folds, scoring="neg_mean_absolute_error"
            )
            return (-scores).tolist()  # è² ã®å€¤ã‚’æ­£ã®å€¤ã«å¤‰æ›
        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _estimate_memory_usage(
        self, model: Any, X_train: np.ndarray, X_test: np.ndarray
    ) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®šï¼ˆMBï¼‰"""
        try:
            import sys

            model_size = sys.getsizeof(model)
            data_size = (X_train.nbytes + X_test.nbytes) / 1024 / 1024
            return model_size / 1024 / 1024 + data_size
        except:
            return 0.0

    def _generate_data_hash(self, X: np.ndarray, y: np.ndarray) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        combined = np.concatenate([X.flatten(), y.flatten()])
        return hashlib.md5(combined.tobytes()).hexdigest()

    def _log_comparison_summary(self, df_results: pd.DataFrame):
        """æ¯”è¼ƒçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        if df_results.empty:
            return

        self.logger.info("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼:")
        self.logger.info(
            f"  ğŸ† æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«: {df_results.iloc[0]['model_name']} (MAE: {df_results.iloc[0]['mae']:.4f})"
        )
        self.logger.info(f"  ğŸ“ˆ å¹³å‡MAE: {df_results['mae'].mean():.4f}")
        self.logger.info(
            f"  â±ï¸ å¹³å‡å­¦ç¿’æ™‚é–“: {df_results['training_time'].mean():.2f}ç§’"
        )
        self.logger.info(
            f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {df_results['memory_usage'].mean():.1f}MB"
        )

        # ä¸Šä½3ãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
        top_3 = df_results.head(3)
        self.logger.info("ğŸ¥‡ ä¸Šä½3ãƒ¢ãƒ‡ãƒ«:")
        for i, (_, row) in enumerate(top_3.iterrows(), 1):
            self.logger.info(
                f"  {i}. {row['model_name']}: MAE={row['mae']:.4f}, RÂ²={row['r2']:.4f}"
            )

    def clear_all_caches(self):
        """å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self.model_cache:
            self.model_cache.clear_cache()
        self.feature_cache.cache.clear()
        self.metrics_cache.clear()
        self.logger.info("ğŸ§¹ å…¨ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")


class BatchModelProcessor:
    """ãƒãƒƒãƒãƒ¢ãƒ‡ãƒ«å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, comparator: OptimizedModelComparator):
        self.comparator = comparator
        self.logger = logging.getLogger(__name__)

    def process_multiple_datasets(
        self,
        datasets: List[Tuple[str, np.ndarray, np.ndarray, np.ndarray, np.ndarray]],
        models_config: Dict[str, Dict],
        output_dir: str = "batch_results",
    ) -> Dict[str, pd.DataFrame]:
        """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€æ‹¬å‡¦ç†"""
        self.logger.info(f"ğŸ“Š ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(datasets)}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

        os.makedirs(output_dir, exist_ok=True)
        results = {}

        for dataset_name, X_train, X_test, y_train, y_test in datasets:
            try:
                self.logger.info(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†: {dataset_name}")

                # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿè¡Œ
                comparison_results = self.comparator.compare_models_optimized(
                    models_config, X_train, X_test, y_train, y_test
                )

                # çµæœã‚’ä¿å­˜
                output_file = os.path.join(output_dir, f"{dataset_name}_results.csv")
                comparison_results.to_csv(output_file, index=False)

                results[dataset_name] = comparison_results
                self.logger.info(f"âœ… {dataset_name} å®Œäº†: {output_file}")

            except Exception as e:
                self.logger.error(f"âŒ {dataset_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        self.logger.info(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}/{len(datasets)} æˆåŠŸ")
        return results


def create_optimized_comparator(
    max_workers: int = None, use_cache: bool = True
) -> OptimizedModelComparator:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå™¨ã‚’ä½œæˆ"""
    return OptimizedModelComparator(max_workers=max_workers, use_cache=use_cache)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    logging.basicConfig(level=logging.INFO)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    from sklearn.datasets import make_regression
    from sklearn.model_selection import train_test_split

    X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    models_config = {
        "random_forest": {"type": "random_forest", "params": {"n_estimators": 100}},
        "xgboost": {"type": "xgboost", "params": {"n_estimators": 100}},
        "linear_regression": {"type": "linear_regression", "params": {}},
        "ridge": {"type": "ridge", "params": {"alpha": 1.0}},
    }

    # æœ€é©åŒ–ã•ã‚ŒãŸæ¯”è¼ƒå™¨ã‚’ä½œæˆ
    comparator = create_optimized_comparator(use_cache=True)

    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿè¡Œ
    results = comparator.compare_models_optimized(
        models_config, X_train, X_test, y_train, y_test, use_parallel=True
    )

    print("\nğŸ“Š æ¯”è¼ƒçµæœ:")
    print(results[["model_name", "mae", "r2", "training_time"]].to_string(index=False))
