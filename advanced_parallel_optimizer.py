#!/usr/bin/env python3
"""
é«˜åº¦ãªä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
CPUä½¿ç”¨ç‡ã¨å‡¦ç†é€Ÿåº¦ã‚’æœ€é©åŒ–ã™ã‚‹é«˜åº¦ãªæ©Ÿèƒ½ã‚’æä¾›
"""

import os
import sys
import time
import logging
import threading
import multiprocessing as mp
from typing import Dict, Any, List, Callable, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass
import yaml
import psutil
import numpy as np
from queue import PriorityQueue
import gc
from contextlib import contextmanager
from functools import wraps
import tracemalloc

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class OptimizationMetrics:
    """æœ€é©åŒ–æŒ‡æ¨™"""
    cpu_usage: float
    memory_usage: float
    execution_time: float
    throughput: float
    efficiency: float
    timestamp: float


@dataclass
class TaskProfile:
    """ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    task_id: str
    task_type: str
    estimated_duration: float
    memory_requirement: float
    cpu_intensity: float
    priority: int
    dependencies: List[str] = None


class AdvancedParallelOptimizer:
    """é«˜åº¦ãªä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = "config_final.yaml"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.cpu_count = mp.cpu_count()
        self.total_memory = psutil.virtual_memory().total
        self.max_workers = self._get_max_workers()
        
        # æœ€é©åŒ–è¨­å®š
        self.optimization_enabled = self.config.get("performance", {}).get("optimization", True)
        self.auto_scaling = self.config.get("performance", {}).get("auto_scaling", True)
        self.memory_threshold = self.config.get("performance", {}).get("memory_threshold", 0.8)
        self.cpu_threshold = self.config.get("performance", {}).get("cpu_threshold", 0.8)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.performance_history: List[OptimizationMetrics] = []
        self.task_profiles: Dict[str, TaskProfile] = {}
        self.optimization_lock = threading.Lock()
        
        # å‹•çš„èª¿æ•´
        self.current_workers = self.max_workers
        self.worker_adjustment_history = []
        self.optimization_interval = 10  # 10ç§’é–“éš”
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        self.memory_optimization_enabled = True
        self.gc_threshold = 0.9  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡90%ã§ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        
        # ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼
        self.task_queue = PriorityQueue()
        self.worker_threads = []
        
        # çµ±è¨ˆæƒ…å ±
        self.total_tasks_processed = 0
        self.total_execution_time = 0.0
        self.optimization_savings = 0.0
        
        logger.info("ğŸš€ é«˜åº¦ãªä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   - CPUæ•°: {self.cpu_count}")
        logger.info(f"   - æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        logger.info(f"   - æœ€é©åŒ–: {'æœ‰åŠ¹' if self.optimization_enabled else 'ç„¡åŠ¹'}")
        logger.info(f"   - è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: {'æœ‰åŠ¹' if self.auto_scaling else 'ç„¡åŠ¹'}")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _get_max_workers(self) -> int:
        """è¨­å®šã‹ã‚‰æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—"""
        try:
            environment = self.config.get("system", {}).get("environment", "production")
            env_config = self.config.get("environments", {}).get(environment, {})
            
            if "performance" in env_config and "max_workers" in env_config["performance"]:
                return env_config["performance"]["max_workers"]
            
            return self.config.get("performance", {}).get("max_workers", 4)
        except Exception as e:
            logger.warning(f"max_workersè¨­å®šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return min(4, mp.cpu_count())
    
    def get_system_metrics(self) -> OptimizationMetrics:
        """ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã‚’å–å¾—"""
        cpu_usage = psutil.cpu_percent(interval=1)
        memory_usage = psutil.virtual_memory().percent / 100
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼ˆã‚¿ã‚¹ã‚¯/ç§’ï¼‰
        if self.total_execution_time > 0:
            throughput = self.total_tasks_processed / self.total_execution_time
        else:
            throughput = 0.0
        
        # åŠ¹ç‡æ€§è¨ˆç®—ï¼ˆCPUä½¿ç”¨ç‡ã«å¯¾ã™ã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰
        if cpu_usage > 0:
            efficiency = throughput / (cpu_usage / 100)
        else:
            efficiency = 0.0
        
        return OptimizationMetrics(
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            execution_time=self.total_execution_time,
            throughput=throughput,
            efficiency=efficiency,
            timestamp=time.time()
        )
    
    def optimize_worker_count(self) -> int:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æœ€é©åŒ–"""
        if not self.auto_scaling:
            return self.current_workers
        
        with self.optimization_lock:
            metrics = self.get_system_metrics()
            
            # CPUä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if metrics.cpu_usage > self.cpu_threshold * 100:
                # CPUä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™
                new_workers = max(1, self.current_workers - 1)
            elif metrics.cpu_usage < (self.cpu_threshold * 100) / 2:
                # CPUä½¿ç”¨ç‡ãŒä½ã„å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å¢—ã‚„ã™
                new_workers = min(self.max_workers, self.current_workers + 1)
            else:
                new_workers = self.current_workers
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if metrics.memory_usage > self.memory_threshold:
                new_workers = max(1, new_workers - 1)
            
            # åŠ¹ç‡æ€§ã«åŸºã¥ãèª¿æ•´
            if len(self.performance_history) >= 3:
                recent_efficiency = [m.efficiency for m in self.performance_history[-3:]]
                avg_efficiency = np.mean(recent_efficiency)
                
                if avg_efficiency < 0.5:  # åŠ¹ç‡æ€§ãŒä½ã„å ´åˆ
                    new_workers = max(1, new_workers - 1)
                elif avg_efficiency > 1.0:  # åŠ¹ç‡æ€§ãŒé«˜ã„å ´åˆ
                    new_workers = min(self.max_workers, new_workers + 1)
            
            if new_workers != self.current_workers:
                old_workers = self.current_workers
                self.current_workers = new_workers
                self.worker_adjustment_history.append({
                    "timestamp": time.time(),
                    "old_workers": old_workers,
                    "new_workers": new_workers,
                    "cpu_usage": metrics.cpu_usage,
                    "memory_usage": metrics.memory_usage,
                    "efficiency": metrics.efficiency
                })
                
                logger.info(f"ğŸ”„ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æœ€é©åŒ–: {old_workers} â†’ {new_workers}")
                logger.info(f"   - CPUä½¿ç”¨ç‡: {metrics.cpu_usage:.1f}%")
                logger.info(f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {metrics.memory_usage:.1f}%")
                logger.info(f"   - åŠ¹ç‡æ€§: {metrics.efficiency:.2f}")
        
        return self.current_workers
    
    def optimize_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
        if not self.memory_optimization_enabled:
            return
        
        current_memory = psutil.virtual_memory().percent / 100
        
        if current_memory > self.gc_threshold:
            logger.info("ğŸ§¹ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé–¾å€¤ã‚’è¶…éã€ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
            gc.collect()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å†ãƒã‚§ãƒƒã‚¯
            new_memory = psutil.virtual_memory().percent / 100
            logger.info(f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {current_memory:.1f}% â†’ {new_memory:.1f}%")
    
    def profile_task(self, task_id: str, task_type: str, estimated_duration: float, 
                     memory_requirement: float, cpu_intensity: float, priority: int = 1):
        """ã‚¿ã‚¹ã‚¯ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
        profile = TaskProfile(
            task_id=task_id,
            task_type=task_type,
            estimated_duration=estimated_duration,
            memory_requirement=memory_requirement,
            cpu_intensity=cpu_intensity,
            priority=priority
        )
        
        self.task_profiles[task_id] = profile
        logger.debug(f"ğŸ“Š ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {task_id}")
    
    def get_optimal_executor(self, task_type: str, task_profile: Optional[TaskProfile] = None) -> Tuple[type, int]:
        """æœ€é©ãªExecutorã‚’å–å¾—"""
        optimized_workers = self.optimize_worker_count()
        
        if task_profile:
            # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãæœ€é©åŒ–
            if task_profile.cpu_intensity > 0.7:
                # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
                return ProcessPoolExecutor, min(optimized_workers, self.cpu_count)
            elif task_profile.memory_requirement > 0.5:
                # ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ã‚¿ã‚¹ã‚¯
                return ThreadPoolExecutor, max(1, optimized_workers // 2)
            else:
                # æ··åˆã‚¿ã‚¹ã‚¯
                return ThreadPoolExecutor, optimized_workers
        else:
            # å¾“æ¥ã®åˆ¤å®š
            if task_type == "cpu_intensive":
                return ProcessPoolExecutor, min(optimized_workers, self.cpu_count)
            elif task_type == "io_intensive":
                return ThreadPoolExecutor, optimized_workers * 2
            else:
                return ThreadPoolExecutor, optimized_workers
    
    def execute_optimized_parallel(
        self,
        tasks: List[Callable],
        task_type: str = "mixed",
        timeout: Optional[int] = None,
        priority: int = 1,
        task_profiles: Optional[List[TaskProfile]] = None
    ) -> List[Any]:
        """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—å®Ÿè¡Œ"""
        if not tasks:
            return []
        
        start_time = time.time()
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        self.optimize_memory_usage()
        
        # ã‚¿ã‚¹ã‚¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®é©ç”¨
        if task_profiles:
            executor_class, max_workers = self.get_optimal_executor(task_type, task_profiles[0])
        else:
            executor_class, max_workers = self.get_optimal_executor(task_type)
        
        logger.info(f"ğŸš€ æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")
        logger.info(f"   - ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        logger.info(f"   - Executor: {executor_class.__name__}")
        logger.info(f"   - å„ªå…ˆåº¦: {priority}")
        
        results = []
        
        try:
            with executor_class(max_workers=max_workers) as executor:
                # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
                future_to_task = {
                    executor.submit(task): i for i, task in enumerate(tasks)
                }
                
                # çµæœã‚’åé›†
                for future in as_completed(future_to_task, timeout=timeout):
                    try:
                        result = future.result()
                        results.append((future_to_task[future], result))
                    except Exception as e:
                        task_index = future_to_task[future]
                        logger.error(f"ã‚¿ã‚¹ã‚¯ {task_index} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                        results.append((task_index, None))
        
        except Exception as e:
            logger.error(f"æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
        # çµæœã‚’å…ƒã®é †åºã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[0])
        final_results = [result for _, result in results]
        
        execution_time = time.time() - start_time
        self.total_tasks_processed += len(tasks)
        self.total_execution_time += execution_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è¨˜éŒ²
        metrics = self.get_system_metrics()
        metrics.execution_time = execution_time
        self.performance_history.append(metrics)
        
        # å±¥æ­´ã‚’æœ€æ–°50ä»¶ã«åˆ¶é™
        if len(self.performance_history) > 50:
            self.performance_history = self.performance_history[-50:]
        
        logger.info(f"âœ… æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
        logger.info(f"   - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        logger.info(f"   - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {metrics.throughput:.2f} ã‚¿ã‚¹ã‚¯/ç§’")
        logger.info(f"   - åŠ¹ç‡æ€§: {metrics.efficiency:.2f}")
        logger.info(f"   - æˆåŠŸç‡: {len([r for r in final_results if r is not None])}/{len(tasks)}")
        
        return final_results
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """æœ€é©åŒ–çµ±è¨ˆã‚’å–å¾—"""
        with self.optimization_lock:
            recent_metrics = self.performance_history[-10:] if self.performance_history else []
            
            stats = {
                "current_workers": self.current_workers,
                "max_workers": self.max_workers,
                "total_tasks_processed": self.total_tasks_processed,
                "total_execution_time": self.total_execution_time,
                "optimization_enabled": self.optimization_enabled,
                "auto_scaling": self.auto_scaling,
                "worker_adjustments": len(self.worker_adjustment_history),
                "task_profiles": len(self.task_profiles)
            }
            
            if recent_metrics:
                stats.update({
                    "avg_cpu_usage": np.mean([m.cpu_usage for m in recent_metrics]),
                    "avg_memory_usage": np.mean([m.memory_usage for m in recent_metrics]),
                    "avg_throughput": np.mean([m.throughput for m in recent_metrics]),
                    "avg_efficiency": np.mean([m.efficiency for m in recent_metrics]),
                    "performance_history_count": len(self.performance_history)
                })
            
            return stats
    
    def start_optimization_monitoring(self):
        """æœ€é©åŒ–ç›£è¦–ã‚’é–‹å§‹"""
        def monitor_worker():
            while True:
                try:
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è¨˜éŒ²
                    metrics = self.get_system_metrics()
                    self.performance_history.append(metrics)
                    
                    # å±¥æ­´ã‚’æœ€æ–°100ä»¶ã«åˆ¶é™
                    if len(self.performance_history) > 100:
                        self.performance_history = self.performance_history[-100:]
                    
                    # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°æœ€é©åŒ–
                    if self.auto_scaling:
                        self.optimize_worker_count()
                    
                    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
                    self.optimize_memory_usage()
                    
                    time.sleep(self.optimization_interval)
                    
                except Exception as e:
                    logger.error(f"æœ€é©åŒ–ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(5)
        
        monitor_thread = threading.Thread(target=monitor_worker, daemon=True)
        monitor_thread.start()
        logger.info("ğŸ“Š æœ€é©åŒ–ç›£è¦–é–‹å§‹")
    
    def create_optimization_report(self) -> Dict[str, Any]:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        stats = self.get_optimization_stats()
        
        report = {
            "system_info": {
                "cpu_count": self.cpu_count,
                "total_memory_gb": self.total_memory / (1024**3),
                "max_workers": self.max_workers,
                "current_workers": self.current_workers
            },
            "performance_metrics": stats,
            "optimization_settings": {
                "optimization_enabled": self.optimization_enabled,
                "auto_scaling": self.auto_scaling,
                "memory_threshold": self.memory_threshold,
                "cpu_threshold": self.cpu_threshold
            },
            "worker_adjustments": self.worker_adjustment_history[-10:],  # æœ€æ–°10ä»¶
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        if not self.optimization_enabled:
            recommendations.append("æœ€é©åŒ–æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if not self.auto_scaling:
            recommendations.append("è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if self.performance_history:
            recent_metrics = self.performance_history[-5:]
            avg_cpu = np.mean([m.cpu_usage for m in recent_metrics])
            avg_memory = np.mean([m.memory_usage for m in recent_metrics])
            
            if avg_cpu > 80:
                recommendations.append("CPUä½¿ç”¨ç‡ãŒé«˜ã„ãŸã‚ã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
            
            if avg_memory > 0.8:
                recommendations.append("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„ãŸã‚ã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
            
            if len(self.performance_history) > 10:
                avg_efficiency = np.mean([m.efficiency for m in recent_metrics])
                if avg_efficiency < 0.5:
                    recommendations.append("åŠ¹ç‡æ€§ãŒä½ã„ãŸã‚ã€ã‚¿ã‚¹ã‚¯ã®åˆ†å‰²ã‚„æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return recommendations


# ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
_global_optimizer = None


def get_advanced_optimizer() -> AdvancedParallelOptimizer:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’å–å¾—"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = AdvancedParallelOptimizer()
        _global_optimizer.start_optimization_monitoring()
    return _global_optimizer


def execute_optimized_parallel(
    tasks: List[Callable],
    task_type: str = "mixed",
    timeout: Optional[int] = None,
    priority: int = 1,
    task_profiles: Optional[List[TaskProfile]] = None
) -> List[Any]:
    """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—å®Ÿè¡Œ"""
    optimizer = get_advanced_optimizer()
    return optimizer.execute_optimized_parallel(tasks, task_type, timeout, priority, task_profiles)


def advanced_parallel_decorator(task_type: str = "mixed", priority: int = 1):
    """é«˜åº¦ãªä¸¦åˆ—å‡¦ç†ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            optimizer = get_advanced_optimizer()
            return optimizer.execute_optimized_parallel([lambda: func(*args, **kwargs)], task_type, priority=priority)[0]
        return wrapper
    return decorator


@contextmanager
def optimization_context(task_type: str = "mixed", max_workers: Optional[int] = None):
    """æœ€é©åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    optimizer = get_advanced_optimizer()
    
    if max_workers:
        old_workers = optimizer.current_workers
        optimizer.current_workers = max_workers
    
    try:
        yield optimizer
    finally:
        if max_workers:
            optimizer.current_workers = old_workers


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    optimizer = get_advanced_optimizer()
    
    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
    def test_task(x):
        time.sleep(0.1)
        return x * 2
    
    # æœ€é©åŒ–ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
    tasks = [lambda x=i: test_task(x) for i in range(10)]
    results = execute_optimized_parallel(tasks, task_type="cpu_intensive")
    print(f"çµæœ: {results}")
    
    # æœ€é©åŒ–çµ±è¨ˆè¡¨ç¤º
    stats = optimizer.get_optimization_stats()
    print(f"æœ€é©åŒ–çµ±è¨ˆ: {stats}")
    
    # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    report = optimizer.create_optimization_report()
    print(f"æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ: {report}")
