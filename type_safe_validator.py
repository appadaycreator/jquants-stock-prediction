#!/usr/bin/env python3
"""
å‹å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã‚’é˜²ãã€ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ä¿è¨¼ã™ã‚‹
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional, Union
from datetime import datetime
import warnings

logger = logging.getLogger(__name__)


class TypeSafeValidator:
    """å‹å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, strict_mode: bool = True):
        self.strict_mode = strict_mode
        self.validation_results = {}

    def validate_dataframe_structure(
        self, df: pd.DataFrame, required_columns: List[str]
    ) -> Dict[str, Any]:
        """DataFrameã®æ§‹é€ æ¤œè¨¼"""
        logger.info("ğŸ” DataFrameæ§‹é€ ã®æ¤œè¨¼ã‚’é–‹å§‹")

        result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "missing_columns": [],
            "extra_columns": [],
        }

        # å¿…é ˆã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            result["missing_columns"] = missing_columns
            result["errors"].append(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_columns}")
            result["is_valid"] = False

        # ä½™åˆ†ãªã‚«ãƒ©ãƒ ã®ç¢ºèª
        extra_columns = [col for col in df.columns if col not in required_columns]
        if extra_columns:
            result["extra_columns"] = extra_columns
            result["warnings"].append(f"äºˆæœŸã—ãªã„ã‚«ãƒ©ãƒ : {extra_columns}")

        logger.info(f"âœ… DataFrameæ§‹é€ æ¤œè¨¼å®Œäº†: {len(df.columns)}ã‚«ãƒ©ãƒ , {len(df)}è¡Œ")
        return result

    def validate_numeric_columns(
        self, df: pd.DataFrame, numeric_columns: List[str]
    ) -> Dict[str, Any]:
        """æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼"""
        logger.info("ğŸ”¢ æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã‚’é–‹å§‹")

        result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "conversion_issues": {},
            "nan_issues": {},
            "inf_issues": {},
        }

        for col in numeric_columns:
            if col not in df.columns:
                result["warnings"].append(f"ã‚«ãƒ©ãƒ  '{col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                continue

            # å‹å¤‰æ›ã®æ¤œè¨¼
            try:
                original_dtype = df[col].dtype
                converted_series = pd.to_numeric(df[col], errors="coerce")

                # NaNå€¤ã®ç¢ºèª
                nan_count = converted_series.isna().sum()
                if nan_count > 0:
                    result["nan_issues"][col] = nan_count
                    if self.strict_mode and nan_count > len(df) * 0.5:  # 50%ä»¥ä¸Š
                        result["errors"].append(
                            f"ã‚«ãƒ©ãƒ  '{col}' ã«éåº¦ã®NaNå€¤: {nan_count}ä»¶"
                        )
                        result["is_valid"] = False
                    else:
                        result["warnings"].append(
                            f"ã‚«ãƒ©ãƒ  '{col}' ã«NaNå€¤: {nan_count}ä»¶"
                        )

                # ç„¡é™å€¤ã®ç¢ºèª
                inf_count = np.isinf(converted_series).sum()
                if inf_count > 0:
                    result["inf_issues"][col] = inf_count
                    result["errors"].append(f"ã‚«ãƒ©ãƒ  '{col}' ã«ç„¡é™å€¤: {inf_count}ä»¶")
                    result["is_valid"] = False

                # å‹å¤‰æ›ã®å•é¡Œ
                if original_dtype != converted_series.dtype:
                    result["conversion_issues"][col] = {
                        "original": str(original_dtype),
                        "converted": str(converted_series.dtype),
                    }
                    result["warnings"].append(
                        f"ã‚«ãƒ©ãƒ  '{col}' ã®å‹å¤‰æ›: {original_dtype} -> {converted_series.dtype}"
                    )

            except Exception as e:
                result["errors"].append(f"ã‚«ãƒ©ãƒ  '{col}' ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                result["is_valid"] = False

        logger.info("âœ… æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼å®Œäº†")
        return result

    def validate_date_columns(
        self, df: pd.DataFrame, date_columns: List[str]
    ) -> Dict[str, Any]:
        """æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼"""
        logger.info("ğŸ“… æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã‚’é–‹å§‹")

        result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "conversion_issues": {},
            "range_issues": {},
        }

        for col in date_columns:
            if col not in df.columns:
                result["warnings"].append(f"ã‚«ãƒ©ãƒ  '{col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                continue

            try:
                # æ—¥ä»˜å¤‰æ›ã®æ¤œè¨¼
                converted_dates = pd.to_datetime(df[col], errors="coerce")

                # NaNå€¤ã®ç¢ºèª
                nan_count = converted_dates.isna().sum()
                if nan_count > 0:
                    result["warnings"].append(
                        f"ã‚«ãƒ©ãƒ  '{col}' ã«ç„¡åŠ¹ãªæ—¥ä»˜: {nan_count}ä»¶"
                    )

                # æ—¥ä»˜ç¯„å›²ã®ç¢ºèª
                if not converted_dates.empty:
                    min_date = converted_dates.min()
                    max_date = converted_dates.max()

                    # ç•°å¸¸ãªæ—¥ä»˜ã®ç¢ºèª
                    current_year = datetime.now().year
                    if min_date.year < 1900 or max_date.year > current_year + 1:
                        result["range_issues"][col] = {
                            "min": str(min_date),
                            "max": str(max_date),
                        }
                        result["warnings"].append(
                            f"ã‚«ãƒ©ãƒ  '{col}' ã«ç•°å¸¸ãªæ—¥ä»˜ç¯„å›²: {min_date} - {max_date}"
                        )

            except Exception as e:
                result["errors"].append(f"ã‚«ãƒ©ãƒ  '{col}' ã®æ—¥ä»˜æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                result["is_valid"] = False

        logger.info("âœ… æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼å®Œäº†")
        return result

    def safe_nan_handling(
        self, df: pd.DataFrame, strategy: str = "forward_fill"
    ) -> pd.DataFrame:
        """å®‰å…¨ãªNaNå€¤å‡¦ç†"""
        logger.info(f"ğŸ§¹ å®‰å…¨ãªNaNå€¤å‡¦ç†ã‚’é–‹å§‹ (æˆ¦ç•¥: {strategy})")

        original_nan_count = df.isnull().sum().sum()
        if original_nan_count == 0:
            logger.info("âœ… NaNå€¤ã¯å­˜åœ¨ã—ã¾ã›ã‚“")
            return df

        df_processed = df.copy()

        try:
            if strategy == "forward_fill":
                df_processed = df_processed.fillna(method="ffill")
                df_processed = df_processed.fillna(
                    method="bfill"
                )  # å‰æ–¹è£œå®Œã§å‡¦ç†ã§ããªã„å ´åˆ
            elif strategy == "backward_fill":
                df_processed = df_processed.fillna(method="bfill")
                df_processed = df_processed.fillna(
                    method="ffill"
                )  # å¾Œæ–¹è£œå®Œã§å‡¦ç†ã§ããªã„å ´åˆ
            elif strategy == "mean":
                numeric_columns = df_processed.select_dtypes(
                    include=[np.number]
                ).columns
                df_processed[numeric_columns] = df_processed[numeric_columns].fillna(
                    df_processed[numeric_columns].mean()
                )
            elif strategy == "median":
                numeric_columns = df_processed.select_dtypes(
                    include=[np.number]
                ).columns
                df_processed[numeric_columns] = df_processed[numeric_columns].fillna(
                    df_processed[numeric_columns].median()
                )
            elif strategy == "drop":
                df_processed = df_processed.dropna()
            else:
                raise ValueError(f"æœªçŸ¥ã®NaNå‡¦ç†æˆ¦ç•¥: {strategy}")

            final_nan_count = df_processed.isnull().sum().sum()
            processed_count = original_nan_count - final_nan_count

            logger.info(
                f"âœ… NaNå€¤å‡¦ç†å®Œäº†: {processed_count}ä»¶å‡¦ç†, æ®‹ã‚Š{final_nan_count}ä»¶"
            )

            if final_nan_count > 0:
                logger.warning(f"âš ï¸ {final_nan_count}ä»¶ã®NaNå€¤ãŒæ®‹ã£ã¦ã„ã¾ã™")

        except Exception as e:
            logger.error(f"âŒ NaNå€¤å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        return df_processed

    def validate_data_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚’é–‹å§‹")

        result = {
            "is_valid": True,
            "errors": [],
            "warnings": [],
            "consistency_issues": {},
        }

        # é‡è¤‡è¡Œã®ç¢ºèª
        duplicate_count = df.duplicated().sum()
        if duplicate_count > 0:
            result["warnings"].append(f"é‡è¤‡è¡Œ: {duplicate_count}ä»¶")
            result["consistency_issues"]["duplicates"] = duplicate_count

        # æ•°å€¤ã‚«ãƒ©ãƒ ã®æ•´åˆæ€§ç¢ºèª
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col in ["Open", "High", "Low", "Close"]:
                # OHLCã®æ•´åˆæ€§ç¢ºèª
                invalid_ohlc = (
                    (df["High"] < df["Low"])
                    | (df["High"] < df["Open"])
                    | (df["High"] < df["Close"])
                    | (df["Low"] > df["Open"])
                    | (df["Low"] > df["Close"])
                ).sum()

                if invalid_ohlc > 0:
                    result["errors"].append(f"OHLCæ•´åˆæ€§ã‚¨ãƒ©ãƒ¼: {invalid_ohlc}ä»¶")
                    result["is_valid"] = False
                    result["consistency_issues"][f"{col}_ohlc"] = invalid_ohlc

        # è² ã®å€¤ã®ç¢ºèª
        for col in ["Volume", "Open", "High", "Low", "Close"]:
            if col in df.columns:
                negative_count = (df[col] < 0).sum()
                if negative_count > 0:
                    result["warnings"].append(
                        f"ã‚«ãƒ©ãƒ  '{col}' ã«è² ã®å€¤: {negative_count}ä»¶"
                    )
                    result["consistency_issues"][f"{col}_negative"] = negative_count

        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼å®Œäº†")
        return result

    def create_type_safe_dataframe(
        self, data: Union[Dict, List, pd.DataFrame], column_types: Dict[str, str]
    ) -> pd.DataFrame:
        """å‹å®‰å…¨ãªDataFrameã®ä½œæˆ"""
        logger.info("ğŸ›¡ï¸ å‹å®‰å…¨ãªDataFrameã®ä½œæˆã‚’é–‹å§‹")

        try:
            if isinstance(data, pd.DataFrame):
                df = data.copy()
            else:
                df = pd.DataFrame(data)

            # ã‚«ãƒ©ãƒ å‹ã®é©ç”¨
            for col, dtype in column_types.items():
                if col in df.columns:
                    try:
                        if dtype == "numeric":
                            df[col] = pd.to_numeric(df[col], errors="coerce")
                        elif dtype == "datetime":
                            df[col] = pd.to_datetime(df[col], errors="coerce")
                        elif dtype == "category":
                            df[col] = df[col].astype("category")
                        else:
                            df[col] = df[col].astype(dtype)
                    except Exception as e:
                        logger.warning(f"ã‚«ãƒ©ãƒ  '{col}' ã®å‹å¤‰æ›ã«å¤±æ•—: {e}")

            # NaNå€¤ã®å®‰å…¨ãªå‡¦ç†
            df = self.safe_nan_handling(df)

            logger.info("âœ… å‹å®‰å…¨ãªDataFrameã®ä½œæˆå®Œäº†")
            return df

        except Exception as e:
            logger.error(f"âŒ å‹å®‰å…¨ãªDataFrameã®ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise


def validate_stock_data_types(df: pd.DataFrame) -> Dict[str, Any]:
    """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å‹å®‰å…¨æ€§æ¤œè¨¼ï¼ˆå°‚ç”¨é–¢æ•°ï¼‰"""
    validator = TypeSafeValidator(strict_mode=True)

    # å¿…é ˆã‚«ãƒ©ãƒ ã®å®šç¾©
    required_columns = ["Date", "Code", "Open", "High", "Low", "Close", "Volume"]
    numeric_columns = ["Open", "High", "Low", "Close", "Volume"]
    date_columns = ["Date"]

    # æ§‹é€ æ¤œè¨¼
    structure_result = validator.validate_dataframe_structure(df, required_columns)

    # æ•°å€¤ã‚«ãƒ©ãƒ æ¤œè¨¼
    numeric_result = validator.validate_numeric_columns(df, numeric_columns)

    # æ—¥ä»˜ã‚«ãƒ©ãƒ æ¤œè¨¼
    date_result = validator.validate_date_columns(df, date_columns)

    # æ•´åˆæ€§æ¤œè¨¼
    consistency_result = validator.validate_data_consistency(df)

    # çµæœã®çµ±åˆ
    combined_result = {
        "is_valid": all(
            [
                structure_result["is_valid"],
                numeric_result["is_valid"],
                date_result["is_valid"],
                consistency_result["is_valid"],
            ]
        ),
        "errors": (
            structure_result["errors"]
            + numeric_result["errors"]
            + date_result["errors"]
            + consistency_result["errors"]
        ),
        "warnings": (
            structure_result["warnings"]
            + numeric_result["warnings"]
            + date_result["warnings"]
            + consistency_result["warnings"]
        ),
        "structure": structure_result,
        "numeric": numeric_result,
        "date": date_result,
        "consistency": consistency_result,
    }

    return combined_result
