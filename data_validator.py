#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯ã¨æ¤œè¨¼æ©Ÿèƒ½ã‚’æä¾›
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class DataValidator:
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆæœŸåŒ–"""
        self.config = config or {}
        self.validation_results = {}
        
    def validate_stock_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        logger.info("ğŸ” æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚’é–‹å§‹")
        
        results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'statistics': {},
            'quality_score': 0.0
        }
        
        try:
            # åŸºæœ¬æ§‹é€ ã®æ¤œè¨¼
            self._validate_basic_structure(df, results)
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼
            self._validate_data_types(df, results)
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
            self._validate_required_fields(df, results)
            
            # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®æ¤œè¨¼
            self._validate_data_ranges(df, results)
            
            # æ™‚ç³»åˆ—æ•´åˆæ€§ã®æ¤œè¨¼
            self._validate_time_series_consistency(df, results)
            
            # ç•°å¸¸å€¤ã®æ¤œå‡º
            self._detect_anomalies(df, results)
            
            # å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            results['quality_score'] = self._calculate_quality_score(results)
            
            # çµ±è¨ˆæƒ…å ±ã®åé›†
            results['statistics'] = self._collect_statistics(df)
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº† - å“è³ªã‚¹ã‚³ã‚¢: {results['quality_score']:.2f}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            results['is_valid'] = False
            results['errors'].append(f"æ¤œè¨¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return results
    
    def _validate_basic_structure(self, df: pd.DataFrame, results: Dict) -> None:
        """åŸºæœ¬æ§‹é€ ã®æ¤œè¨¼"""
        logger.info("ğŸ“Š åŸºæœ¬æ§‹é€ ã®æ¤œè¨¼")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å­˜åœ¨ç¢ºèª
        if df is None:
            results['errors'].append("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒNoneã§ã™")
            results['is_valid'] = False
            return
        
        # ç©ºãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
        if df.empty:
            results['errors'].append("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã§ã™")
            results['is_valid'] = False
            return
        
        # æœ€å°è¡Œæ•°ã®ãƒã‚§ãƒƒã‚¯
        min_rows = self.config.get('min_records', 1)
        if len(df) < min_rows:
            results['errors'].append(f"ãƒ‡ãƒ¼ã‚¿è¡Œæ•°ãŒä¸è¶³: {len(df)} < {min_rows}")
            results['is_valid'] = False
        
        logger.info(f"âœ… åŸºæœ¬æ§‹é€ æ¤œè¨¼å®Œäº†: {df.shape}")
    
    def _validate_data_types(self, df: pd.DataFrame, results: Dict) -> None:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼"""
        logger.info("ğŸ”¢ ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼")
        
        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®æ¤œè¨¼
        if 'Date' in df.columns:
            try:
                pd.to_datetime(df['Date'])
                logger.info("âœ… æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹æ¤œè¨¼å®Œäº†")
            except Exception as e:
                results['errors'].append(f"æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹ã‚¨ãƒ©ãƒ¼: {e}")
                results['is_valid'] = False
        
        # æ•°å€¤ã‚«ãƒ©ãƒ ã®æ¤œè¨¼
        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in numeric_columns:
            if col in df.columns:
                try:
                    pd.to_numeric(df[col], errors='raise')
                except Exception as e:
                    results['warnings'].append(f"{col}ã‚«ãƒ©ãƒ ã®æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼å®Œäº†")
    
    def _validate_required_fields(self, df: pd.DataFrame, results: Dict) -> None:
        """å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼"""
        logger.info("ğŸ“‹ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼")
        
        required_fields = self.config.get('required_fields', 
            ['Code', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume'])
        
        missing_fields = [field for field in required_fields if field not in df.columns]
        
        if missing_fields:
            results['errors'].append(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³: {missing_fields}")
            results['is_valid'] = False
        else:
            logger.info("âœ… å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œè¨¼å®Œäº†")
    
    def _validate_data_ranges(self, df: pd.DataFrame, results: Dict) -> None:
        """ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®æ¤œè¨¼"""
        logger.info("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®æ¤œè¨¼")
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        price_columns = ['Open', 'High', 'Low', 'Close']
        for col in price_columns:
            if col in df.columns:
                # è² ã®å€¤ã®ãƒã‚§ãƒƒã‚¯
                negative_count = (df[col] < 0).sum()
                if negative_count > 0:
                    results['warnings'].append(f"{col}ã«è² ã®å€¤ãŒ{negative_count}ä»¶ã‚ã‚Šã¾ã™")
                
                # ã‚¼ãƒ­å€¤ã®ãƒã‚§ãƒƒã‚¯
                zero_count = (df[col] == 0).sum()
                if zero_count > 0:
                    results['warnings'].append(f"{col}ã«ã‚¼ãƒ­å€¤ãŒ{zero_count}ä»¶ã‚ã‚Šã¾ã™")
                
                # High >= Low ã®ãƒã‚§ãƒƒã‚¯
                if 'High' in df.columns and 'Low' in df.columns:
                    invalid_hl = (df['High'] < df['Low']).sum()
                    if invalid_hl > 0:
                        results['errors'].append(f"High < Low ã®ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ãŒ{invalid_hl}ä»¶ã‚ã‚Šã¾ã™")
                        results['is_valid'] = False
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
        if 'Volume' in df.columns:
            negative_volume = (df['Volume'] < 0).sum()
            if negative_volume > 0:
                results['warnings'].append(f"Volumeã«è² ã®å€¤ãŒ{negative_volume}ä»¶ã‚ã‚Šã¾ã™")
        
        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ç¯„å›²æ¤œè¨¼å®Œäº†")
    
    def _validate_time_series_consistency(self, df: pd.DataFrame, results: Dict) -> None:
        """æ™‚ç³»åˆ—æ•´åˆæ€§ã®æ¤œè¨¼"""
        logger.info("â° æ™‚ç³»åˆ—æ•´åˆæ€§ã®æ¤œè¨¼")
        
        if 'Date' not in df.columns:
            return
        
        try:
            # æ—¥ä»˜ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            date_duplicates = df['Date'].duplicated().sum()
            if date_duplicates > 0:
                results['warnings'].append(f"é‡è¤‡ã™ã‚‹æ—¥ä»˜ãŒ{date_duplicates}ä»¶ã‚ã‚Šã¾ã™")
            
            # æ—¥ä»˜ã®é †åºãƒã‚§ãƒƒã‚¯
            df_sorted = df.sort_values('Date')
            date_diff = df_sorted['Date'].diff()
            negative_diff = (date_diff < pd.Timedelta(0)).sum()
            
            if negative_diff > 0:
                results['warnings'].append(f"æ—¥ä»˜ã®é †åºãŒé€†è»¢ã—ã¦ã„ã‚‹ç®‡æ‰€ãŒ{negative_diff}ä»¶ã‚ã‚Šã¾ã™")
            
            # æ—¥ä»˜ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
            date_range = df['Date'].max() - df['Date'].min()
            if date_range > pd.Timedelta(days=365*10):  # 10å¹´ä»¥ä¸Š
                results['warnings'].append("ãƒ‡ãƒ¼ã‚¿æœŸé–“ãŒ10å¹´ä»¥ä¸Šã§ã™")
            
            logger.info("âœ… æ™‚ç³»åˆ—æ•´åˆæ€§æ¤œè¨¼å®Œäº†")
            
        except Exception as e:
            results['warnings'].append(f"æ™‚ç³»åˆ—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _detect_anomalies(self, df: pd.DataFrame, results: Dict) -> None:
        """ç•°å¸¸å€¤ã®æ¤œå‡º"""
        logger.info("ğŸš¨ ç•°å¸¸å€¤ã®æ¤œå‡º")
        
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in ['Open', 'High', 'Low', 'Close', 'Volume']:
                # çµ±è¨ˆçš„ç•°å¸¸å€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ï¼‰
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                
                if outliers > 0:
                    outlier_rate = outliers / len(df) * 100
                    if outlier_rate > 5:  # 5%ä»¥ä¸Š
                        results['warnings'].append(f"{col}ã«ç•°å¸¸å€¤ãŒ{outliers}ä»¶ ({outlier_rate:.1f}%) ã‚ã‚Šã¾ã™")
                    else:
                        logger.info(f"ğŸ“Š {col}ã®ç•°å¸¸å€¤: {outliers}ä»¶ ({outlier_rate:.1f}%)")
        
        logger.info("âœ… ç•°å¸¸å€¤æ¤œå‡ºå®Œäº†")
    
    def _calculate_quality_score(self, results: Dict) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        score = 100.0
        
        # ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹æ¸›ç‚¹
        error_penalty = len(results['errors']) * 20
        score -= error_penalty
        
        # è­¦å‘Šã«ã‚ˆã‚‹æ¸›ç‚¹
        warning_penalty = len(results['warnings']) * 5
        score -= warning_penalty
        
        return max(0.0, score)
    
    def _collect_statistics(self, df: pd.DataFrame) -> Dict:
        """çµ±è¨ˆæƒ…å ±ã®åé›†"""
        stats = {
            'total_records': len(df),
            'total_columns': len(df.columns),
            'date_range': None,
            'missing_values': df.isnull().sum().to_dict(),
            'data_types': df.dtypes.to_dict()
        }
        
        if 'Date' in df.columns:
            stats['date_range'] = {
                'start': df['Date'].min(),
                'end': df['Date'].max(),
                'days': (df['Date'].max() - df['Date'].min()).days
            }
        
        return stats
    
    def generate_validation_report(self, results: Dict) -> str:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = []
        report.append("=" * 50)
        report.append("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 50)
        report.append(f"æ¤œè¨¼æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"å“è³ªã‚¹ã‚³ã‚¢: {results['quality_score']:.1f}/100")
        report.append(f"æ¤œè¨¼çµæœ: {'âœ… åˆæ ¼' if results['is_valid'] else 'âŒ ä¸åˆæ ¼'}")
        report.append("")
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        if results['errors']:
            report.append("âŒ ã‚¨ãƒ©ãƒ¼:")
            for error in results['errors']:
                report.append(f"  - {error}")
            report.append("")
        
        # è­¦å‘Šæƒ…å ±
        if results['warnings']:
            report.append("âš ï¸ è­¦å‘Š:")
            for warning in results['warnings']:
                report.append(f"  - {warning}")
            report.append("")
        
        # çµ±è¨ˆæƒ…å ±
        if results['statistics']:
            stats = results['statistics']
            report.append("ğŸ“Š çµ±è¨ˆæƒ…å ±:")
            report.append(f"  - ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['total_records']:,}")
            report.append(f"  - ç·ã‚«ãƒ©ãƒ æ•°: {stats['total_columns']}")
            
            if stats['date_range']:
                report.append(f"  - ãƒ‡ãƒ¼ã‚¿æœŸé–“: {stats['date_range']['start']} ï½ {stats['date_range']['end']}")
                report.append(f"  - æœŸé–“æ—¥æ•°: {stats['date_range']['days']}æ—¥")
            
            # æ¬ æå€¤æƒ…å ±
            missing_values = stats['missing_values']
            if any(count > 0 for count in missing_values.values()):
                report.append("  - æ¬ æå€¤:")
                for col, count in missing_values.items():
                    if count > 0:
                        report.append(f"    * {col}: {count}ä»¶")
        
        report.append("=" * 50)
        
        return "\n".join(report)

def validate_data_quality(df: pd.DataFrame, config: Optional[Dict] = None) -> Tuple[bool, str]:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç°¡æ˜“æ¤œè¨¼"""
    validator = DataValidator(config)
    results = validator.validate_stock_data(df)
    report = validator.generate_validation_report(results)
    
    return results['is_valid'], report
