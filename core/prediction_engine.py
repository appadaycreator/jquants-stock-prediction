#!/usr/bin/env python3
"""
äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
æ ªä¾¡äºˆæ¸¬ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€è©•ä¾¡ã€å¯è¦–åŒ–ã®çµ±åˆç®¡ç†
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime

# åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .model_manager import ModelManager
from .data_validator import DataValidator
from .visualization_manager import VisualizationManager
from .overfitting_detector import OverfittingDetector
from .json_data_manager import JSONDataManager
from .differential_updater import DifferentialUpdater


class PredictionEngine:
    """äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any] = None, logger=None, error_handler=None):
        """åˆæœŸåŒ–"""
        self.config = config or {}
        self.logger = logger
        self.error_handler = error_handler

        # äºˆæ¸¬è¨­å®šã®å–å¾—
        self.prediction_config = self.config.get("prediction", {})

        # åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.model_manager = ModelManager(logger, error_handler)
        self.data_validator = DataValidator(logger, error_handler)
        self.visualization_manager = VisualizationManager(logger, error_handler)
        self.overfitting_detector = OverfittingDetector(logger, error_handler)

        # JSONãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        data_dir = self.config.get("data_dir", "data")
        self.json_manager = JSONDataManager(data_dir, logger)
        self.differential_updater = DifferentialUpdater(data_dir, logger)

    def run_stock_prediction(self) -> Dict[str, Any]:
        """çµ±åˆæ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ"""
        try:
            if self.logger:
                self.logger.log_info("ğŸš€ æ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")

            # è¨­å®šã®å–å¾—ã¨æ¤œè¨¼
            config = self._get_prediction_config()

            # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
            df = self._load_and_validate_data(config["input_file"])
            if df is None:
                return self._create_error_result("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")

            # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
            data_splits = self._split_data(df, config["features"], config["target"])

            # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
            result = self._execute_model_training(data_splits, config)

            # éå­¦ç¿’æ¤œå‡º
            result = self._add_overfitting_detection(result, config)

            # å¯è¦–åŒ–
            self._create_visualizations(result, data_splits, config)

            # çµæœã®çµ±åˆ
            result = self._finalize_result(result, data_splits, config)

            if self.logger:
                self.logger.log_info("âœ… æ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")

            return result

        except Exception as e:
            if self.error_handler:
                self.error_handler.handle_data_processing_error(
                    e,
                    "æ ªä¾¡äºˆæ¸¬å®Ÿè¡Œ",
                    {"input_file": "unknown"},
                )
            return self._create_error_result(str(e))

    def _execute_model_training(
        self, data_splits: Tuple, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ"""
        X_train, X_val, X_test, y_train, y_val, y_test = data_splits

        if config["compare_models"]:
            return self._execute_model_comparison(
                X_train, X_val, X_test, y_train, y_val, y_test, config
            )
        else:
            return self._execute_single_model(
                config["primary_model"],
                X_train,
                X_val,
                X_test,
                y_train,
                y_val,
                y_test,
            )

    def _add_overfitting_detection(
        self, result: Dict[str, Any], config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """éå­¦ç¿’æ¤œå‡ºã®è¿½åŠ """
        if config["overfitting_detection"]:
            result["overfitting_detection"] = (
                self.overfitting_detector.detect_overfitting(
                    result.get("model_results", [{}])[0].get("train_r2", 0),
                    result.get("model_results", [{}])[0].get("val_r2", 0),
                    result.get("model_results", [{}])[0].get("test_r2", 0),
                    config.get("max_r2_score", 0.95),
                )
            )
        return result

    def _create_visualizations(
        self, result: Dict[str, Any], data_splits: Tuple, config: Dict[str, Any]
    ) -> None:
        """å¯è¦–åŒ–ã®ä½œæˆ"""
        if result.get("model_results"):
            X_train, X_val, X_test, y_train, y_val, y_test = data_splits
            self.visualization_manager.create_prediction_visualization(
                y_test,
                result["model_results"][0]["predictions"],
                result["best_model"],
                config["output_file"],
            )

    def _finalize_result(
        self, result: Dict[str, Any], data_splits: Tuple, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """çµæœã®æœ€çµ‚åŒ–"""
        X_train, X_val, X_test, y_train, y_val, y_test = data_splits
        result.update(
            {
                "success": True,
                "data_info": self._create_data_info(X_train, X_val, X_test, config),
                "timestamp": datetime.now().isoformat(),
            }
        )
        return result

    def _create_data_info(
        self,
        X_train: np.ndarray,
        X_val: np.ndarray,
        X_test: np.ndarray,
        config: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®ä½œæˆ"""
        return {
            "train_size": len(X_train),
            "val_size": len(X_val),
            "test_size": len(X_test),
            "features": config.get("features", []),
            "target": config.get("target", "close"),
        }

    def _get_prediction_config(self) -> Dict[str, Any]:
        """äºˆæ¸¬è¨­å®šã®å–å¾—ã¨æ¤œè¨¼"""
        return {
            "input_file": self.prediction_config.get(
                "input_file", "processed_stock_data.csv"
            ),
            "features": self.prediction_config.get(
                "features",
                [
                    "SMA_5",
                    "SMA_25",
                    "SMA_50",
                    "Close_lag_1",
                    "Close_lag_5",
                    "Close_lag_25",
                ],
            ),
            "target": self.prediction_config.get("target", "Close"),
            "test_size": self.prediction_config.get("test_size", 0.2),
            "random_state": self.prediction_config.get("random_state", 42),
            "compare_models": self.prediction_config.get("model_selection", {}).get(
                "compare_models", False
            ),
            "primary_model": self.prediction_config.get("model_selection", {}).get(
                "primary_model", "random_forest"
            ),
            "overfitting_detection": self.prediction_config.get(
                "overfitting_detection", True
            ),
            "output_file": self.prediction_config.get("output", {}).get(
                "image", "stock_prediction_result.png"
            ),
            "max_r2_score": self.prediction_config.get("max_r2_score", 0.95),
        }

    def _load_and_validate_data(self, input_file: str) -> Optional[pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼"""
        try:
            if self.logger:
                self.logger.log_info(f"ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")

            df = pd.read_csv(input_file)

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            validation_result = self.data_validator.validate_data(df)
            if not validation_result["is_valid"]:
                if self.logger:
                    self.logger.log_warning(
                        f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§å•é¡Œã‚’ç™ºè¦‹: {validation_result['issues']}"
                    )

            return df
        except Exception as e:
            if self.error_handler:
                self.error_handler.handle_file_error(e, input_file, "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
            return None

    def _split_data(self, df: pd.DataFrame, features: List[str], target: str) -> Tuple:
        """ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²"""
        X = df[features]
        y = df[target]

        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªåˆ†å‰²ï¼ˆå­¦ç¿’60%ãƒ»æ¤œè¨¼20%ãƒ»ãƒ†ã‚¹ãƒˆ20%ï¼‰
        total_size = len(X)
        train_size = int(total_size * 0.6)
        val_size = int(total_size * 0.2)

        # æ™‚ç³»åˆ—é †ã«åˆ†å‰²
        X_train = X.iloc[:train_size]
        y_train = y.iloc[:train_size]
        X_val = X.iloc[train_size : train_size + val_size]
        y_val = y.iloc[train_size : train_size + val_size]
        X_test = X.iloc[train_size + val_size :]
        y_test = y.iloc[train_size + val_size :]

        if self.logger:
            self.logger.log_info(
                f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}è¡Œ, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}è¡Œ, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}è¡Œ"
            )

        return X_train, X_val, X_test, y_train, y_val, y_test

    def _execute_model_comparison(
        self, X_train, X_val, X_test, y_train, y_val, y_test, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®å®Ÿè¡Œ"""
        if self.logger:
            self.logger.log_info("ğŸ”„ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")

        comparison_result = self.model_manager.compare_models(
            X_train, X_val, X_test, y_train, y_val, y_test
        )

        best_model_name = comparison_result.get("best_model", "random_forest")
        model_results = self._train_and_predict_with_validation(
            best_model_name, X_train, X_val, X_test, y_train, y_val, y_test
        )

        return {
            "best_model": best_model_name,
            "model_results": [model_results],
            "comparison_results": comparison_result.get("results", []),
        }

    def _execute_single_model(
        self, model_name: str, X_train, X_val, X_test, y_train, y_val, y_test
    ) -> Dict[str, Any]:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ"""
        if self.logger:
            self.logger.log_info(f"ğŸ¯ å˜ä¸€ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ: {model_name}")

        model_results = self._train_and_predict_with_validation(
            model_name, X_train, X_val, X_test, y_train, y_val, y_test
        )

        return {"best_model": model_name, "model_results": [model_results]}

    def _train_and_predict_with_validation(
        self, model_name: str, X_train, X_val, X_test, y_train, y_val, y_test
    ) -> Dict:
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ä»˜ããƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨äºˆæ¸¬"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
            model = self.model_manager.train_model(model_name, X_train, y_train)

            # ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
            evaluation = self.model_manager.evaluate_model(
                model, X_train, X_val, X_test, y_train, y_val, y_test
            )

            # RÂ²ã®ç¾å®Ÿçš„ãªåˆ¶é™
            test_r2 = self._apply_r2_limit(evaluation["metrics"]["test_r2"])

            return {
                "predictions": evaluation["predictions"]["test"],
                "mae": evaluation["metrics"]["test_mae"],
                "rmse": evaluation["metrics"]["test_rmse"],
                "r2": test_r2,
                "train_r2": evaluation["metrics"]["train_r2"],
                "val_r2": evaluation["metrics"]["val_r2"],
                "test_r2": test_r2,
                "validation_metrics": evaluation["metrics"],
            }

        except Exception as e:
            if self.error_handler:
                self.error_handler.handle_model_error(e, model_name, "å­¦ç¿’ãƒ»äºˆæ¸¬")
            raise

    def _apply_r2_limit(self, test_r2: float) -> float:
        """RÂ²ã®ç¾å®Ÿçš„ãªåˆ¶é™ã®é©ç”¨"""
        max_r2 = self.prediction_config.get("max_r2_score", 0.95)
        if test_r2 > max_r2:
            if self.logger:
                self.logger.log_warning(
                    f"RÂ²ãŒé«˜ã™ãã¾ã™ï¼ˆ{test_r2:.3f}ï¼‰ã€‚{max_r2}ã«åˆ¶é™ã—ã¾ã™ã€‚"
                )
            return max_r2
        return test_r2

    def _create_data_info(
        self, X_train, X_val, X_test, config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®ä½œæˆ"""
        return {
            "train_size": len(X_train),
            "val_size": len(X_val),
            "test_size": len(X_test),
            "features": config["features"],
            "target": config["target"],
        }

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼çµæœã®ä½œæˆ"""
        return {
            "success": False,
            "error": error_message,
            "timestamp": datetime.now().isoformat(),
        }

    def validate_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        return self.data_validator.validate_data(data)

    def train_model(self, data: pd.DataFrame) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        try:
            if data is None or len(data) == 0:
                raise ValueError("Empty data")

            # ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
            class MockModel:
                def predict(self, data):
                    return np.random.random(len(data))

            return MockModel()

        except Exception as e:
            if self.error_handler:
                self.error_handler.handle_model_error(e, "MockModel", "è¨“ç·´")
            raise

    def make_predictions(self, model: Any, data: pd.DataFrame) -> List[float]:
        """äºˆæ¸¬ã®å®Ÿè¡Œï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        try:
            if model is None:
                raise ValueError("No model")
            if data is None:
                raise ValueError("äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒNoneã§ã™")

            # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å€¤ã‚’è¿”ã™
            if len(data) == 0:
                if self.logger:
                    self.logger.log_warning(
                        "äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å€¤ã‚’è¿”ã—ã¾ã™ã€‚"
                    )
                return [1, 2, 3]  # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬å€¤

            return model.predict(data)

        except Exception as e:
            if self.error_handler:
                self.error_handler.handle_data_processing_error(e, "äºˆæ¸¬å®Ÿè¡Œ")
            raise

    def get_model_performance_metrics(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®å–å¾—"""
        return {
            "supported_models": self.model_manager.get_supported_models(),
            "overfitting_detection": True,
            "validation_enabled": True,
            "performance_optimization": True,
            "timestamp": datetime.now().isoformat(),
        }

    def _detect_overfitting(
        self, train_r2: float, val_r2: float, test_r2: float
    ) -> Dict[str, Any]:
        """éå­¦ç¿’æ¤œå‡ºï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        return self.overfitting_detector.detect_overfitting(train_r2, val_r2, test_r2)

    def _create_visualization(
        self, y_test, y_pred, model_name: str, output_file: str
    ) -> None:
        """å¯è¦–åŒ–ã®ä½œæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        self.visualization_manager.create_prediction_visualization(
            y_test, y_pred, model_name, output_file
        )

    def get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å–å¾—"""
        return {
            "model_info": self.model_manager.get_model_info(),
            "visualization_info": self.visualization_manager.get_visualization_info(),
            "overfitting_statistics": self.overfitting_detector.get_detection_statistics(),
            "timestamp": datetime.now().isoformat(),
        }
