#!/usr/bin/env python3
"""
çµ±åˆJ-Quantsã‚·ã‚¹ãƒ†ãƒ 
å®Œå…¨ã«çµ±åˆã•ã‚ŒãŸã€ã‚»ã‚­ãƒ¥ã‚¢ã§å …ç‰¢ãªJ-Quants APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import logging
import requests
import pandas as pd
import numpy as np
import time
import re
import traceback
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from dotenv import load_dotenv
from unified_config_loader import get_unified_config
from unified_error_handler import get_unified_error_handler
from model_factory import ModelFactory, ModelEvaluator, ModelComparator
from technical_indicators import TechnicalIndicators, get_enhanced_features_list
from data_validator import DataValidator
from unified_error_logging_system import (
    get_unified_error_logging_system,
    ErrorCategory,
    LogCategory,
    configure_global_logging,
)
from type_safe_validator import TypeSafeValidator
from font_config import setup_japanese_font


class UnifiedJQuantsSystem:
    """çµ±åˆJ-Quantsã‚·ã‚¹ãƒ†ãƒ  - å˜ä¸€è²¬ä»»åŸå‰‡ã«åŸºã¥ãå®Œå…¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.logger = logging.getLogger(__name__)
        self.error_count = 0
        self.sensitive_keys = ["password", "token", "key", "secret", "auth", "email"]

        # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
        load_dotenv()

        # çµ±åˆè¨­å®šã®èª­ã¿è¾¼ã¿
        try:
            self.config_loader = get_unified_config()
            self.jquants_config = self.config_loader.get_jquants_config()
            self.data_fetch_config = self.config_loader.get_data_fetch_config()
            self.preprocessing_config = self.config_loader.get_preprocessing_config()
            self.prediction_config = self.config_loader.get_prediction_config()

            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ã‚°è¨­å®šã®é©ç”¨
            configure_global_logging(self.config_loader.config)
        except Exception as e:
            self.logger.error(f"è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        # èªè¨¼æƒ…å ±ã®å®‰å…¨ãªå–å¾—
        self._load_credentials()

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.timeout = self.data_fetch_config.get("timeout", 30)

        # èªè¨¼çŠ¶æ…‹
        self.refresh_token = None
        self.id_token = None
        self.token_expires_at = None

        # æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.model_factory = ModelFactory()
        self.model_evaluator = ModelEvaluator()
        self.model_comparator = ModelComparator()

        # æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å™¨ã®åˆæœŸåŒ–
        self.technical_indicators = TechnicalIndicators()

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
        self.data_validator = DataValidator()

        # å‹å®‰å…¨ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self.type_safe_validator = TypeSafeValidator(strict_mode=True)

        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        setup_japanese_font()

        # çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self.error_logger = get_unified_error_logging_system(
            "UnifiedJQuantsSystem", self.config_loader.config
        )

        self.logger.info("âœ… çµ±åˆJ-Quantsã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def _load_credentials(self) -> None:
        """èªè¨¼æƒ…å ±ã®å®‰å…¨ãªèª­ã¿è¾¼ã¿"""
        try:
            self.email = os.getenv("JQUANTS_EMAIL")
            self.password = os.getenv("JQUANTS_PASSWORD")

            # èªè¨¼æƒ…å ±ã®æ¤œè¨¼ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯ãƒ­ã‚°ã«å‡ºåŠ›ã—ãªã„ï¼‰
            if not self.email or not self.password:
                error_msg = "èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
                masked_context = {
                    "email_set": bool(self.email),
                    "password_set": bool(self.password),
                    "env_file_exists": os.path.exists(".env"),
                }
                self._log_error(
                    ValueError(error_msg),
                    "èªè¨¼æƒ…å ±æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
                    masked_context,
                )
                self.logger.error(
                    "âŒ ç’°å¢ƒå¤‰æ•° JQUANTS_EMAIL ã¨ JQUANTS_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
                )
                self.logger.error(
                    "ğŸ’¡ .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
                )
                raise ValueError(error_msg)

            self.logger.info("âœ… èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿å®Œäº†")

        except Exception as e:
            self._log_error(e, "èªè¨¼æƒ…å ±èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")
            raise

    def _sanitize_message(self, message: str) -> str:
        """æ©Ÿå¯†æƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°"""
        sensitive_patterns = [
            r'password["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
            r'token["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
            r'key["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
            r'secret["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
            r'auth["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
            r'email["\']?\s*[:=]\s*["\']?[^"\']+["\']?',
        ]

        sanitized = message
        for pattern in sensitive_patterns:
            sanitized = re.sub(
                pattern, r"\1***MASKED***", sanitized, flags=re.IGNORECASE
            )
        return sanitized

    def _mask_sensitive_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¹ã‚­ãƒ³ã‚°"""
        masked_data = data.copy()

        for key, value in masked_data.items():
            if any(
                sensitive_key in key.lower() for sensitive_key in self.sensitive_keys
            ):
                if isinstance(value, str) and len(value) > 4:
                    masked_data[key] = value[:2] + "*" * (len(value) - 4) + value[-2:]
                else:
                    masked_data[key] = "***"

        return masked_data

    def _log_error(
        self,
        error: Exception,
        context: str = "",
        additional_info: Dict[str, Any] = None,
        include_traceback: bool = True,
    ):
        """ã‚»ã‚­ãƒ¥ã‚¢ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›"""
        self.error_count += 1

        # æ©Ÿå¯†æƒ…å ±ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°
        sanitized_context = self._sanitize_message(context)
        sanitized_error_msg = self._sanitize_message(str(error))

        # è¿½åŠ æƒ…å ±ã®ãƒã‚¹ã‚­ãƒ³ã‚°
        masked_info = None
        if additional_info:
            masked_info = self._mask_sensitive_data(additional_info)

        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®å‡ºåŠ›
        self.logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ #{self.error_count}: {sanitized_context}")
        self.logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {sanitized_error_msg}")

        if masked_info:
            self.logger.error(f"è¿½åŠ æƒ…å ±: {masked_info}")

        if include_traceback:
            traceback_str = self._sanitize_message(
                "".join(
                    traceback.format_exception(type(error), error, error.__traceback__)
                )
            )
            self.logger.error(f"ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback_str}")

    def _handle_api_error(
        self,
        error: Exception,
        api_name: str,
        url: str,
        status_code: int = None,
    ):
        """APIã‚¨ãƒ©ãƒ¼ã®å‡¦ç†ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨ï¼‰"""
        self.error_logger.handle_api_error(error, api_name, url, status_code)

    def _handle_file_error(
        self,
        error: Exception,
        file_path: str,
        operation: str,
    ):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨ï¼‰"""
        self.error_logger.handle_file_error(error, file_path, operation)

    def get_refresh_token(self) -> str:
        """ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—"""
        self.logger.info("ğŸ”‘ ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")

        try:
            auth_url = "https://api.jquants.com/v1/token/auth_user"
            auth_payload = {"mailaddress": self.email, "password": self.password}

            response = self.session.post(auth_url, json=auth_payload, timeout=30)
            response.raise_for_status()

            auth_data = response.json()

            if "refreshToken" not in auth_data:
                error_msg = "ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                self._handle_api_error(
                    ValueError(error_msg),
                    "J-Quants API",
                    auth_url,
                    response.status_code,
                )
                raise ValueError(error_msg)

            self.refresh_token = auth_data["refreshToken"]
            self.logger.info("âœ… ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return self.refresh_token

        except requests.exceptions.RequestException as e:
            self._handle_api_error(e, "J-Quants API", auth_url)
            raise
        except Exception as e:
            self._log_error(e, "ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼")
            raise

    def get_id_token(self) -> str:
        """IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—"""
        self.logger.info("ğŸ« IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ä¸­...")

        try:
            if not self.refresh_token:
                self.get_refresh_token()

            id_token_url = "https://api.jquants.com/v1/token/auth_refresh"
            id_token_params = {"refreshtoken": self.refresh_token}

            response = self.session.post(
                id_token_url, params=id_token_params, timeout=30
            )
            response.raise_for_status()

            id_token_data = response.json()

            if "idToken" not in id_token_data:
                error_msg = "IDãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
                self._handle_api_error(
                    ValueError(error_msg),
                    "J-Quants API",
                    id_token_url,
                    response.status_code,
                )
                raise ValueError(error_msg)

            self.id_token = id_token_data["idToken"]
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®šï¼ˆé€šå¸¸24æ™‚é–“ï¼‰
            self.token_expires_at = datetime.now() + timedelta(hours=23)
            self.logger.info("âœ… IDãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return self.id_token

        except requests.exceptions.RequestException as e:
            self._handle_api_error(e, "J-Quants API", id_token_url)
            raise
        except Exception as e:
            self._log_error(e, "IDãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼")
            raise

    def ensure_valid_token(self) -> str:
        """æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºä¿"""
        if (
            self.id_token is None
            or self.token_expires_at is None
            or datetime.now() >= self.token_expires_at
        ):
            self.logger.info("ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒ³ã®æ›´æ–°ãŒå¿…è¦ã§ã™")
            self.get_id_token()

        return self.id_token

    def get_auth_headers(self) -> Dict[str, str]:
        """èªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã®å–å¾—"""
        token = self.ensure_valid_token()
        return {"Authorization": f"Bearer {token}"}

    def _make_request_with_retry(
        self, method: str, url: str, **kwargs
    ) -> requests.Response:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        max_retries = self.data_fetch_config.get("max_retries", 3)
        retry_interval = self.data_fetch_config.get("retry_interval", 5)

        for attempt in range(max_retries + 1):
            try:
                self.logger.info(
                    f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {method} {url}"
                )
                response = self.session.request(method, url, **kwargs)

                if response.status_code == 200:
                    self.logger.info(f"âœ… APIãƒªã‚¯ã‚¨ã‚¹ãƒˆæˆåŠŸ: {response.status_code}")
                    return response
                else:
                    # HTTPã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒ­ã‚°
                    self._handle_api_error(
                        requests.exceptions.HTTPError(
                            f"HTTP {response.status_code}: {response.text}"
                        ),
                        "J-Quants API",
                        url,
                        response.status_code,
                    )
                    self.logger.warning(f"âš ï¸ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {response.status_code}")

            except requests.exceptions.Timeout as e:
                if attempt < max_retries:
                    self.logger.warning(
                        f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt + 1}/{max_retries + 1})"
                    )
                    self.logger.info(f"â³ {retry_interval}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(retry_interval)
                    continue
                else:
                    self._handle_api_error(e, "J-Quants API", url)
                    raise

            except requests.exceptions.ConnectionError as e:
                if attempt < max_retries:
                    self.logger.warning(
                        f"ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries + 1})"
                    )
                    self.logger.info(f"â³ {retry_interval}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(retry_interval)
                    continue
                else:
                    self._handle_api_error(e, "J-Quants API", url)
                    raise

            except requests.exceptions.RequestException as e:
                self._handle_api_error(e, "J-Quants API", url)
                if attempt < max_retries:
                    self.logger.info(f"â³ {retry_interval}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(retry_interval)
                    continue
                else:
                    raise

        # å…¨ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ
        final_error = Exception(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒ{max_retries + 1}å›å¤±æ•—ã—ã¾ã—ãŸ")
        self._log_error(
            final_error,
            "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ€çµ‚å¤±æ•—",
            {
                "method": method,
                "url": url,
                "max_retries": max_retries,
                "retry_interval": retry_interval,
            },
        )
        raise final_error

    def _validate_stock_data(self, data: Dict[str, Any]) -> bool:
        """å–å¾—ã—ãŸæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        self.logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")

        # åŸºæœ¬çš„ãªæ§‹é€ ãƒã‚§ãƒƒã‚¯
        if not isinstance(data, dict):
            self.logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            return False

        if "daily_quotes" not in data:
            self.logger.error("âŒ daily_quotesã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        quotes = data["daily_quotes"]
        if not isinstance(quotes, list):
            self.logger.error("âŒ daily_quotesãŒãƒªã‚¹ãƒˆå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
            return False

        if len(quotes) == 0:
            self.logger.warning("âš ï¸ å–å¾—ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return False

        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        required_fields = ["Code", "Date", "Open", "High", "Low", "Close", "Volume"]
        sample_quote = quotes[0]
        missing_fields = [
            field for field in required_fields if field not in sample_quote
        ]

        if missing_fields:
            self.logger.error(f"âŒ å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³: {missing_fields}")
            return False

        # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼
        for i, quote in enumerate(quotes[:5]):  # æœ€åˆã®5ä»¶ã‚’ã‚µãƒ³ãƒ—ãƒ«ãƒã‚§ãƒƒã‚¯
            try:
                float(quote.get("Close", 0))
                float(quote.get("Volume", 0))
                pd.to_datetime(quote.get("Date"))
            except (ValueError, TypeError) as e:
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‹ã‚¨ãƒ©ãƒ¼ (è¡Œ{i}): {e}")
                return False

        self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†: {len(quotes)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª")
        return True

    def fetch_stock_data(self, target_date: str) -> pd.DataFrame:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—"""
        self.logger.info(f"ğŸ“Š æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹: {target_date}")

        try:
            # èªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã®å–å¾—
            headers = self.get_auth_headers()

            # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            price_url = f"{self.jquants_config.get('base_url', 'https://api.jquants.com/v1')}/prices/daily_quotes"
            params = {"date": target_date}

            response = self._make_request_with_retry(
                "GET", price_url, headers=headers, params=params
            )
            data = response.json()

            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if not self._validate_stock_data(data):
                raise ValueError("å–å¾—ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")

            # DataFrameã«å¤‰æ›
            df = pd.DataFrame(data["daily_quotes"])
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}ä»¶")

            return df

        except Exception as e:
            self._log_error(e, "æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼")
            raise

    def save_data(self, df: pd.DataFrame, output_file: str) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        self.logger.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­: {output_file}")

        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åŸºæœ¬æ¤œè¨¼
            if df is None or df.empty:
                raise ValueError("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")

            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªãƒ»ä½œæˆ
            output_dir = os.path.dirname(output_file)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
                self.logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {output_dir}")

            df.to_csv(output_file, index=False)
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {output_file} ({len(df)}ä»¶)")

        except Exception as e:
            self._handle_file_error(e, output_file, "write")
            raise

    def preprocess_data(self, input_file: str, output_file: str = None) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é–‹å§‹")

        if output_file is None:
            output_file = self.preprocessing_config.get(
                "output_file", "processed_stock_data.csv"
            )

        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            self.logger.info("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
            df = self._load_and_clean_data(input_file)

            # 2. åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            self.logger.info("ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
            df = self._engineer_basic_features(df)

            # 3. é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™ã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            self.logger.info("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—3: é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—")
            df = self._engineer_advanced_features(df)

            # 4. ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼
            self.logger.info("ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—4: ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼")
            df, available_features = self._feature_selection_and_validation(df)

            # 5. æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
            self.logger.info("ğŸ§¹ ã‚¹ãƒ†ãƒƒãƒ—5: æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†")
            initial_rows = len(df)
            df = df.dropna()
            final_rows = len(df)
            dropped_rows = initial_rows - final_rows

            if dropped_rows > 0:
                self.logger.info(
                    f"ğŸ—‘ï¸ æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤: {initial_rows} -> {final_rows} è¡Œ ({dropped_rows} è¡Œå‰Šé™¤)"
                )

            # 6. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®å®Ÿè¡Œ
            self.logger.info("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼")
            if not self._validate_processed_data(df):
                self.logger.warning(
                    "âš ï¸ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™"
                )

            # 7. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            self.logger.info("ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
            df.to_csv(output_file, index=False)
            self.logger.info(f"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")

            # 8. æœ€çµ‚çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
            self.logger.info("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            self.logger.info(f"  ğŸ“ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            self.logger.info(
                f"  ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['Date'].min()} ï½ {df['Date'].max()}"
            )
            self.logger.info(f"  ğŸ“ˆ ç‰¹å¾´é‡æ•°: {len(df.columns)}å€‹")
            self.logger.info(f"  ğŸ¯ æ¨å¥¨ç‰¹å¾´é‡: {len(available_features)}å€‹")

            return df

        except Exception as e:
            error_handler = get_unified_error_handler("preprocess_data")
            error_handler.log_error(
                e,
                "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼",
                {
                    "input_file": input_file,
                    "output_file": output_file,
                    "data_shape": (
                        df.shape if "df" in locals() and df is not None else None
                    ),
                },
            )
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def predict_stock_prices(
        self, input_file: str, output_image: str = None
    ) -> Dict[str, Any]:
        """æ ªä¾¡äºˆæ¸¬ã®å®Ÿè¡Œï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸ¯ æ ªä¾¡äºˆæ¸¬ã‚’é–‹å§‹")

        if output_image is None:
            output_image = self.prediction_config.get(
                "output_image", "stock_prediction_result.png"
            )

        try:
            # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            self.logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")
            df = pd.read_csv(input_file)

            # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®è¨­å®š
            features = self.prediction_config.get(
                "features",
                [
                    "SMA_5",
                    "SMA_25",
                    "SMA_50",
                    "Close_lag_1",
                    "Close_lag_5",
                    "Close_lag_25",
                ],
            )
            target = self.prediction_config.get("target", "Close")

            # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
            available_features = [col for col in features if col in df.columns]
            missing_features = [col for col in features if col not in df.columns]

            if missing_features:
                self.logger.warning(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡: {missing_features}")

            X = df[available_features]
            y = df[target]

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
            from sklearn.model_selection import train_test_split

            test_size = self.prediction_config.get("test_size", 0.2)
            random_state = self.prediction_config.get("random_state", 42)

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )

            self.logger.info(
                f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}è¡Œ, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}è¡Œ"
            )

            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®å–å¾—
            model_selection = self.prediction_config.get("model_selection", {})
            compare_models = model_selection.get("compare_models", False)
            primary_model = model_selection.get("primary_model", "random_forest")

            if compare_models:
                self.logger.info("ğŸ”„ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")
                models_config = self.prediction_config.get("models", {})

                if not models_config:
                    self.logger.warning(
                        "è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                    )
                    from model_factory import get_default_models_config

                    models_config = get_default_models_config()

                # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿè¡Œ
                comparison_results = self.model_comparator.compare_models(
                    models_config, X_train, X_test, y_train, y_test, available_features
                )

                if not comparison_results.empty:
                    self.logger.info("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ:")
                    for idx, row in comparison_results.iterrows():
                        self.logger.info(
                            f"{row['model_name']:<15} | MAE: {row['mae']:.4f} | RMSE: {row['rmse']:.4f} | RÂ²: {row['r2']:.4f}"
                        )

                    # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
                    best_model_name = comparison_results.iloc[0]["model_name"]
                    self.logger.info(f"ğŸ† æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«: {best_model_name}")

                    # æ¯”è¼ƒçµæœã‚’CSVã«ä¿å­˜
                    comparison_csv = self.prediction_config.get(
                        "comparison_csv", "model_comparison_results.csv"
                    )
                    comparison_results.to_csv(comparison_csv, index=False)
                    self.logger.info(f"ğŸ“ æ¯”è¼ƒçµæœã‚’ '{comparison_csv}' ã«ä¿å­˜ã—ã¾ã—ãŸ")

                    # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«ã§å†å­¦ç¿’
                    best_config = models_config[best_model_name]
                    model = self.model_factory.create_model(
                        best_config["type"], best_config.get("params", {})
                    )
                else:
                    self.logger.error(
                        "âŒ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã§æœ‰åŠ¹ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                    )
                    model = self.model_factory.create_model("random_forest")
                    best_model_name = "random_forest"
            else:
                self.logger.info(f"ğŸ¯ å˜ä¸€ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ: {primary_model}")
                models_config = self.prediction_config.get("models", {})
                if primary_model in models_config:
                    model_config = models_config[primary_model]
                    model = self.model_factory.create_model(
                        model_config["type"], model_config.get("params", {})
                    )
                else:
                    self.logger.warning(
                        f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ« '{primary_model}' ã®è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                    )
                    model = self.model_factory.create_model(primary_model)

                best_model_name = primary_model

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            self.logger.info(f"ğŸ“š ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­: {best_model_name}")
            model.fit(X_train, y_train)

            # äºˆæ¸¬ã¨è©•ä¾¡
            y_pred = model.predict(X_test)
            metrics = self.model_evaluator.evaluate_model(model, X_test, y_test, y_pred)

            self.logger.info(f"ğŸ“Š æœ€çµ‚è©•ä¾¡çµæœ:")
            self.logger.info(f"  MAE (å¹³å‡çµ¶å¯¾èª¤å·®): {metrics['mae']:.4f}")
            self.logger.info(f"  RMSE (å¹³å‡å¹³æ–¹æ ¹èª¤å·®): {metrics['rmse']:.4f}")
            self.logger.info(f"  RÂ² (æ±ºå®šä¿‚æ•°): {metrics['r2']:.4f}")

            # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
            feature_importance_df = self.model_evaluator.get_feature_importance(
                model, available_features
            )
            if not feature_importance_df.empty:
                self.logger.info("ğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦:")
                for idx, row in feature_importance_df.iterrows():
                    self.logger.info(f"  {row['feature']}: {row['importance']:.4f}")

            # çµæœã®å¯è¦–åŒ–
            self.logger.info(f"ğŸ¨ çµæœã‚’ '{output_image}' ã«ä¿å­˜ä¸­...")
            self._create_prediction_visualization(
                y_test, y_pred, feature_importance_df, best_model_name, output_image
            )

            return {
                "model_name": best_model_name,
                "metrics": metrics,
                "feature_importance": feature_importance_df,
                "output_image": output_image,
            }

        except Exception as e:
            error_handler = get_unified_error_handler("predict_stock_prices")
            error_handler.log_error(
                e,
                "æ ªä¾¡äºˆæ¸¬ã‚¨ãƒ©ãƒ¼",
                {"input_file": input_file, "output_image": output_image},
            )
            self.logger.error(f"âŒ æ ªä¾¡äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®å–å¾—"""
        return {
            "system_name": "çµ±åˆJ-Quantsã‚·ã‚¹ãƒ†ãƒ ",
            "version": "2.0.0",
            "error_count": self.error_count,
            "has_valid_token": self.id_token is not None
            and (
                self.token_expires_at is None or datetime.now() < self.token_expires_at
            ),
            "token_expires_at": (
                self.token_expires_at.isoformat() if self.token_expires_at else None
            ),
            "config_loaded": bool(self.jquants_config and self.data_fetch_config),
            "features": [
                "ãƒ‡ãƒ¼ã‚¿å–å¾—",
                "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†",
                "æ ªä¾¡äºˆæ¸¬",
                "çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°",
            ],
        }

    def _load_and_clean_data(self, input_file: str) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        if not os.path.exists(input_file):
            raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")

        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡ºï¼‰
        encodings = ["utf-8", "shift_jis", "cp932", "utf-8-sig"]
        df = None
        successful_encoding = None

        for encoding in encodings:
            try:
                df = pd.read_csv(input_file, encoding=encoding)
                successful_encoding = encoding
                self.logger.info(
                    f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})"
                )
                break
            except UnicodeDecodeError:
                continue

        if df is None:
            raise ValueError("ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")

        if df.empty:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")

        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å¤‰æ›
        if "Date" in df.columns:
            df["Date"] = pd.to_datetime(df["Date"])

        # æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å¤‰æ›
        numeric_columns = ["Open", "High", "Low", "Close", "Volume"]
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

        # æ¬ æå€¤ã®å‡¦ç†
        df = self.type_safe_validator.safe_nan_handling(df, strategy="forward_fill")
        df = df.dropna()

        # é‡è¤‡è¡Œã®å‰Šé™¤
        df = df.drop_duplicates()

        self.logger.info(f"ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        return df

    def _engineer_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸ”§ åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é–‹å§‹")

        # åŸºæœ¬çš„ãªç§»å‹•å¹³å‡
        basic_sma_windows = self.preprocessing_config.get(
            "sma_windows", [5, 10, 25, 50]
        )
        for window in basic_sma_windows:
            if f"SMA_{window}" not in df.columns:
                df[f"SMA_{window}"] = df["Close"].rolling(window=window).mean()

        # åŸºæœ¬çš„ãªãƒ©ã‚°ç‰¹å¾´é‡
        lag_days = self.preprocessing_config.get("lag_days", [1, 3, 5])
        for lag in lag_days:
            df[f"Close_lag_{lag}"] = df["Close"].shift(lag)

        self.logger.info("âœ… åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
        return df

    def _engineer_advanced_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™ã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸš€ é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚’é–‹å§‹")

        try:
            # æŠ€è¡“æŒ‡æ¨™è¨­å®šã‚’å–å¾—
            technical_config = self.preprocessing_config.get(
                "technical_indicators", self.technical_indicators._get_default_config()
            )

            # å…¨ã¦ã®æŠ€è¡“æŒ‡æ¨™ã‚’è¨ˆç®—
            enhanced_df = self.technical_indicators.calculate_all_indicators(
                df, technical_config
            )

            # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸæŒ‡æ¨™ã‚’ãƒ­ã‚°å‡ºåŠ›
            original_columns = set(df.columns)
            new_columns = [
                col for col in enhanced_df.columns if col not in original_columns
            ]

            self.logger.info(f"ğŸ“ˆ è¿½åŠ ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™: {len(new_columns)}å€‹")
            return enhanced_df

        except Exception as e:
            self.logger.error(f"âŒ æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.warning("ğŸ”„ åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ã§ç¶šè¡Œã—ã¾ã™")
            return df

    def _feature_selection_and_validation(self, df: pd.DataFrame) -> tuple:
        """ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸ¯ ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼ã‚’é–‹å§‹")

        # åˆ©ç”¨å¯èƒ½ãªæ‹¡å¼µç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        enhanced_features = get_enhanced_features_list()

        # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
        available_features = [col for col in enhanced_features if col in df.columns]
        missing_features = [col for col in enhanced_features if col not in df.columns]

        self.logger.info(
            f"âœ… åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡: {len(available_features)}/{len(enhanced_features)}"
        )

        if missing_features:
            self.logger.warning(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡: {len(missing_features)}å€‹")

        # å‹å®‰å…¨ãªç„¡é™å€¤ãƒ»ç•°å¸¸å€¤ã®ãƒã‚§ãƒƒã‚¯
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        validation_result = self.type_safe_validator.validate_numeric_columns(
            df, numeric_columns
        )

        if not validation_result["is_valid"]:
            self.logger.error("âŒ æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã«å¤±æ•—")
            for error in validation_result["errors"]:
                self.logger.error(f"  - {error}")
            raise ValueError("æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼")

        # ç„¡é™å€¤ã®å®‰å…¨ãªå‡¦ç†
        inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
        if inf_count > 0:
            self.logger.warning(f"âš ï¸ ç„¡é™å€¤ã‚’æ¤œå‡º: {inf_count}å€‹")
            df = df.replace([np.inf, -np.inf], np.nan)
            df = self.type_safe_validator.safe_nan_handling(df, strategy="forward_fill")

        return df, available_features

    def _validate_processed_data(self, df: pd.DataFrame) -> bool:
        """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆçµ±åˆç‰ˆï¼‰"""
        self.logger.info("ğŸ” å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã‚’é–‹å§‹")

        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®å®Ÿè¡Œ
            validation_results = self.data_validator.validate_stock_data(df)

            # æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨è¡¨ç¤º
            report = self.data_validator.generate_validation_report(validation_results)
            self.logger.info(f"\n{report}")

            if not validation_results["is_valid"]:
                self.logger.error("âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False

            self.logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            return True

        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _create_prediction_visualization(
        self, y_test, y_pred, feature_importance_df, model_name: str, output_image: str
    ):
        """äºˆæ¸¬çµæœã®å¯è¦–åŒ–ï¼ˆçµ±åˆç‰ˆï¼‰"""
        plt.figure(figsize=(15, 8))

        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
        plt.subplot(2, 2, 1)
        plt.plot(
            y_test.values, label="å®Ÿéš›ã®æ ªä¾¡", color="blue", alpha=0.7, linewidth=2
        )
        plt.plot(y_pred, label="äºˆæ¸¬æ ªä¾¡", color="red", alpha=0.7, linewidth=2)
        plt.legend()
        plt.title(f"æ ªä¾¡äºˆæ¸¬çµæœ ({model_name})")
        plt.xlabel("ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        plt.ylabel("æ ªä¾¡")
        plt.grid(True, alpha=0.3)

        # æ•£å¸ƒå›³
        plt.subplot(2, 2, 2)
        plt.scatter(y_test, y_pred, alpha=0.6, color="green")
        plt.plot(
            [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2
        )
        plt.xlabel("å®Ÿéš›ã®æ ªä¾¡")
        plt.ylabel("äºˆæ¸¬æ ªä¾¡")
        plt.title("å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤")
        plt.grid(True, alpha=0.3)

        # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        plt.subplot(2, 2, 3)
        residuals = y_test - y_pred
        plt.scatter(y_pred, residuals, alpha=0.6, color="orange")
        plt.axhline(y=0, color="r", linestyle="--")
        plt.xlabel("äºˆæ¸¬æ ªä¾¡")
        plt.ylabel("æ®‹å·®")
        plt.title("æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ")
        plt.grid(True, alpha=0.3)

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10å€‹ï¼‰
        if not feature_importance_df.empty:
            plt.subplot(2, 2, 4)
            top_features = feature_importance_df.head(10)
            plt.barh(
                range(len(top_features)), top_features["importance"], color="skyblue"
            )
            plt.yticks(range(len(top_features)), top_features["feature"])
            plt.xlabel("é‡è¦åº¦")
            plt.title("ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)")
            plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_image, dpi=300, bbox_inches="tight")
        plt.close()  # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ã


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç† - å®Œå…¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    try:
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        system = UnifiedJQuantsSystem()

        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®è¡¨ç¤º
        status = system.get_system_status()
        print(f"ğŸš€ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {status}")

        # å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        from datetime import datetime

        today = datetime.now().strftime("%Y%m%d")

        print(f"\nğŸ”„ å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹...")

        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å–å¾—
        print(f"\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾— ({today})")
        raw_data = system.fetch_stock_data(today)
        raw_output_file = f"stock_data_{today}.csv"
        system.save_data(raw_data, raw_output_file)
        print(f"âœ… ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {raw_output_file}")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        print(f"\nğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†")
        processed_data = system.preprocess_data(raw_output_file)
        processed_output_file = f"processed_stock_data_{today}.csv"
        processed_data.to_csv(processed_output_file, index=False)
        print(f"âœ… å‰å‡¦ç†å®Œäº†: {processed_output_file}")

        # ã‚¹ãƒ†ãƒƒãƒ—3: æ ªä¾¡äºˆæ¸¬
        print(f"\nğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—3: æ ªä¾¡äºˆæ¸¬")
        prediction_result = system.predict_stock_prices(processed_output_file)
        print(f"âœ… äºˆæ¸¬å®Œäº†: {prediction_result['output_image']}")
        print(
            f"ğŸ“Š äºˆæ¸¬ç²¾åº¦: MAE={prediction_result['metrics']['mae']:.4f}, RÂ²={prediction_result['metrics']['r2']:.4f}"
        )

        print(f"\nğŸ‰ å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_output_file}")
        print(f"  - å‰å‡¦ç†æ¸ˆã¿: {processed_output_file}")
        print(f"  - äºˆæ¸¬çµæœ: {prediction_result['output_image']}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
