#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
30-50%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹é«˜åº¦ãªæœ€é©åŒ–æ©Ÿèƒ½
"""

import pandas as pd
import numpy as np
import psutil
import gc
import logging
import weakref
import tracemalloc
from typing import Dict, List, Optional, Any, Callable, Union
from contextlib import contextmanager
import time
import hashlib
import joblib
import os
from dataclasses import dataclass
from functools import lru_cache
import threading
from concurrent.futures import ThreadPoolExecutor
import sys

logger = logging.getLogger(__name__)


@dataclass
class MemoryStats:
    """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆæƒ…å ±"""
    current_mb: float
    peak_mb: float
    available_mb: float
    usage_percent: float
    gc_count: int
    objects_count: int


class EnhancedMemoryOptimizer:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, memory_limit_mb: int = 1024, aggressive_mode: bool = True):
        self.memory_limit_mb = memory_limit_mb
        self.aggressive_mode = aggressive_mode
        self.process = psutil.Process()
        self.memory_history = []
        self.optimization_stats = {
            "total_optimizations": 0,
            "memory_saved_mb": 0.0,
            "compression_ratio": 0.0
        }
        
        # ãƒ¡ãƒ¢ãƒªç›£è¦–ã®é–‹å§‹
        tracemalloc.start()
        
        # å¼±å‚ç…§ã«ã‚ˆã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¿½è·¡
        self.tracked_objects = weakref.WeakSet()
        
        logger.info(f"ğŸ”§ å¼·åŒ–ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   - ãƒ¡ãƒ¢ãƒªåˆ¶é™: {memory_limit_mb}MB")
        logger.info(f"   - ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if aggressive_mode else 'ç„¡åŠ¹'}")

    def get_memory_stats(self) -> MemoryStats:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’å–å¾—"""
        memory_info = self.process.memory_info()
        virtual_memory = psutil.virtual_memory()
        
        return MemoryStats(
            current_mb=memory_info.rss / 1024 / 1024,
            peak_mb=memory_info.peak_rss / 1024 / 1024,
            available_mb=virtual_memory.available / 1024 / 1024,
            usage_percent=virtual_memory.percent,
            gc_count=gc.get_count()[0],
            objects_count=len(gc.get_objects())
        )

    def optimize_dataframe_aggressive(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–"""
        logger.info("ğŸš€ ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–é–‹å§‹")
        
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        original_dtypes = df.dtypes.copy()
        
        # 1. ãƒ‡ãƒ¼ã‚¿å‹ã®æœ€é©åŒ–ï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
        df_optimized = self._optimize_dtypes_aggressive(df)
        
        # 2. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–
        df_optimized = self._optimize_categorical_data(df_optimized)
        
        # 3. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®é‡å­åŒ–
        df_optimized = self._quantize_numeric_data(df_optimized)
        
        # 4. æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–
        df_optimized = self._optimize_string_data(df_optimized)
        
        # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–
        df_optimized = self._optimize_index(df_optimized)
        
        optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024
        reduction = (original_memory - optimized_memory) / original_memory * 100
        
        # çµ±è¨ˆæ›´æ–°
        self.optimization_stats["total_optimizations"] += 1
        self.optimization_stats["memory_saved_mb"] += (original_memory - optimized_memory)
        self.optimization_stats["compression_ratio"] = optimized_memory / original_memory
        
        logger.info(f"ğŸ’¾ ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–æœ€é©åŒ–å®Œäº†:")
        logger.info(f"   - å…ƒãƒ¡ãƒ¢ãƒª: {original_memory:.1f}MB")
        logger.info(f"   - æœ€é©åŒ–å¾Œ: {optimized_memory:.1f}MB")
        logger.info(f"   - å‰Šæ¸›ç‡: {reduction:.1f}%")
        logger.info(f"   - åœ§ç¸®æ¯”: {self.optimization_stats['compression_ratio']:.2f}")
        
        return df_optimized

    def _optimize_dtypes_aggressive(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãªãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–"""
        df_optimized = df.copy()
        
        for col in df_optimized.columns:
            col_type = df_optimized[col].dtype
            
            if col_type == "object":
                # æ–‡å­—åˆ—ã®æœ€é©åŒ–
                df_optimized[col] = df_optimized[col].astype("string")
            elif col_type in ["int64", "int32"]:
                # æ•´æ•°ã®æœ€é©åŒ–
                c_min = df_optimized[col].min()
                c_max = df_optimized[col].max()
                
                if c_min >= 0:  # ç¬¦å·ãªã—æ•´æ•°
                    if c_max < 255:
                        df_optimized[col] = df_optimized[col].astype("uint8")
                    elif c_max < 65535:
                        df_optimized[col] = df_optimized[col].astype("uint16")
                    elif c_max < 4294967295:
                        df_optimized[col] = df_optimized[col].astype("uint32")
                else:  # ç¬¦å·ä»˜ãæ•´æ•°
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df_optimized[col] = df_optimized[col].astype("int8")
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df_optimized[col] = df_optimized[col].astype("int16")
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df_optimized[col] = df_optimized[col].astype("int32")
            elif col_type in ["float64", "float32"]:
                # æµ®å‹•å°æ•°ç‚¹ã®æœ€é©åŒ–
                c_min = df_optimized[col].min()
                c_max = df_optimized[col].max()
                
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df_optimized[col] = df_optimized[col].astype("float16")
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df_optimized[col] = df_optimized[col].astype("float32")
        
        return df_optimized

    def _optimize_categorical_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–"""
        df_optimized = df.copy()
        
        for col in df_optimized.columns:
            if df_optimized[col].dtype == "object":
                # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’ãƒã‚§ãƒƒã‚¯
                unique_ratio = df_optimized[col].nunique() / len(df_optimized)
                
                if unique_ratio < 0.5:  # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒä½ã„å ´åˆ
                    df_optimized[col] = df_optimized[col].astype("category")
                    logger.debug(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ›: {col} (ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {unique_ratio:.2f})")
        
        return df_optimized

    def _quantize_numeric_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®é‡å­åŒ–"""
        df_optimized = df.copy()
        
        for col in df_optimized.select_dtypes(include=[np.number]).columns:
            if df_optimized[col].dtype in ["float64", "float32"]:
                # ç²¾åº¦ã‚’ä¸‹ã’ã¦é‡å­åŒ–
                if self.aggressive_mode:
                    # ã‚ˆã‚Šç©æ¥µçš„ãªé‡å­åŒ–
                    df_optimized[col] = pd.qcut(
                        df_optimized[col], 
                        q=100, 
                        duplicates='drop'
                    ).cat.codes.astype("int8")
                else:
                    # æ¨™æº–çš„ãªé‡å­åŒ–
                    df_optimized[col] = (df_optimized[col] * 100).round().astype("int16")
        
        return df_optimized

    def _optimize_string_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã®æœ€é©åŒ–"""
        df_optimized = df.copy()
        
        for col in df_optimized.select_dtypes(include=["object", "string"]).columns:
            # æ–‡å­—åˆ—ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
            max_length = df_optimized[col].astype(str).str.len().max()
            
            if max_length < 255:
                df_optimized[col] = df_optimized[col].astype("string")
            elif max_length < 65535:
                # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ–‡å­—åˆ—å‹ã‚’ä½¿ç”¨
                df_optimized[col] = df_optimized[col].astype("string")
        
        return df_optimized

    def _optimize_index(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–"""
        df_optimized = df.copy()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ•°å€¤ã®å ´åˆã€æœ€é©åŒ–
        if df_optimized.index.dtype in ["int64", "int32"]:
            if df_optimized.index.min() >= 0:
                if df_optimized.index.max() < 255:
                    df_optimized.index = df_optimized.index.astype("uint8")
                elif df_optimized.index.max() < 65535:
                    df_optimized.index = df_optimized.index.astype("uint16")
                elif df_optimized.index.max() < 4294967295:
                    df_optimized.index = df_optimized.index.astype("uint32")
        
        return df_optimized

    def process_large_dataframe_chunked(
        self, 
        df: pd.DataFrame, 
        processing_func: Callable,
        chunk_size: int = 5000,
        overlap: int = 100
    ) -> pd.DataFrame:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†"""
        logger.info(f"ğŸ“Š ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–‹å§‹: {len(df)}è¡Œ, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        
        if len(df) <= chunk_size:
            return processing_func(df)
        
        results = []
        total_chunks = (len(df) - 1) // chunk_size + 1
        
        for i in range(0, len(df), chunk_size):
            # ãƒãƒ£ãƒ³ã‚¯ã®ç¯„å›²ã‚’è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è€ƒæ…®ï¼‰
            start_idx = max(0, i - overlap if i > 0 else i)
            end_idx = min(len(df), i + chunk_size + overlap)
            
            chunk = df.iloc[start_idx:end_idx]
            
            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self._check_memory_limit():
                logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
                self._force_garbage_collection()
            
            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            try:
                processed_chunk = processing_func(chunk)
                results.append(processed_chunk)
                
                logger.debug(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {i//chunk_size + 1}/{total_chunks}")
                
            except Exception as e:
                logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not results:
            logger.error("âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†çµæœãŒç©ºã§ã™")
            return df
        
        # çµæœã‚’çµåˆ
        final_result = pd.concat(results, ignore_index=True)
        
        # é‡è¤‡ã‚’é™¤å»ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã«ã‚ˆã‚‹ï¼‰
        if overlap > 0:
            final_result = final_result.drop_duplicates()
        
        logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {len(final_result)}è¡Œ")
        return final_result

    def _check_memory_limit(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        current_memory = self.get_memory_stats().current_mb
        return current_memory < self.memory_limit_mb

    def _force_garbage_collection(self):
        """å¼·åˆ¶çš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"""
        # è¤‡æ•°å›å®Ÿè¡Œã—ã¦ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for _ in range(3):
            gc.collect()
        
        # å¼±å‚ç…§ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.tracked_objects.clear()

    @contextmanager
    def memory_monitoring(self, operation_name: str):
        """ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        start_stats = self.get_memory_stats()
        start_time = time.time()
        
        logger.info(f"ğŸ” ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹: {operation_name}")
        logger.info(f"   - é–‹å§‹ãƒ¡ãƒ¢ãƒª: {start_stats.current_mb:.1f}MB")
        
        try:
            yield
        finally:
            end_stats = self.get_memory_stats()
            end_time = time.time()
            
            memory_delta = end_stats.current_mb - start_stats.current_mb
            processing_time = end_time - start_time
            
            logger.info(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªç›£è¦–å®Œäº†: {operation_name}")
            logger.info(f"   - å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
            logger.info(f"   - ãƒ¡ãƒ¢ãƒªå¤‰åŒ–: {memory_delta:+.1f}MB")
            logger.info(f"   - æœ€çµ‚ãƒ¡ãƒ¢ãƒª: {end_stats.current_mb:.1f}MB")
            
            # ãƒ¡ãƒ¢ãƒªå±¥æ­´ã«è¨˜éŒ²
            self.memory_history.append({
                "operation": operation_name,
                "start_memory": start_stats.current_mb,
                "end_memory": end_stats.current_mb,
                "memory_delta": memory_delta,
                "processing_time": processing_time,
                "timestamp": time.time()
            })

    def optimize_memory_usage(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–"""
        logger.info("ğŸ§¹ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–é–‹å§‹")
        
        with self.memory_monitoring("memory_optimization"):
            # 1. ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
            self._force_garbage_collection()
            
            # 2. ä¸è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‰Šé™¤
            self._cleanup_unused_objects()
            
            # 3. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€é©åŒ–
            self._optimize_caches()
            
            # 4. ãƒ¡ãƒ¢ãƒªãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è§£æ¶ˆ
            self._defragment_memory()
        
        # æœ€é©åŒ–çµæœã‚’è¿”ã™
        final_stats = self.get_memory_stats()
        
        return {
            "optimization_completed": True,
            "final_memory_mb": final_stats.current_mb,
            "memory_saved_mb": self.optimization_stats["memory_saved_mb"],
            "compression_ratio": self.optimization_stats["compression_ratio"],
            "total_optimizations": self.optimization_stats["total_optimizations"]
        }

    def _cleanup_unused_objects(self):
        """ä¸è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‰Šé™¤"""
        # å¼±å‚ç…§ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.tracked_objects.clear()
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«åå‰ç©ºé–“ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for name in list(globals().keys()):
            if name.startswith('_temp_'):
                del globals()[name]

    def _optimize_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€é©åŒ–"""
        # é–¢æ•°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢
        if hasattr(self, '_cache'):
            self._cache.clear()
        
        # LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€é©åŒ–
        for obj in gc.get_objects():
            if hasattr(obj, 'cache_clear'):
                try:
                    obj.cache_clear()
                except:
                    pass

    def _defragment_memory(self):
        """ãƒ¡ãƒ¢ãƒªãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è§£æ¶ˆ"""
        # è¤‡æ•°å›ã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è§£æ¶ˆ
        for _ in range(5):
            gc.collect()

    def get_optimization_report(self) -> Dict[str, Any]:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        current_stats = self.get_memory_stats()
        
        return {
            "current_memory_mb": current_stats.current_mb,
            "peak_memory_mb": current_stats.peak_mb,
            "memory_usage_percent": current_stats.usage_percent,
            "optimization_stats": self.optimization_stats,
            "memory_history_count": len(self.memory_history),
            "tracked_objects_count": len(self.tracked_objects),
            "recommendations": self._generate_memory_recommendations()
        }

    def _generate_memory_recommendations(self) -> List[str]:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        current_stats = self.get_memory_stats()
        
        if current_stats.usage_percent > 80:
            recommendations.append("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ80%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å‡¦ç†ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if current_stats.objects_count > 100000:
            recommendations.append("ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°ãŒå¤šã™ãã¾ã™ã€‚ä¸è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‰Šé™¤ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if self.optimization_stats["compression_ratio"] > 0.7:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ç‡ãŒä½ã„ã§ã™ã€‚ã‚ˆã‚Šç©æ¥µçš„ãªæœ€é©åŒ–ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        return recommendations

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self._force_garbage_collection()
        self.tracked_objects.clear()
        self.memory_history.clear()
        
        logger.info("ğŸ§¹ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")


def create_enhanced_memory_optimizer(
    memory_limit_mb: int = 1024, 
    aggressive_mode: bool = True
) -> EnhancedMemoryOptimizer:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""
    return EnhancedMemoryOptimizer(memory_limit_mb, aggressive_mode)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    import pandas as pd
    import numpy as np
    
    # å¤§è¦æ¨¡ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_rows = 100000
    
    sample_data = pd.DataFrame({
        'id': range(n_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),
        'value1': np.random.randn(n_rows),
        'value2': np.random.randint(0, 1000, n_rows),
        'value3': np.random.uniform(0, 1, n_rows),
        'text': [f'text_{i}' for i in range(n_rows)]
    })
    
    # å¼·åŒ–ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    optimizer = create_enhanced_memory_optimizer(memory_limit_mb=512, aggressive_mode=True)
    
    print(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(sample_data.columns)}åˆ—, {len(sample_data)}è¡Œ")
    print(f"ğŸ’¾ å…ƒãƒ¡ãƒ¢ãƒª: {sample_data.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    optimized_data = optimizer.optimize_dataframe_aggressive(sample_data)
    
    print(f"ğŸ“ˆ æœ€é©åŒ–å¾Œ: {len(optimized_data.columns)}åˆ—, {len(optimized_data)}è¡Œ")
    print(f"ğŸ’¾ æœ€é©åŒ–å¾Œãƒ¡ãƒ¢ãƒª: {optimized_data.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = optimizer.get_optimization_report()
    print(f"ğŸ“‹ æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ: {report}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    optimizer.cleanup()
