#!/usr/bin/env python3
"""
å¼·åŒ–AIäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨é€£æºã«ã‚ˆã‚‹æœˆé–“15-25%ã®åˆ©ç›Šå‘ä¸Š

ä¸»è¦æ©Ÿèƒ½:
1. æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬
2. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š
3. äºˆæ¸¬ä¿¡é ¼åº¦ã®å‹•çš„èª¿æ•´
4. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®ç¶™ç¶šç›£è¦–
5. çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨é€£æº
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union
import json
import logging
from dataclasses import dataclass, asdict
from enum import Enum
import warnings
from sklearn.ensemble import (
    RandomForestRegressor,
    GradientBoostingRegressor,
    VotingRegressor,
)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
import joblib
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
import os
import sys
import requests

# çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from unified_system import UnifiedSystem, ErrorCategory, LogLevel, LogCategory

# çµ±åˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from unified_error_handling_system import (
    get_unified_error_handler,
    ErrorCategory as UnifiedErrorCategory,
    ErrorSeverity,
    error_handler,
    error_context,
    log_api_error,
    log_data_error,
    log_model_error,
    log_file_error
)

warnings.filterwarnings("ignore")


class ModelType(Enum):
    """äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""

    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    LINEAR_REGRESSION = "linear_regression"
    RIDGE_REGRESSION = "ridge_regression"
    LASSO_REGRESSION = "lasso_regression"
    SVM_REGRESSION = "svm_regression"
    NEURAL_NETWORK = "neural_network"
    ENSEMBLE = "ensemble"


class PredictionConfidence(Enum):
    """äºˆæ¸¬ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«"""

    VERY_LOW = "very_low"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


class ModelHealthStatus(Enum):
    """ãƒ¢ãƒ‡ãƒ«å¥å…¨æ€§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""

    OK = "ok"
    WARNING = "warning"
    STOP = "stop"


@dataclass
class ModelHealthReport:
    """å¥å…¨æ€§ã‚²ãƒ¼ãƒˆã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ"""

    status: ModelHealthStatus
    detail: Dict[str, float]
    reasons: List[str]
    checked_at: datetime


@dataclass
class PredictionResult:
    """äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    symbol: str
    predicted_price: float
    confidence: PredictionConfidence
    confidence_score: float
    model_name: str
    prediction_time: datetime
    features_used: List[str]
    additional_info: Dict[str, any] = None

    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return asdict(self)

    def to_json(self) -> str:
        """JSONå½¢å¼ã«å¤‰æ›"""
        return json.dumps(self.to_dict(), default=str, ensure_ascii=False)


@dataclass
class ModelPerformance:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    model_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    mse: float
    rmse: float
    mae: float
    r2_score: float
    last_updated: datetime
    prediction_count: int = 0
    success_rate: float = 0.0

    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return asdict(self)


class EnhancedAIPredictionSystem:
    """å¼·åŒ–AIäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, unified_system: UnifiedSystem = None):
        """åˆæœŸåŒ–"""
        self.unified_system = unified_system or UnifiedSystem()
        self.models = {}
        self.scalers = {}
        self.performance_metrics = {}
        self.prediction_history = []
        self.model_cache = {}
        self.is_training = False
        self.training_lock = threading.Lock()

        # è¨­å®š
        self.config = {
            "max_models": 10,
            "retrain_interval_hours": 24,
            "min_confidence_threshold": 0.6,
            "ensemble_weight_threshold": 0.1,
            "feature_importance_threshold": 0.05,
            "prediction_cache_size": 1000,
            "model_save_path": "models/",
            "performance_log_path": "logs/ai_prediction_performance.log",
        }

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs(self.config["model_save_path"], exist_ok=True)
        os.makedirs("logs", exist_ok=True)

        # ãƒ­ã‚°è¨­å®š
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        self.logger.setLevel(logging.INFO)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¿½åŠ 
        file_handler = logging.FileHandler(self.config["performance_log_path"])
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        self.logger.info("ğŸš€ å¼·åŒ–AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

        # å¥å…¨æ€§ã‚²ãƒ¼ãƒˆè¨­å®šï¼ˆé–¾å€¤ï¼‰
        self.health_config = {
            "max_missing_ratio": 0.1,  # ç‰¹å¾´é‡æ¬ æç‡ã®ä¸Šé™ï¼ˆ10%ï¼‰
            "max_feature_z_abs": 5.0,  # ç‰¹å¾´é‡zå€¤ã®çµ¶å¯¾æœ€å¤§è¨±å®¹ï¼ˆ5Ïƒï¼‰
            "max_mahalanobis": 25.0,  # ãƒãƒãƒ©ãƒãƒ“ã‚¹è¿‘ä¼¼ï¼ˆz^2åˆè¨ˆï¼‰ã®ä¸Šé™
            "warning_mahalanobis": 16.0,  # è­¦å‘ŠåŸŸï¼ˆç´„4Ïƒç›¸å½“ï¼‰
            "min_confidence_threshold": 0.6,  # å†…éƒ¨ä¿¡é ¼åº¦ä¸‹é™
            "health_output_path": os.path.join(
                "web-app", "public", "data", "model_health.json"
            ),
        }

    class ModelHealthException(Exception):
        pass

    def create_model(self, model_type: ModelType, **kwargs) -> object:
        """ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        try:
            if model_type == ModelType.RANDOM_FOREST:
                return RandomForestRegressor(
                    n_estimators=kwargs.get("n_estimators", 100),
                    max_depth=kwargs.get("max_depth", 10),
                    random_state=42,
                )
            elif model_type == ModelType.GRADIENT_BOOSTING:
                return GradientBoostingRegressor(
                    n_estimators=kwargs.get("n_estimators", 100),
                    learning_rate=kwargs.get("learning_rate", 0.1),
                    random_state=42,
                )
            elif model_type == ModelType.LINEAR_REGRESSION:
                return LinearRegression()
            elif model_type == ModelType.RIDGE_REGRESSION:
                return Ridge(alpha=kwargs.get("alpha", 1.0))
            elif model_type == ModelType.LASSO_REGRESSION:
                return Lasso(alpha=kwargs.get("alpha", 1.0))
            elif model_type == ModelType.SVM_REGRESSION:
                return SVR(
                    kernel=kwargs.get("kernel", "rbf"),
                    C=kwargs.get("C", 1.0),
                    gamma=kwargs.get("gamma", "scale"),
                )
            elif model_type == ModelType.NEURAL_NETWORK:
                return MLPRegressor(
                    hidden_layer_sizes=kwargs.get("hidden_layer_sizes", (100, 50)),
                    max_iter=kwargs.get("max_iter", 1000),
                    random_state=42,
                )
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context=f"ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {model_type.value}",
            )
            raise

    def prepare_features(
        self,
        data: pd.DataFrame,
        target_column: str = "price",
        feature_columns: List[str] = None,
    ) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """ç‰¹å¾´é‡ã®æº–å‚™"""
        try:
            # åŸºæœ¬ç‰¹å¾´é‡
            if feature_columns is None:
                features = []

                # ä¾¡æ ¼é–¢é€£ç‰¹å¾´é‡
                if "price" in data.columns:
                    features.extend(["price", "volume", "high", "low", "open", "close"])

                # æŠ€è¡“æŒ‡æ¨™
                if len(data) > 20:
                    # ç§»å‹•å¹³å‡
                    data["ma_5"] = data["price"].rolling(window=5).mean()
                    data["ma_10"] = data["price"].rolling(window=10).mean()
                    data["ma_20"] = data["price"].rolling(window=20).mean()

                    # RSI
                    delta = data["price"].diff()
                    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                    rs = gain / loss
                    data["rsi"] = 100 - (100 / (1 + rs))

                    # MACD
                    exp1 = data["price"].ewm(span=12).mean()
                    exp2 = data["price"].ewm(span=26).mean()
                    data["macd"] = exp1 - exp2
                    data["macd_signal"] = data["macd"].ewm(span=9).mean()

                    features.extend(
                        ["ma_5", "ma_10", "ma_20", "rsi", "macd", "macd_signal"]
                    )

                # ç‰¹å¾´é‡ã®é¸æŠ
                feature_columns = [col for col in features if col in data.columns]

            # ç‰¹å¾´é‡ã®å–å¾—ã¨æ¬ æå€¤ã®å‡¦ç†
            X = data[feature_columns].fillna(0).values
            y = (
                data[target_column].values
                if target_column in data.columns
                else np.zeros(len(data))
            )

            return X, y, feature_columns

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.DATA_PROCESSING_ERROR,
                context="ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼",
            )
            raise

    def train_model(
        self,
        model_type: ModelType,
        data: pd.DataFrame,
        target_column: str = "price",
        model_name: str = None,
    ) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
        try:
            with self.training_lock:
                self.is_training = True

                # ç‰¹å¾´é‡ã®æº–å‚™
                X, y, feature_columns = self.prepare_features(data, target_column)

                # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
                split_idx = int(len(X) * 0.8)
                X_train, X_test = X[:split_idx], X[split_idx:]
                y_train, y_test = y[:split_idx], y[split_idx:]

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)

                # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨å­¦ç¿’
                model = self.create_model(model_type)
                model.fit(X_train_scaled, y_train)

                # äºˆæ¸¬ã¨è©•ä¾¡
                y_pred = model.predict(X_test_scaled)
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                # ãƒ¢ãƒ‡ãƒ«åã®ç”Ÿæˆ
                if model_name is None:
                    model_name = (
                        f"{model_type.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    )

                # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚‚ä¿å­˜ï¼‰
                model.feature_columns = feature_columns
                self.models[model_name] = model
                self.scalers[model_name] = scaler

                # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²
                self.performance_metrics[model_name] = ModelPerformance(
                    model_name=model_name,
                    accuracy=r2,
                    precision=0.0,  # å›å¸°ã§ã¯ä½¿ç”¨ã—ãªã„
                    recall=0.0,  # å›å¸°ã§ã¯ä½¿ç”¨ã—ãªã„
                    f1_score=0.0,  # å›å¸°ã§ã¯ä½¿ç”¨ã—ãªã„
                    mse=mse,
                    rmse=np.sqrt(mse),
                    mae=mae,
                    r2_score=r2,
                    last_updated=datetime.now(),
                    prediction_count=0,
                    success_rate=0.0,
                )

                # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                self.save_model(model_name)

                self.logger.info(
                    f"âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†: {model_name} (RÂ²={r2:.4f}, MSE={mse:.4f})"
                )

                return model_name

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context=f"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {model_type.value}",
            )
            raise
        finally:
            self.is_training = False

    def predict(
        self, model_name: str, data: pd.DataFrame, confidence_threshold: float = None
    ) -> PredictionResult:
        """äºˆæ¸¬ã®å®Ÿè¡Œ"""
        try:
            if model_name not in self.models:
                raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_name}")

            # ãƒ¢ãƒ‡ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’å–å¾—
            model_feature_columns = getattr(
                self.models[model_name], "feature_columns", None
            )

            # ç‰¹å¾´é‡ã®æº–å‚™ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
            X, _, feature_columns = self.prepare_features(
                data, feature_columns=model_feature_columns
            )

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = self.scalers[model_name]
            X_scaled = scaler.transform(X)

            # æ¨è«–ç›´å‰ã®å¥å…¨æ€§ã‚²ãƒ¼ãƒˆ
            health = self.check_model_health(
                model_name=model_name,
                X_raw=X,
                X_scaled=X_scaled,
                feature_columns=feature_columns,
                data_frame=data,
            )
            # å¥å…¨æ€§çµæœã‚’å‡ºåŠ›ï¼ˆUIå‚ç…§ç”¨ï¼‰
            try:
                self.export_model_health(health)
            except Exception:
                pass

            if health.status == ModelHealthStatus.STOP:
                self.logger.error(
                    f"ğŸ›‘ å¥å…¨æ€§ã‚²ãƒ¼ãƒˆ: åœæ­¢åˆ¤å®š - ç†ç”±: {', '.join(health.reasons)}"
                )
                self._notify_health(health, severity="critical")
                raise EnhancedAIPredictionSystem.ModelHealthException(
                    f"å¥å…¨æ€§ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚Šæ¨è«–åœæ­¢: {health.reasons}"
                )
            elif health.status == ModelHealthStatus.WARNING:
                self.logger.warning(
                    f"âš ï¸ å¥å…¨æ€§ã‚²ãƒ¼ãƒˆ: è­¦å‘Š - ç†ç”±: {', '.join(health.reasons)}"
                )
                self._notify_health(health, severity="warning")

            # äºˆæ¸¬
            model = self.models[model_name]
            prediction = model.predict(X_scaled[-1:])[0]

            # ä¿¡é ¼åº¦ã®è¨ˆç®—
            confidence_score = self.calculate_confidence(model, X_scaled[-1:])
            confidence = self.get_confidence_level(confidence_score)

            # ä¿¡é ¼åº¦é–¾å€¤ã®ãƒã‚§ãƒƒã‚¯
            if confidence_threshold and confidence_score < confidence_threshold:
                self.logger.warning(
                    f"äºˆæ¸¬ä¿¡é ¼åº¦ãŒé–¾å€¤ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™: {confidence_score:.4f} < {confidence_threshold:.4f}"
                )

            # äºˆæ¸¬çµæœã®ä½œæˆ
            result = PredictionResult(
                symbol=(
                    data.get("symbol", "UNKNOWN").iloc[-1]
                    if "symbol" in data.columns
                    else "UNKNOWN"
                ),
                predicted_price=float(prediction),
                confidence=confidence,
                confidence_score=confidence_score,
                model_name=model_name,
                prediction_time=datetime.now(),
                features_used=feature_columns,
                additional_info={
                    "model_performance": self.performance_metrics[model_name].to_dict(),
                    "feature_importance": self.get_feature_importance(
                        model, feature_columns
                    ),
                    "prediction_interval": self.calculate_prediction_interval(
                        model, X_scaled[-1:]
                    ),
                    "model_health": {
                        "status": health.status.value,
                        "detail": health.detail,
                        "reasons": health.reasons,
                    },
                },
            )

            # äºˆæ¸¬å±¥æ­´ã®è¨˜éŒ²
            self.prediction_history.append(result)
            if len(self.prediction_history) > self.config["prediction_cache_size"]:
                self.prediction_history.pop(0)

            # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°
            self.performance_metrics[model_name].prediction_count += 1

            self.logger.info(
                f"ğŸ”® äºˆæ¸¬å®Œäº†: {model_name} - ä¾¡æ ¼: {prediction:.2f}, ä¿¡é ¼åº¦: {confidence_score:.4f}"
            )

            return result

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context=f"äºˆæ¸¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {model_name}",
            )
            raise

    def calculate_confidence(self, model, X: np.ndarray) -> float:
        """ä¿¡é ¼åº¦ã®è¨ˆç®—"""
        try:
            # è¤‡æ•°ã®æ–¹æ³•ã§ä¿¡é ¼åº¦ã‚’è¨ˆç®—
            confidence_scores = []

            # 1. ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãƒ™ãƒ¼ã‚¹
            if hasattr(model, "score"):
                confidence_scores.append(max(0, min(1, model.score(X, X))))

            # 2. ç‰¹å¾´é‡ã®åˆ†æ•£ãƒ™ãƒ¼ã‚¹
            feature_variance = np.var(X)
            confidence_scores.append(max(0, min(1, 1 - feature_variance)))

            # 3. ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X)
                confidence_scores.append(np.max(proba))

            # å¹³å‡ä¿¡é ¼åº¦
            return np.mean(confidence_scores) if confidence_scores else 0.5

        except Exception:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def get_confidence_level(self, confidence_score: float) -> PredictionConfidence:
        """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ã®å–å¾—"""
        if confidence_score >= 0.9:
            return PredictionConfidence.VERY_HIGH
        elif confidence_score >= 0.8:
            return PredictionConfidence.HIGH
        elif confidence_score >= 0.7:
            return PredictionConfidence.MEDIUM
        elif confidence_score >= 0.6:
            return PredictionConfidence.LOW
        else:
            return PredictionConfidence.VERY_LOW

    def check_model_health(
        self,
        model_name: str,
        X_raw: np.ndarray,
        X_scaled: np.ndarray,
        feature_columns: List[str],
        data_frame: pd.DataFrame,
    ) -> ModelHealthReport:
        """åˆ†å¸ƒé€¸è„±ãƒ»ãƒ‡ãƒ¼ã‚¿æ¬ å¦‚ãƒ»ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’åˆ¤å®šã™ã‚‹å¥å…¨æ€§ã‚²ãƒ¼ãƒˆ"""
        reasons: List[str] = []
        detail: Dict[str, float] = {}

        try:
            # 1) ãƒ‡ãƒ¼ã‚¿æ¬ å¦‚ãƒã‚§ãƒƒã‚¯
            if len(feature_columns) == 0 or X_raw.size == 0:
                reasons.append("ç‰¹å¾´é‡ãŒç©º")
                detail["missing_ratio"] = 1.0
                status = ModelHealthStatus.STOP
                return ModelHealthReport(
                    status=status,
                    detail=detail,
                    reasons=reasons,
                    checked_at=datetime.now(),
                )

            # æ¬ æç‡
            missing_ratio = float(np.isnan(X_raw).sum()) / float(X_raw.size)
            detail["missing_ratio"] = missing_ratio
            if missing_ratio > self.health_config["max_missing_ratio"]:
                reasons.append(f"æ¬ æç‡ãŒé–¾å€¤è¶…é: {missing_ratio:.3f}")

            # 2) åˆ†å¸ƒé€¸è„±ï¼ˆç°¡æ˜“Zã‚¹ã‚³ã‚¢ã¨ç–‘ä¼¼ãƒãƒãƒ©ãƒãƒ“ã‚¹ï¼‰
            # ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆæœ€çµ‚ã‚µãƒ³ãƒ—ãƒ«ã§ã®ãƒã‚§ãƒƒã‚¯
            x = X_scaled[-1:].astype(float)
            z_abs_max = float(np.max(np.abs(x))) if x.size > 0 else 0.0
            detail["z_abs_max"] = z_abs_max
            if z_abs_max > self.health_config["max_feature_z_abs"]:
                reasons.append(f"|z|max è¶…é: {z_abs_max:.2f}")

            mahalanobis_approx = float(np.sum(np.square(x)))  # å…±åˆ†æ•£=Iã®è¿‘ä¼¼
            detail["mahalanobis_approx"] = mahalanobis_approx
            if mahalanobis_approx > self.health_config["max_mahalanobis"]:
                reasons.append(f"åˆ†å¸ƒé€¸è„±ãŒå¤§ï¼ˆè¿‘ä¼¼D^2={mahalanobis_approx:.1f}ï¼‰")
            elif mahalanobis_approx > self.health_config["warning_mahalanobis"]:
                reasons.append(f"åˆ†å¸ƒé€¸è„±ãŒä¸­ï¼ˆè¿‘ä¼¼D^2={mahalanobis_approx:.1f}ï¼‰")

            # 3) ç•°å¸¸ã‚¹ã‚³ã‚¢ï¼ˆå†…éƒ¨ä¿¡é ¼åº¦ã®ä¸‹é™ãƒã‚§ãƒƒã‚¯ï¼‰
            # ã“ã“ã§ã¯ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿1ç‚¹ã«å¯¾ã™ã‚‹ç°¡æ˜“ä¿¡é ¼åº¦ã‚’å†è©•ä¾¡
            try:
                model = self.models[model_name]
                conf = float(self.calculate_confidence(model, x))
            except Exception:
                conf = 0.5
            detail["confidence_estimate"] = conf
            if conf < self.health_config["min_confidence_threshold"]:
                reasons.append(f"å†…éƒ¨ä¿¡é ¼åº¦ãŒä½ã„: {conf:.2f}")

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
            stop_conditions = (
                missing_ratio > self.health_config["max_missing_ratio"]
                or z_abs_max > self.health_config["max_feature_z_abs"]
                or mahalanobis_approx > self.health_config["max_mahalanobis"]
            )
            warn_conditions = (
                mahalanobis_approx > self.health_config["warning_mahalanobis"]
                or conf < self.health_config["min_confidence_threshold"]
            )

            if stop_conditions:
                status = ModelHealthStatus.STOP
            elif warn_conditions:
                status = ModelHealthStatus.WARNING
            else:
                status = ModelHealthStatus.OK

            return ModelHealthReport(
                status=status, detail=detail, reasons=reasons, checked_at=datetime.now()
            )

        except Exception as e:
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã¯ãªããƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ï¼ˆSTOPï¼‰
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="å¥å…¨æ€§ã‚²ãƒ¼ãƒˆè©•ä¾¡ã‚¨ãƒ©ãƒ¼",
            )
            return ModelHealthReport(
                status=ModelHealthStatus.STOP,
                detail={"error": 1.0},
                reasons=["å¥å…¨æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼"],
                checked_at=datetime.now(),
            )

    def export_model_health(self, report: ModelHealthReport) -> bool:
        """å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONã¨ã—ã¦æ›¸ãå‡ºã—ï¼ˆWeb UIå‚ç…§ç”¨ï¼‰"""
        try:
            os.makedirs(
                os.path.dirname(self.health_config["health_output_path"]), exist_ok=True
            )
            payload = {
                "status": report.status.value,
                "detail": report.detail,
                "reasons": report.reasons,
                "checked_at": report.checked_at.isoformat(),
            }
            with open(
                self.health_config["health_output_path"], "w", encoding="utf-8"
            ) as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            return True
        except Exception:
            return False

    def _notify_health(
        self, report: ModelHealthReport, severity: str = "warning"
    ) -> None:
        """å¥å…¨æ€§ã‚¢ãƒ©ãƒ¼ãƒˆã‚’Webhookã¸é€šçŸ¥ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ã€‚å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ç¶™ç¶šã€‚"""
        try:
            webhook = os.getenv("HEALTH_WEBHOOK_URL") or os.getenv("SLACK_WEBHOOK_URL")
            if not webhook:
                return
            payload = {
                "text": f"[ModelHealth] status={report.status.value} severity={severity} reasons={', '.join(report.reasons)}",
                "attachments": [
                    {
                        "color": (
                            "#ff0000"
                            if report.status == ModelHealthStatus.STOP
                            else "#ffcc00"
                        ),
                        "fields": [
                            {"title": k, "value": str(v), "short": True}
                            for k, v in report.detail.items()
                        ],
                        "ts": int(report.checked_at.timestamp()),
                    }
                ],
            }
            headers = {"Content-Type": "application/json"}
            requests.post(webhook, data=json.dumps(payload), headers=headers, timeout=3)
        except Exception:
            # é€šçŸ¥å¤±æ•—ã¯è‡´å‘½ã§ã¯ãªã„
            return

    def get_feature_importance(
        self, model, feature_columns: List[str]
    ) -> Dict[str, float]:
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—"""
        try:
            if hasattr(model, "feature_importances_"):
                importance_dict = dict(zip(feature_columns, model.feature_importances_))
                return {
                    k: v
                    for k, v in importance_dict.items()
                    if v >= self.config["feature_importance_threshold"]
                }
            else:
                return {}
        except Exception:
            return {}

    def calculate_prediction_interval(
        self, model, X: np.ndarray, confidence: float = 0.95
    ) -> Dict[str, float]:
        """äºˆæ¸¬åŒºé–“ã®è¨ˆç®—"""
        try:
            # ç°¡æ˜“çš„ãªäºˆæ¸¬åŒºé–“ã®è¨ˆç®—
            prediction = model.predict(X)[0]

            # ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«åŸºã¥ãåŒºé–“
            if model in self.performance_metrics:
                rmse = self.performance_metrics[model].rmse
                interval = 1.96 * rmse  # 95%ä¿¡é ¼åŒºé–“

                return {
                    "lower_bound": prediction - interval,
                    "upper_bound": prediction + interval,
                    "confidence_level": confidence,
                }
            else:
                return {
                    "lower_bound": prediction * 0.95,
                    "upper_bound": prediction * 1.05,
                    "confidence_level": confidence,
                }
        except Exception:
            return {"lower_bound": 0, "upper_bound": 0, "confidence_level": confidence}

    def create_ensemble_model(
        self, model_names: List[str], weights: List[float] = None
    ) -> str:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        try:
            if len(model_names) < 2:
                raise ValueError("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯2ã¤ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã§ã™")

            # é‡ã¿ã®æ­£è¦åŒ–
            if weights is None:
                weights = [1.0 / len(model_names)] * len(model_names)
            else:
                weights = np.array(weights)
                weights = weights / np.sum(weights)

            # ãƒ¢ãƒ‡ãƒ«ã®å–å¾—
            models = [self.models[name] for name in model_names if name in self.models]

            if len(models) != len(model_names):
                raise ValueError("ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
            ensemble_name = f"ensemble_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            # é‡ã¿ä»˜ãå¹³å‡ã«ã‚ˆã‚‹äºˆæ¸¬
            def ensemble_predict(X):
                predictions = np.array([model.predict(X) for model in models])
                return np.average(predictions, axis=0, weights=weights)

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            self.models[ensemble_name] = type(
                "EnsembleModel",
                (),
                {
                    "predict": ensemble_predict,
                    "feature_importances_": np.zeros(
                        len(models[0].feature_importances_)
                    ),
                    "score": lambda X, y: np.mean(
                        [model.score(X, y) for model in models]
                    ),
                },
            )()

            self.logger.info(f"âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {ensemble_name}")
            return ensemble_name

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼",
            )
            raise

    def batch_predict(
        self, model_name: str, data_list: List[pd.DataFrame]
    ) -> List[PredictionResult]:
        """ãƒãƒƒãƒäºˆæ¸¬ã®å®Ÿè¡Œ"""
        try:
            results = []

            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_data = {
                    executor.submit(self.predict, model_name, data): data
                    for data in data_list
                }

                for future in as_completed(future_to_data):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        self.unified_system.log_error(
                            error=e,
                            category=ErrorCategory.MODEL_ERROR,
                            context="ãƒãƒƒãƒäºˆæ¸¬ã‚¨ãƒ©ãƒ¼",
                        )

            return results

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="ãƒãƒƒãƒäºˆæ¸¬å®Ÿè¡Œã‚¨ãƒ©ãƒ¼",
            )
            raise

    def save_model(self, model_name: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        try:
            if model_name not in self.models:
                return False

            model_path = os.path.join(
                self.config["model_save_path"], f"{model_name}.joblib"
            )
            scaler_path = os.path.join(
                self.config["model_save_path"], f"{model_name}_scaler.joblib"
            )

            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            joblib.dump(self.models[model_name], model_path)
            joblib.dump(self.scalers[model_name], scaler_path)

            self.logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_name}")
            return True

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.FILE_ERROR,
                context=f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {model_name}",
            )
            return False

    def load_model(self, model_name: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            model_path = os.path.join(
                self.config["model_save_path"], f"{model_name}.joblib"
            )
            scaler_path = os.path.join(
                self.config["model_save_path"], f"{model_name}_scaler.joblib"
            )

            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                return False

            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            self.models[model_name] = joblib.load(model_path)
            self.scalers[model_name] = joblib.load(scaler_path)

            self.logger.info(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_name}")
            return True

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.FILE_ERROR,
                context=f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {model_name}",
            )
            return False

    def get_model_performance(
        self, model_name: str = None
    ) -> Union[ModelPerformance, Dict[str, ModelPerformance]]:
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®å–å¾—"""
        try:
            if model_name:
                return self.performance_metrics.get(model_name)
            else:
                return self.performance_metrics
        except Exception as e:
            self.unified_system.log_error(
                error=e, category=ErrorCategory.MODEL_ERROR, context="æ€§èƒ½å–å¾—ã‚¨ãƒ©ãƒ¼"
            )
            return {}

    def retrain_models(
        self, data: pd.DataFrame, target_column: str = "price"
    ) -> Dict[str, str]:
        """ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’"""
        try:
            retrained_models = {}

            for model_name in list(self.models.keys()):
                try:
                    # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®æ¨å®š
                    model = self.models[model_name]
                    model_type = self.infer_model_type(model)

                    # å†å­¦ç¿’
                    new_model_name = self.train_model(
                        model_type,
                        data,
                        target_column,
                        f"{model_name}_retrained_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    )

                    retrained_models[model_name] = new_model_name

                except Exception as e:
                    self.unified_system.log_error(
                        error=e,
                        category=ErrorCategory.MODEL_ERROR,
                        context=f"ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {model_name}",
                    )

            self.logger.info(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Œäº†: {len(retrained_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«")
            return retrained_models

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’å®Ÿè¡Œã‚¨ãƒ©ãƒ¼",
            )
            raise

    def infer_model_type(self, model) -> ModelType:
        """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®æ¨å®š"""
        model_class = type(model).__name__

        if "RandomForest" in model_class:
            return ModelType.RANDOM_FOREST
        elif "GradientBoosting" in model_class:
            return ModelType.GRADIENT_BOOSTING
        elif "LinearRegression" in model_class:
            return ModelType.LINEAR_REGRESSION
        elif "Ridge" in model_class:
            return ModelType.RIDGE_REGRESSION
        elif "Lasso" in model_class:
            return ModelType.LASSO_REGRESSION
        elif "SVR" in model_class:
            return ModelType.SVM_REGRESSION
        elif "MLPRegressor" in model_class:
            return ModelType.NEURAL_NETWORK
        else:
            return ModelType.LINEAR_REGRESSION  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def cleanup_old_models(self, keep_count: int = 5) -> int:
        """å¤ã„ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if len(self.models) <= keep_count:
                return 0

            # æ€§èƒ½ã®æ‚ªã„ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
            model_scores = []
            for name, perf in self.performance_metrics.items():
                model_scores.append((name, perf.r2_score))

            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            model_scores.sort(key=lambda x: x[1], reverse=True)

            # å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤
            models_to_remove = model_scores[keep_count:]
            removed_count = 0

            for model_name, _ in models_to_remove:
                if model_name in self.models:
                    del self.models[model_name]
                    del self.scalers[model_name]
                    del self.performance_metrics[model_name]
                    removed_count += 1

            self.logger.info(f"ğŸ—‘ï¸ å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {removed_count}å€‹å‰Šé™¤")
            return removed_count

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼",
            )
            return 0

    def get_prediction_summary(self) -> Dict[str, any]:
        """äºˆæ¸¬ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        try:
            if not self.prediction_history:
                return {}

            recent_predictions = self.prediction_history[-100:]  # æœ€æ–°100ä»¶

            summary = {
                "total_predictions": len(self.prediction_history),
                "recent_predictions": len(recent_predictions),
                "average_confidence": np.mean(
                    [p.confidence_score for p in recent_predictions]
                ),
                "high_confidence_predictions": len(
                    [
                        p
                        for p in recent_predictions
                        if p.confidence
                        in [PredictionConfidence.HIGH, PredictionConfidence.VERY_HIGH]
                    ]
                ),
                "model_usage": {},
                "symbol_distribution": {},
                "prediction_trends": self.analyze_prediction_trends(recent_predictions),
            }

            # ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çŠ¶æ³
            for pred in recent_predictions:
                model_name = pred.model_name
                summary["model_usage"][model_name] = (
                    summary["model_usage"].get(model_name, 0) + 1
                )

                symbol = pred.symbol
                summary["symbol_distribution"][symbol] = (
                    summary["symbol_distribution"].get(symbol, 0) + 1
                )

            return summary

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.MODEL_ERROR,
                context="äºˆæ¸¬ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼",
            )
            return {}

    def analyze_prediction_trends(
        self, predictions: List[PredictionResult]
    ) -> Dict[str, any]:
        """äºˆæ¸¬ãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ"""
        try:
            if len(predictions) < 2:
                return {}

            prices = [p.predicted_price for p in predictions]
            confidences = [p.confidence_score for p in predictions]

            return {
                "price_trend": "up" if prices[-1] > prices[0] else "down",
                "confidence_trend": (
                    "up" if confidences[-1] > confidences[0] else "down"
                ),
                "price_volatility": np.std(prices),
                "confidence_volatility": np.std(confidences),
                "trend_strength": (
                    abs(prices[-1] - prices[0]) / prices[0] if prices[0] != 0 else 0
                ),
            }

        except Exception:
            return {}

    def export_predictions(self, file_path: str, format: str = "json") -> bool:
        """äºˆæ¸¬çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            if format.lower() == "json":
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(
                        [pred.to_dict() for pred in self.prediction_history],
                        f,
                        default=str,
                        ensure_ascii=False,
                        indent=2,
                    )
            elif format.lower() == "csv":
                df = pd.DataFrame([pred.to_dict() for pred in self.prediction_history])
                df.to_csv(file_path, index=False, encoding="utf-8")
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {format}")

            self.logger.info(f"ğŸ“Š äºˆæ¸¬çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {file_path}")
            return True

        except Exception as e:
            self.unified_system.log_error(
                error=e,
                category=ErrorCategory.FILE_ERROR,
                context=f"äºˆæ¸¬çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {file_path}",
            )
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        unified_system = UnifiedSystem()

        # AIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        ai_system = EnhancedAIPredictionSystem(unified_system)

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        np.random.seed(42)
        dates = pd.date_range(start="2024-01-01", end="2024-01-31", freq="D")
        data = pd.DataFrame(
            {
                "date": dates,
                "price": 100 + np.cumsum(np.random.randn(len(dates)) * 0.5),
                "volume": np.random.randint(1000, 10000, len(dates)),
                "high": 100
                + np.cumsum(np.random.randn(len(dates)) * 0.5)
                + np.random.randn(len(dates)) * 0.1,
                "low": 100
                + np.cumsum(np.random.randn(len(dates)) * 0.5)
                - np.random.randn(len(dates)) * 0.1,
                "open": 100 + np.cumsum(np.random.randn(len(dates)) * 0.5),
                "close": 100 + np.cumsum(np.random.randn(len(dates)) * 0.5),
                "symbol": "TEST",
            }
        )

        # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        model_name = ai_system.train_model(ModelType.RANDOM_FOREST, data)

        # äºˆæ¸¬ã®å®Ÿè¡Œ
        prediction = ai_system.predict(model_name, data.tail(10))

        print(f"äºˆæ¸¬çµæœ: {prediction.predicted_price:.2f}")
        print(f"ä¿¡é ¼åº¦: {prediction.confidence_score:.4f}")
        print(f"ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«: {prediction.confidence.value}")

        # äºˆæ¸¬ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        summary = ai_system.get_prediction_summary()
        print(f"\näºˆæ¸¬ã‚µãƒãƒªãƒ¼: {summary}")

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
