#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿è©•ä¾¡ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
- é©åˆ‡ãªå­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- éå­¦ç¿’æ¤œå‡º
- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
- è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°èª¬æ˜
"""

import json
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    KFold,
    validation_curve,
)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    mean_absolute_percentage_error,
)
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings("ignore")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedDataValidator:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿è©•ä¾¡ãƒ»æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.logger = logging.getLogger(__name__)

        # è©•ä¾¡æŒ‡æ¨™ã®èª¬æ˜
        self.metrics_explanation = {
            "mae": {
                "name": "å¹³å‡çµ¶å¯¾èª¤å·® (MAE)",
                "description": "äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å€¤ã®å·®ã®çµ¶å¯¾å€¤ã®å¹³å‡ã€‚å°ã•ã„ã»ã©è‰¯ã„ã€‚",
                "unit": "å††",
                "interpretation": "äºˆæ¸¬ãŒå®Ÿéš›ã®å€¤ã‹ã‚‰å¹³å‡çš„ã«ã©ã®ç¨‹åº¦ãšã‚Œã¦ã„ã‚‹ã‹ã‚’ç¤ºã™",
            },
            "rmse": {
                "name": "å¹³å‡å¹³æ–¹æ ¹èª¤å·® (RMSE)",
                "description": "äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å€¤ã®å·®ã®äºŒä¹—ã®å¹³å‡ã®å¹³æ–¹æ ¹ã€‚MAEã‚ˆã‚Šå¤§ããªèª¤å·®ã‚’é‡ãè©•ä¾¡ã€‚",
                "unit": "å††",
                "interpretation": "å¤§ããªèª¤å·®ã‚’é‡ãè©•ä¾¡ã™ã‚‹æŒ‡æ¨™",
            },
            "r2": {
                "name": "æ±ºå®šä¿‚æ•° (RÂ²)",
                "description": "ãƒ¢ãƒ‡ãƒ«ãŒèª¬æ˜ã§ãã‚‹åˆ†æ•£ã®å‰²åˆã€‚1ã«è¿‘ã„ã»ã©è‰¯ã„ï¼ˆãŸã ã—1.0ã¯éå­¦ç¿’ã®å¯èƒ½æ€§ï¼‰ã€‚",
                "unit": "0-1ã®ç¯„å›²",
                "interpretation": "ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã®å¤‰å‹•ã‚’ã©ã®ç¨‹åº¦èª¬æ˜ã§ãã‚‹ã‹ã‚’ç¤ºã™",
            },
            "mape": {
                "name": "å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·® (MAPE)",
                "description": "äºˆæ¸¬èª¤å·®ã®å‰²åˆã®å¹³å‡ã€‚å°ã•ã„ã»ã©è‰¯ã„ã€‚",
                "unit": "%",
                "interpretation": "äºˆæ¸¬èª¤å·®ã®ç›¸å¯¾çš„ãªå¤§ãã•ã‚’ç¤ºã™",
            },
        }

    def create_models(self) -> Dict[str, Any]:
        """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
        return {
            "Random Forest": RandomForestRegressor(
                n_estimators=100,
                random_state=self.random_state,
                max_depth=10,  # éå­¦ç¿’é˜²æ­¢
            ),
            "Gradient Boosting": GradientBoostingRegressor(
                n_estimators=100,
                random_state=self.random_state,
                max_depth=6,  # éå­¦ç¿’é˜²æ­¢
            ),
            "Linear Regression": LinearRegression(),
            "Ridge Regression": Ridge(alpha=1.0),
            "Lasso Regression": Lasso(alpha=0.1),
            "SVR": SVR(kernel="rbf", C=1.0, gamma="scale"),
        }

    def split_data_properly(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        test_size: float = 0.2,
        val_size: float = 0.2,
    ) -> Tuple[
        pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series
    ]:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«é©åˆ‡ã«åˆ†å‰²

        Args:
            X: ç‰¹å¾´é‡
            y: ç›®çš„å¤‰æ•°
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            val_size: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å‰²åˆï¼‰

        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        # ã¾ãšå­¦ç¿’+æ¤œè¨¼ã¨ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )

        # å­¦ç¿’+æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã¨æ¤œè¨¼ã«åˆ†å‰²
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size, random_state=self.random_state
        )

        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
        self.logger.info(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)} ã‚µãƒ³ãƒ—ãƒ«")
        self.logger.info(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)} ã‚µãƒ³ãƒ—ãƒ«")
        self.logger.info(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)} ã‚µãƒ³ãƒ—ãƒ«")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def evaluate_model_with_cv(
        self,
        model: Any,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        cv_folds: int = 5,
    ) -> Dict[str, Any]:
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_scores = cross_val_score(
            model, X_train, y_train, cv=cv_folds, scoring="neg_mean_absolute_error"
        )

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model.fit(X_train, y_train)

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
        y_val_pred = model.predict(X_val)

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        metrics = {
            "mae": mean_absolute_error(y_val, y_val_pred),
            "rmse": np.sqrt(mean_squared_error(y_val, y_val_pred)),
            "r2": r2_score(y_val, y_val_pred),
            "mape": mean_absolute_percentage_error(y_val, y_val_pred) * 100,
            "cv_mae_mean": -cv_scores.mean(),
            "cv_mae_std": cv_scores.std(),
            "overfitting_risk": self._detect_overfitting(y_val, y_val_pred, cv_scores),
        }

        return metrics

    def _detect_overfitting(
        self, y_val: pd.Series, y_val_pred: np.ndarray, cv_scores: np.ndarray
    ) -> str:
        """éå­¦ç¿’ã®æ¤œå‡º"""
        val_mae = mean_absolute_error(y_val, y_val_pred)
        cv_mae_mean = -cv_scores.mean()

        # RÂ²ãŒ0.99ä»¥ä¸Šã¯éå­¦ç¿’ã®å¯èƒ½æ€§
        r2 = r2_score(y_val, y_val_pred)
        if r2 > 0.99:
            return "é«˜ãƒªã‚¹ã‚¯ï¼ˆRÂ² > 0.99ï¼‰"

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨CVã‚¹ã‚³ã‚¢ã®å·®ãŒå¤§ãã„å ´åˆã¯éå­¦ç¿’
        if val_mae > cv_mae_mean * 1.5:
            return "ä¸­ãƒªã‚¹ã‚¯ï¼ˆæ¤œè¨¼æ€§èƒ½ãŒCVæ€§èƒ½ã‚ˆã‚Šæ‚ªã„ï¼‰"

        if val_mae > cv_mae_mean * 1.2:
            return "ä½ãƒªã‚¹ã‚¯ï¼ˆæ¤œè¨¼æ€§èƒ½ãŒCVæ€§èƒ½ã‚ˆã‚Šã‚„ã‚„æ‚ªã„ï¼‰"

        return "ä½ãƒªã‚¹ã‚¯"

    def compare_models(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
    ) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ"""

        models = self.create_models()
        results = []

        self.logger.info("è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’é–‹å§‹...")

        for name, model in models.items():
            self.logger.info(f"è©•ä¾¡ä¸­: {name}")

            try:
                # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
                metrics = self.evaluate_model_with_cv(
                    model, X_train, y_train, X_val, y_val
                )

                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æœ€çµ‚è©•ä¾¡
                model.fit(X_train, y_train)  # å…¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§å†å­¦ç¿’
                y_test_pred = model.predict(X_test)

                test_metrics = {
                    "mae": mean_absolute_error(y_test, y_test_pred),
                    "rmse": np.sqrt(mean_squared_error(y_test, y_test_pred)),
                    "r2": r2_score(y_test, y_test_pred),
                    "mape": mean_absolute_percentage_error(y_test, y_test_pred) * 100,
                }

                result = {
                    "model_name": name,
                    "model_type": type(model).__name__,
                    "validation_metrics": metrics,
                    "test_metrics": test_metrics,
                    "rank": 0,  # å¾Œã§è¨­å®š
                }

                results.append(result)

            except Exception as e:
                self.logger.error(f"ãƒ¢ãƒ‡ãƒ« {name} ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»˜ã‘ï¼ˆMAEåŸºæº–ï¼‰
        results.sort(key=lambda x: x["validation_metrics"]["mae"])
        for i, result in enumerate(results):
            result["rank"] = i + 1

        return {
            "model_results": results,
            "best_model": results[0]["model_name"] if results else None,
            "total_models": len(results),
        }

    def generate_evaluation_report(
        self, comparison_results: Dict[str, Any], output_dir: Path
    ) -> None:
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿
        model_comparison = []
        for result in comparison_results["model_results"]:
            model_comparison.append(
                {
                    "model_name": result["model_name"],
                    "model_type": result["model_type"],
                    "mae": result["validation_metrics"]["mae"],
                    "rmse": result["validation_metrics"]["rmse"],
                    "r2": result["validation_metrics"]["r2"],
                    "mape": result["validation_metrics"]["mape"],
                    "rank": result["rank"],
                    "overfitting_risk": result["validation_metrics"][
                        "overfitting_risk"
                    ],
                }
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼‰
        best_model = comparison_results["model_results"][0]
        performance_metrics = {
            "model_name": best_model["model_name"],
            "mae": best_model["test_metrics"]["mae"],
            "rmse": best_model["test_metrics"]["rmse"],
            "r2": best_model["test_metrics"]["r2"],
            "mape": best_model["test_metrics"]["mape"],
            "validation_mae": best_model["validation_metrics"]["mae"],
            "cv_mae_mean": best_model["validation_metrics"]["cv_mae_mean"],
            "cv_mae_std": best_model["validation_metrics"]["cv_mae_std"],
            "overfitting_risk": best_model["validation_metrics"]["overfitting_risk"],
        }

        # è©•ä¾¡ã‚µãƒãƒªãƒ¼
        evaluation_summary = {
            "total_models_evaluated": comparison_results["total_models"],
            "best_model": comparison_results["best_model"],
            "evaluation_method": "3åˆ†å‰²ï¼ˆå­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆï¼‰+ 5-fold CV",
            "overfitting_detection": "å®Ÿè£…æ¸ˆã¿",
            "metrics_explanation": self.metrics_explanation,
            "recommendations": self._generate_recommendations(comparison_results),
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        with open(output_dir / "model_comparison.json", "w", encoding="utf-8") as f:
            json.dump(model_comparison, f, indent=2, ensure_ascii=False)

        with open(output_dir / "performance_metrics.json", "w", encoding="utf-8") as f:
            json.dump(performance_metrics, f, indent=2, ensure_ascii=False)

        with open(output_dir / "evaluation_summary.json", "w", encoding="utf-8") as f:
            json.dump(evaluation_summary, f, indent=2, ensure_ascii=False)

        self.logger.info("âœ… è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        if not results["model_results"]:
            return ["ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"]

        best_model = results["model_results"][0]

        # éå­¦ç¿’ãƒã‚§ãƒƒã‚¯
        if best_model["validation_metrics"]["overfitting_risk"] != "ä½ãƒªã‚¹ã‚¯":
            recommendations.append(
                f"âš ï¸ éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {best_model['validation_metrics']['overfitting_risk']}"
            )

        # RÂ²ãŒé«˜ã™ãã‚‹å ´åˆ
        if best_model["validation_metrics"]["r2"] > 0.99:
            recommendations.append("âš ï¸ RÂ²ãŒ0.99ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚„éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡
        mae = best_model["validation_metrics"]["mae"]
        if mae < 10:
            recommendations.append("âœ… ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯è‰¯å¥½ã§ã™ï¼ˆMAE < 10å††ï¼‰")
        elif mae < 50:
            recommendations.append("âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯ä¸­ç¨‹åº¦ã§ã™ï¼ˆMAE < 50å††ï¼‰")
        else:
            recommendations.append("âŒ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯æ”¹å–„ãŒå¿…è¦ã§ã™ï¼ˆMAE > 50å††ï¼‰")

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®å®‰å®šæ€§
        cv_std = best_model["validation_metrics"]["cv_mae_std"]
        if cv_std < 5:
            recommendations.append("âœ… ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã¯å®‰å®šã—ã¦ã„ã¾ã™")
        else:
            recommendations.append("âš ï¸ ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã«ã°ã‚‰ã¤ããŒã‚ã‚Šã¾ã™")

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿è©•ä¾¡ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹å§‹")

    # è¨­å®š
    output_dir = Path("web-app/public/data")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    input_file = "processed_stock_data.csv"
    if not Path(input_file).exists():
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return

    df = pd.read_csv(input_file)
    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} è¡Œ")

    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®è¨­å®š
    features = ["Close", "Volume", "Open", "High", "Low", "SMA_5", "SMA_25", "SMA_50"]
    target = "Close"

    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
    available_features = [col for col in features if col in df.columns]
    if not available_features:
        logger.error("åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    X = df[available_features].dropna()
    y = df[target].iloc[: len(X)]

    logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {available_features}")
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X)} ã‚µãƒ³ãƒ—ãƒ«")

    # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ
    validator = EnhancedDataValidator()

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_val, X_test, y_train, y_val, y_test = validator.split_data_properly(
        X, y, test_size=0.2, val_size=0.2
    )

    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    comparison_results = validator.compare_models(
        X_train, y_train, X_val, y_val, X_test, y_test
    )

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    validator.generate_evaluation_report(comparison_results, output_dir)

    logger.info("âœ… ãƒ‡ãƒ¼ã‚¿è©•ä¾¡ãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


if __name__ == "__main__":
    main()
