#!/usr/bin/env python3
"""
ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰max_workersã‚’èª­ã¿è¾¼ã¿ã€å‹•çš„ã«ä¸¦åˆ—å‡¦ç†ã‚’æœ€é©åŒ–
"""

import os
import sys
import time
import psutil
import logging
from typing import Dict, Any, Optional, List, Callable
from unified_parallel_processing_system import (
    execute_parallel,
    get_parallel_config,
    set_parallel_config
)
from unified_system import get_unified_system
from concurrent.futures import as_completed, ProcessPoolExecutor, ThreadPoolExecutor
from functools import wraps
import multiprocessing as mp
import threading
from dataclasses import dataclass
import yaml

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™"""

    cpu_usage: float
    memory_usage: float
    active_threads: int
    active_processes: int
    timestamp: float


class ParallelProcessingOptimizer:
    """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path: str = "config_final.yaml"):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.max_workers = self._get_max_workers()
        self.current_workers = self.max_workers
        self.performance_history: List[PerformanceMetrics] = []
        self.lock = threading.Lock()

        # å‹•çš„èª¿æ•´è¨­å®š
        self.auto_adjust = self.config.get("performance", {}).get("auto_adjust", True)
        self.adjustment_interval = self.config.get("performance", {}).get(
            "adjustment_interval", 30
        )
        self.min_workers = 1
        self.max_workers_limit = min(self.max_workers * 2, mp.cpu_count() * 2)

        logger.info(f"ğŸš€ ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   - æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        logger.info(f"   - å‹•çš„èª¿æ•´: {'æœ‰åŠ¹' if self.auto_adjust else 'ç„¡åŠ¹'}")

    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _get_max_workers(self) -> int:
        """è¨­å®šã‹ã‚‰æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å–å¾—"""
        try:
            # ç’°å¢ƒåˆ¥è¨­å®šã‚’å„ªå…ˆ
            environment = self.config.get("system", {}).get("environment", "production")
            env_config = self.config.get("environments", {}).get(environment, {})

            if (
                "performance" in env_config
                and "max_workers" in env_config["performance"]
            ):
                return env_config["performance"]["max_workers"]

            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return self.config.get("performance", {}).get("max_workers", 4)
        except Exception as e:
            logger.warning(f"max_workersè¨­å®šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return min(4, mp.cpu_count())

    def get_performance_metrics(self) -> PerformanceMetrics:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’å–å¾—"""
        return PerformanceMetrics(
            cpu_usage=psutil.cpu_percent(interval=1),
            memory_usage=psutil.virtual_memory().percent,
            active_threads=threading.active_count(),
            active_processes=len(psutil.pids()),
            timestamp=time.time(),
        )

    def should_adjust_workers(self) -> bool:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’èª¿æ•´ã™ã¹ãã‹åˆ¤å®š"""
        if not self.auto_adjust:
            return False

        with self.lock:
            if len(self.performance_history) < 5:
                return False

            recent_metrics = self.performance_history[-5:]
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(
                recent_metrics
            )

            # CPUä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™
            if avg_cpu > 80 and self.current_workers > self.min_workers:
                return True

            # CPUä½¿ç”¨ç‡ãŒä½ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å¢—ã‚„ã™
            if avg_cpu < 30 and self.current_workers < self.max_workers_limit:
                return True

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã™ãã‚‹å ´åˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æ¸›ã‚‰ã™
            if avg_memory > 85 and self.current_workers > self.min_workers:
                return True

        return False

    def adjust_workers(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å‹•çš„èª¿æ•´"""
        if not self.should_adjust_workers():
            return

        with self.lock:
            recent_metrics = self.performance_history[-5:]
            avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(
                recent_metrics
            )

            old_workers = self.current_workers

            # CPUä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if avg_cpu > 80:
                self.current_workers = max(self.min_workers, self.current_workers - 1)
            elif avg_cpu < 30:
                self.current_workers = min(
                    self.max_workers_limit, self.current_workers + 1
                )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã«åŸºã¥ãèª¿æ•´
            if avg_memory > 85:
                self.current_workers = max(self.min_workers, self.current_workers - 1)

            if old_workers != self.current_workers:
                logger.info(
                    f"ğŸ”„ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´: {old_workers} â†’ {self.current_workers}"
                )
                logger.info(f"   - CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
                logger.info(f"   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {avg_memory:.1f}%")

    def monitor_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                metrics = self.get_performance_metrics()

                with self.lock:
                    self.performance_history.append(metrics)
                    # å±¥æ­´ã‚’æœ€æ–°10ä»¶ã«åˆ¶é™
                    if len(self.performance_history) > 10:
                        self.performance_history = self.performance_history[-10:]

                # å‹•çš„èª¿æ•´
                self.adjust_workers()

                time.sleep(self.adjustment_interval)

            except Exception as e:
                logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(5)

    def get_optimal_executor(self, task_type: str = "cpu_intensive") -> tuple:
        """
        ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©ãªExecutorã‚’å–å¾—

        Args:
            task_type: "cpu_intensive", "io_intensive", "mixed"

        Returns:
            (Executor, max_workers)
        """
        with self.lock:
            current_workers = self.current_workers

        if task_type == "cpu_intensive":
            # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã¯ProcessPoolExecutor
            return ProcessPoolExecutor, min(current_workers, mp.cpu_count())
        elif task_type == "io_intensive":
            # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã¯ThreadPoolExecutor
            return ThreadPoolExecutor, current_workers * 2
        else:  # mixed
            # æ··åˆã‚¿ã‚¹ã‚¯ã¯ThreadPoolExecutor
            return ThreadPoolExecutor, current_workers

    def parallel_execute(
        self,
        tasks: List[Callable],
        task_type: str = "mixed",
        timeout: Optional[int] = None,
    ) -> List[Any]:
        """
        ä¸¦åˆ—å®Ÿè¡Œ

        Args:
            tasks: å®Ÿè¡Œã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

        Returns:
            å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not tasks:
            return []

        executor_class, max_workers = self.get_optimal_executor(task_type)

        logger.info(f"ğŸš€ ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
        logger.info(f"   - ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")
        logger.info(f"   - ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
        logger.info(f"   - Executor: {executor_class.__name__}")

        start_time = time.time()
        results = []

        try:
            with executor_class(max_workers=max_workers) as executor:
                # ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
                future_to_task = {
                    executor.submit(task): i for i, task in enumerate(tasks)
                }

                # çµæœã‚’åé›†
                for future in as_completed(future_to_task, timeout=timeout):
                    try:
                        result = future.result()
                        results.append((future_to_task[future], result))
                    except Exception as e:
                        task_index = future_to_task[future]
                        logger.error(f"ã‚¿ã‚¹ã‚¯ {task_index} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                        results.append((task_index, None))

        except Exception as e:
            logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return []

        # çµæœã‚’å…ƒã®é †åºã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x[0])
        final_results = [result for _, result in results]

        execution_time = time.time() - start_time
        logger.info(f"âœ… ä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
        logger.info(f"   - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        logger.info(
            f"   - æˆåŠŸç‡: {len([r for r in final_results if r is not None])}/{len(tasks)}"
        )

        return final_results

    def parallel_map(
        self,
        func: Callable,
        iterable: List[Any],
        task_type: str = "mixed",
        chunk_size: Optional[int] = None,
    ) -> List[Any]:
        """
        ä¸¦åˆ—ãƒãƒƒãƒ—å®Ÿè¡Œ

        Args:
            func: å®Ÿè¡Œã™ã‚‹é–¢æ•°
            iterable: ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º

        Returns:
            å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
        """
        if not iterable:
            return []

        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®è‡ªå‹•è¨ˆç®—
        if chunk_size is None:
            executor_class, max_workers = self.get_optimal_executor(task_type)
            chunk_size = max(1, len(iterable) // max_workers)

        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = [
            iterable[i : i + chunk_size] for i in range(0, len(iterable), chunk_size)
        ]

        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°
        def process_chunk(chunk):
            return [func(item) for item in chunk]

        # ä¸¦åˆ—å®Ÿè¡Œ
        chunk_results = self.parallel_execute(
            [lambda c=chunk: process_chunk(c) for chunk in chunks], task_type
        )

        # çµæœã‚’å¹³å¦åŒ–
        results = []
        for chunk_result in chunk_results:
            if chunk_result is not None:
                results.extend(chunk_result)

        return results


def parallel_optimized(func):
    """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å–å¾—
        optimizer = getattr(parallel_optimized, "optimizer", None)
        if optimizer is None:
            optimizer = get_unified_system()
            parallel_optimized.optimizer = optimizer

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        task_type = kwargs.get("task_type", "mixed")

        # ä¸¦åˆ—å®Ÿè¡Œ
        if hasattr(func, "__iter__"):
            return optimizer.parallel_execute(func, task_type)
        else:
            return optimizer.parallel_execute([func], task_type)[0]

    return wrapper


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
_global_optimizer = None


def get_optimizer() -> ParallelProcessingOptimizer:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’å–å¾—"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = get_unified_system()
    return _global_optimizer


def start_performance_monitoring():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹"""
    optimizer = get_optimizer()
    monitor_thread = threading.Thread(target=optimizer.monitor_performance, daemon=True)
    monitor_thread.start()
    logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹")


def parallel_execute_unified(
    tasks: List[Callable], task_type: str = "mixed", timeout: Optional[int] = None
) -> List[Any]:
    """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—å®Ÿè¡Œ"""
    optimizer = get_optimizer()
    return optimizer.parallel_execute(tasks, task_type, timeout)


def parallel_map_unified(
    func: Callable,
    iterable: List[Any],
    task_type: str = "mixed",
    chunk_size: Optional[int] = None,
) -> List[Any]:
    """æœ€é©åŒ–ã•ã‚ŒãŸä¸¦åˆ—ãƒãƒƒãƒ—"""
    optimizer = get_optimizer()
    return optimizer.parallel_map(func, iterable, task_type, chunk_size)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    optimizer = get_unified_system()

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
    start_performance_monitoring()

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
    def test_task(x):
        time.sleep(0.1)
        return x * 2

    # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
    tasks = [lambda x=i: test_task(x) for i in range(10)]
    results = parallel_execute_unified(tasks, task_type="cpu_intensive")
    print(f"çµæœ: {results}")

    # ä¸¦åˆ—ãƒãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
    data = list(range(20))
    results = parallel_map_unified(test_task, data, task_type="mixed")
    print(f"ãƒãƒƒãƒ—çµæœ: {results}")
