#!/usr/bin/env python3
"""
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨å‡¦ç†æ™‚é–“ã‚’æœ€é©åŒ–
"""

import pandas as pd
import numpy as np
import psutil
import gc
import logging
from typing import Dict, List, Optional, Generator, Tuple, Any
from unified_parallel_processing_system import (
    execute_parallel, 
    get_parallel_config, 
    set_parallel_config
)
from unified_system import get_unified_system
import multiprocessing as mp
from functools import lru_cache
import tracemalloc
from contextlib import contextmanager

logger = logging.getLogger(__name__)


class MemoryMonitor:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.peak_memory = 0
        self.memory_history = []
        self.process = psutil.Process()
        self.initial_memory = self.get_current_memory()

    def get_current_memory(self) -> int:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        return self.process.memory_info().rss / 1024 / 1024

    def track_memory(self, operation_name: str = ""):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¿½è·¡"""
        current_memory = self.get_current_memory()
        self.memory_history.append(
            {
                "operation": operation_name,
                "memory_mb": current_memory,
                "timestamp": pd.Timestamp.now(),
            }
        )

        if current_memory > self.peak_memory:
            self.peak_memory = current_memory

        logger.debug(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {current_memory:.1f}MB ({operation_name})")
        return current_memory

    def get_memory_stats(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’å–å¾—"""
        current_memory = self.get_current_memory()
        return {
            "current_mb": current_memory,
            "peak_mb": self.peak_memory,
            "increase_mb": current_memory - self.initial_memory,
            "history_count": len(self.memory_history),
        }


class MemoryOptimizedProcessor:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""

    def __init__(self, chunk_size: int = 10000, memory_limit_mb: int = 2048):
        self.chunk_size = chunk_size
        self.memory_limit_mb = memory_limit_mb
        self.monitor = MemoryMonitor()
        self.logger = logging.getLogger(__name__)

    def process_large_data(
        self, data_source: str, processing_func, **kwargs
    ) -> Generator[pd.DataFrame, None, None]:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        self.logger.info(f"ğŸ“Š å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self.chunk_size})")

        try:
            for chunk in pd.read_csv(data_source, chunksize=self.chunk_size):
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
                if self.monitor.get_current_memory() > self.memory_limit_mb:
                    self.logger.warning(
                        "âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
                    )
                    gc.collect()

                # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
                processed_chunk = self._process_chunk(chunk, processing_func, **kwargs)
                yield processed_chunk

                # ãƒ¡ãƒ¢ãƒªè¿½è·¡
                self.monitor.track_memory("chunk_processing")

        except Exception as e:
            self.logger.error(f"âŒ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _process_chunk(
        self, chunk: pd.DataFrame, processing_func, **kwargs
    ) -> pd.DataFrame:
        """ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†"""
        try:
            return processing_func(chunk, **kwargs)
        except Exception as e:
            self.logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    @contextmanager
    def memory_context(self, operation_name: str):
        """ãƒ¡ãƒ¢ãƒªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        initial_memory = self.monitor.get_current_memory()
        self.logger.info(f"ğŸ”§ {operation_name} é–‹å§‹ (ãƒ¡ãƒ¢ãƒª: {initial_memory:.1f}MB)")

        try:
            yield
        finally:
            final_memory = self.monitor.get_current_memory()
            memory_increase = final_memory - initial_memory
            self.logger.info(
                f"âœ… {operation_name} å®Œäº† (ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase:.1f}MB)"
            )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§ãã„å ´åˆã¯ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            if memory_increase > 100:  # 100MBä»¥ä¸Šå¢—åŠ ã—ãŸå ´åˆ
                self.logger.info("ğŸ§¹ ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
                gc.collect()


class DataTypeOptimizer:
    """ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    @staticmethod
    def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å‹ã‚’æœ€é©åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›"""
        logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ã‚’é–‹å§‹")

        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024

        for col in df.columns:
            if df[col].dtype == "object":
                # æ–‡å­—åˆ—ã‚«ãƒ©ãƒ ã®æœ€é©åŒ–
                if df[col].nunique() / len(df) < 0.5:  # ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãŒä½ã„å ´åˆ
                    df[col] = df[col].astype("category")
            elif df[col].dtype == "int64":
                # æ•´æ•°å‹ã®æœ€é©åŒ–
                if df[col].min() >= 0:
                    if df[col].max() < 255:
                        df[col] = df[col].astype("uint8")
                    elif df[col].max() < 65535:
                        df[col] = df[col].astype("uint16")
                    elif df[col].max() < 4294967295:
                        df[col] = df[col].astype("uint32")
                else:
                    if df[col].min() > -128 and df[col].max() < 127:
                        df[col] = df[col].astype("int8")
                    elif df[col].min() > -32768 and df[col].max() < 32767:
                        df[col] = df[col].astype("int16")
                    elif df[col].min() > -2147483648 and df[col].max() < 2147483647:
                        df[col] = df[col].astype("int32")
            elif df[col].dtype == "float64":
                # æµ®å‹•å°æ•°ç‚¹å‹ã®æœ€é©åŒ–
                if df[col].min() >= 0:
                    df[col] = pd.to_numeric(df[col], downcast="float")
                else:
                    df[col] = pd.to_numeric(df[col], downcast="float")

        optimized_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        reduction = original_memory - optimized_memory
        reduction_pct = (reduction / original_memory) * 100

        logger.info(
            f"âœ… ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–å®Œäº†: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB ({reduction_pct:.1f}%å‰Šæ¸›)"
        )

        return df


class ParallelProcessor:
    """ä¸¦åˆ—å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, max_workers: int = None):
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰max_workersã‚’èª­ã¿è¾¼ã¿
        try:
            import yaml

            with open("config_final.yaml", "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            self.max_workers = max_workers or config.get("performance", {}).get(
                "max_workers", 4
            )
        except Exception:
            self.max_workers = max_workers or min(4, mp.cpu_count())
        self.logger = logging.getLogger(__name__)

    def process_indicators_parallel(
        self, df: pd.DataFrame, indicator_funcs: List[callable]
    ) -> pd.DataFrame:
        """æŠ€è¡“æŒ‡æ¨™ã‚’ä¸¦åˆ—å‡¦ç†ã§è¨ˆç®—"""
        self.logger.info(f"ğŸš€ ä¸¦åˆ—å‡¦ç†é–‹å§‹ (ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers})")

        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        chunk_size = len(df) // self.max_workers
        chunks = [df.iloc[i : i + chunk_size] for i in range(0, len(df), chunk_size)]

        results = []
        with get_unified_system().execute_parallel(self.max_workers) as executor:
            # å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦ä¸¦åˆ—å‡¦ç†
            futures = []
            for i, chunk in enumerate(chunks):
                future = executor.submit(
                    self._process_chunk_indicators, chunk, indicator_funcs
                )
                futures.append((i, future))

            # çµæœã‚’åé›†
            for i, future in futures:
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {i} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    results.append(chunk)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿”ã™

        # çµæœã‚’çµåˆ
        final_result = pd.concat(results, ignore_index=True)
        self.logger.info(f"âœ… ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(final_result)}è¡Œ")

        return final_result

    def _process_chunk_indicators(
        self, chunk: pd.DataFrame, indicator_funcs: List[callable]
    ) -> pd.DataFrame:
        """ãƒãƒ£ãƒ³ã‚¯ã®æŠ€è¡“æŒ‡æ¨™è¨ˆç®—"""
        result = chunk.copy()

        for func in indicator_funcs:
            try:
                result = func(result)
            except Exception as e:
                self.logger.warning(f"âš ï¸ æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return result


class CacheManager:
    """è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""

    def __init__(self, max_size: int = 100):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}
        self.logger = logging.getLogger(__name__)

    def get_cached_result(self, key: str, compute_func: callable, *args, **kwargs):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã€ãªã‘ã‚Œã°è¨ˆç®—"""
        if key in self.cache:
            self.access_count[key] = self.access_count.get(key, 0) + 1
            self.logger.debug(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {key}")
            return self.cache[key]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.cache) >= self.max_size:
            self._evict_least_used()

        # è¨ˆç®—å®Ÿè¡Œ
        result = compute_func(*args, **kwargs)
        self.cache[key] = result
        self.access_count[key] = 1

        self.logger.debug(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {key}")
        return result

    def _evict_least_used(self):
        """æœ€ã‚‚ä½¿ç”¨é »åº¦ã®ä½ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        if not self.cache:
            return

        least_used_key = min(
            self.access_count.keys(), key=lambda k: self.access_count[k]
        )
        del self.cache[least_used_key]
        del self.access_count[least_used_key]

        self.logger.debug(f"ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤: {least_used_key}")

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        self.access_count.clear()
        self.logger.info("ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")


class OptimizedTechnicalIndicators:
    """æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.memory_processor = MemoryOptimizedProcessor()
        self.cache_manager = CacheManager()
        self.parallel_processor = ParallelProcessor()
        self.dtype_optimizer = DataTypeOptimizer()

    def calculate_indicators_optimized(
        self, df: pd.DataFrame, config: Dict = None
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—"""
        self.logger.info("ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚’é–‹å§‹")

        with self.memory_processor.memory_context("æŠ€è¡“æŒ‡æ¨™è¨ˆç®—"):
            # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
            df = self.dtype_optimizer.optimize_dtypes(df)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = f"indicators_{hash(str(df.shape))}"

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã¾ãŸã¯è¨ˆç®—
            result = self.cache_manager.get_cached_result(
                cache_key, self._calculate_indicators_internal, df, config
            )

            return result

    def _calculate_indicators_internal(
        self, df: pd.DataFrame, config: Dict = None
    ) -> pd.DataFrame:
        """å†…éƒ¨çš„ãªæŒ‡æ¨™è¨ˆç®—"""
        result_df = df.copy()

        # ä¸¦åˆ—å‡¦ç†å¯èƒ½ãªæŒ‡æ¨™è¨ˆç®—é–¢æ•°ã®ãƒªã‚¹ãƒˆ
        indicator_funcs = [
            self._calculate_moving_averages_optimized,
            self._calculate_rsi_optimized,
            self._calculate_macd_optimized,
            self._calculate_bollinger_bands_optimized,
        ]

        # ä¸¦åˆ—å‡¦ç†ã§æŒ‡æ¨™è¨ˆç®—
        result_df = self.parallel_processor.process_indicators_parallel(
            result_df, indicator_funcs
        )

        return result_df

    def _calculate_moving_averages_optimized(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸç§»å‹•å¹³å‡è¨ˆç®—"""
        result_df = df.copy()

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’å‡¦ç†
        if "Close" in df.columns:
            windows = [5, 10, 20, 50]
            for window in windows:
                if len(df) >= window:
                    result_df[f"SMA_{window}"] = (
                        df["Close"].rolling(window=window, min_periods=1).mean()
                    )

        return result_df

    def _calculate_rsi_optimized(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸRSIè¨ˆç®—"""
        result_df = df.copy()

        if "Close" in df.columns and len(df) >= 14:
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸRSIè¨ˆç®—
            delta = df["Close"].diff()
            gain = delta.where(delta > 0, 0)
            loss = -delta.where(delta < 0, 0)

            # æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚ŠåŠ¹ç‡çš„ã«è¨ˆç®—
            avg_gain = gain.ewm(span=14, min_periods=1).mean()
            avg_loss = loss.ewm(span=14, min_periods=1).mean()

            rs = avg_gain / avg_loss
            result_df["RSI"] = 100 - (100 / (1 + rs))

        return result_df

    def _calculate_macd_optimized(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸMACDè¨ˆç®—"""
        result_df = df.copy()

        if "Close" in df.columns and len(df) >= 26:
            # æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—
            ema_12 = df["Close"].ewm(span=12, min_periods=1).mean()
            ema_26 = df["Close"].ewm(span=26, min_periods=1).mean()

            result_df["MACD"] = ema_12 - ema_26
            result_df["MACD_Signal"] = (
                result_df["MACD"].ewm(span=9, min_periods=1).mean()
            )
            result_df["MACD_Histogram"] = result_df["MACD"] - result_df["MACD_Signal"]

        return result_df

    def _calculate_bollinger_bands_optimized(self, df: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰è¨ˆç®—"""
        result_df = df.copy()

        if "Close" in df.columns and len(df) >= 20:
            # ç§»å‹•å¹³å‡ã¨æ¨™æº–åå·®ã‚’ä¸€åº¦ã«è¨ˆç®—
            sma = df["Close"].rolling(window=20, min_periods=1).mean()
            std = df["Close"].rolling(window=20, min_periods=1).std()

            result_df["BB_Middle"] = sma
            result_df["BB_Upper"] = sma + (std * 2)
            result_df["BB_Lower"] = sma - (std * 2)
            result_df["BB_Width"] = (
                result_df["BB_Upper"] - result_df["BB_Lower"]
            ) / result_df["BB_Middle"]

        return result_df


def create_optimized_processor(
    chunk_size: int = 10000, memory_limit_mb: int = 2048
) -> MemoryOptimizedProcessor:
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ"""
    return MemoryOptimizedProcessor(
        chunk_size=chunk_size, memory_limit_mb=memory_limit_mb
    )


def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–"""
    optimizer = DataTypeOptimizer()
    return optimizer.optimize_dtypes(df)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    logging.basicConfig(level=logging.INFO)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    dates = pd.date_range("2020-01-01", periods=10000, freq="D")
    np.random.seed(42)

    sample_data = pd.DataFrame(
        {
            "Date": dates,
            "Open": np.random.uniform(100, 200, 10000),
            "High": np.random.uniform(100, 200, 10000),
            "Low": np.random.uniform(100, 200, 10000),
            "Close": np.random.uniform(100, 200, 10000),
            "Volume": np.random.randint(1000000, 10000000, 10000),
        }
    )

    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    processor = create_optimized_processor()
    monitor = MemoryMonitor()

    print("ğŸ“Š ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"åˆæœŸãƒ¡ãƒ¢ãƒª: {monitor.get_current_memory():.1f}MB")

    # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
    optimized_data = optimize_dataframe_memory(sample_data)
    print(f"æœ€é©åŒ–å¾Œãƒ¡ãƒ¢ãƒª: {monitor.get_current_memory():.1f}MB")

    # æŠ€è¡“æŒ‡æ¨™è¨ˆç®—
    calculator = OptimizedTechnicalIndicators()
    result = calculator.calculate_indicators_optimized(optimized_data)

    print(f"æœ€çµ‚ãƒ¡ãƒ¢ãƒª: {monitor.get_current_memory():.1f}MB")
    print(f"ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ: {monitor.get_memory_stats()}")
    print(f"çµæœãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {result.shape}")
