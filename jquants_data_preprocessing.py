#!/usr/bin/env python3
"""
æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæŠ€è¡“æŒ‡æ¨™çµ±åˆç‰ˆï¼‰
ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™ã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ
"""

import pandas as pd
import numpy as np
import logging
import os
from technical_indicators import TechnicalIndicators, get_enhanced_features_list
from data_validator import DataValidator
from unified_error_handler import get_unified_error_handler

# è¨­å®šã‚’èª­ã¿è¾¼ã¿
from config_loader import ConfigLoader

config_loader = ConfigLoader()
preprocessing_config = config_loader.get_preprocessing_config()

# å¼·åŒ–ã•ã‚ŒãŸãƒ­ã‚°è¨­å®š
from enhanced_logging import setup_enhanced_logging, LogLevel, LogCategory

enhanced_logger = setup_enhanced_logging("DataPreprocessing", LogLevel.INFO)
logger = enhanced_logger.get_logger()


def validate_input_file(input_file):
    """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã¨ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ã‚’æ¤œè¨¼"""
    error_handler = get_unified_error_handler("validate_input_file")

    logger.info(f"ğŸ” å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼: {input_file}")

    try:
        if not os.path.exists(input_file):
            error_msg = f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}"
            error_handler.handle_file_error(
                FileNotFoundError(error_msg), input_file, "read"
            )
            raise FileNotFoundError(error_msg)

        if not os.access(input_file, os.R_OK):
            error_msg = f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šæ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {input_file}"
            error_handler.handle_file_error(
                PermissionError(error_msg), input_file, "read"
            )
            raise PermissionError(error_msg)

        file_size = os.path.getsize(input_file)
        if file_size == 0:
            error_msg = f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {input_file}"
            error_handler.log_error(
                ValueError(error_msg),
                "å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
                {
                    "file_path": input_file,
                    "file_size": file_size,
                    "file_exists": True,
                    "file_readable": True,
                },
            )
            raise ValueError(error_msg)

        logger.info(f"âœ… å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†: {file_size} bytes")
        return True

    except Exception as e:
        error_handler.log_error(
            e,
            "å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
            {
                "file_path": input_file,
                "file_exists": os.path.exists(input_file) if input_file else False,
                "file_readable": (
                    os.access(input_file, os.R_OK)
                    if input_file and os.path.exists(input_file)
                    else False
                ),
            },
        )
        raise


def load_and_clean_data(input_file):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå …ç‰¢æ€§å¼·åŒ–ç‰ˆï¼‰"""
    error_handler = get_unified_error_handler("load_and_clean_data")
    # specific_error_handler = get_specific_error_handler("load_and_clean_data")  # çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ä¸è¦

    logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")

    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
    validate_input_file(input_file)

    try:
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡ºï¼‰
        encodings = ["utf-8", "shift_jis", "cp932", "utf-8-sig"]
        df = None
        successful_encoding = None

        for encoding in encodings:
            try:
                df = pd.read_csv(input_file, encoding=encoding)
                successful_encoding = encoding
                logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})")
                break
            except UnicodeDecodeError as e:
                logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° {encoding} ã§ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
                continue
            except Exception as e:
                error_handler.log_error(
                    e,
                    f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})",
                    {"encoding": encoding, "file_path": input_file},
                )
                continue

        if df is None:
            error_msg = "ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ"
            error_handler.log_error(
                ValueError(error_msg),
                "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã‚¨ãƒ©ãƒ¼",
                {"file_path": input_file, "tried_encodings": encodings},
            )
            raise ValueError(error_msg)

        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æ¤œè¨¼
        if df.empty:
            error_msg = "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™"
            error_handler.log_error(
                ValueError(error_msg),
                "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
                {
                    "file_path": input_file,
                    "encoding": successful_encoding,
                    "data_shape": df.shape,
                },
            )
            raise ValueError(error_msg)

        enhanced_logger.log_data_info("èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿", shape=df.shape)

        # ãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        try:
            df["Date"] = pd.to_datetime(df["Date"])
            logger.info("âœ… æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å¤‰æ›å®Œäº†")
        except Exception as e:
            error_handler.handle_data_error(e, "æ—¥ä»˜ã‚«ãƒ©ãƒ å¤‰æ›", df.shape, "Date")
            logger.error(f"âŒ æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’é¸æŠ
        columns = preprocessing_config.get(
            "columns",
            ["Date", "Code", "CompanyName", "High", "Low", "Open", "Close", "Volume"],
        )
        available_columns = [col for col in columns if col in df.columns]
        missing_columns = [col for col in columns if col not in df.columns]

        if missing_columns:
            logger.warning(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ : {missing_columns}")

        df = df[available_columns]
        logger.info(f"âœ… ã‚«ãƒ©ãƒ é¸æŠå®Œäº†: {len(available_columns)}å€‹")

        # æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å¤‰æ›
        numeric_columns = ["Open", "High", "Low", "Close", "Volume"]
        for col in numeric_columns:
            if col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors="coerce")
                except Exception as e:
                    error_handler.handle_data_error(
                        e, f"æ•°å€¤ã‚«ãƒ©ãƒ å¤‰æ› ({col})", df.shape, col
                    )
                    logger.warning(f"âš ï¸ {col}ã‚«ãƒ©ãƒ ã®æ•°å€¤å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼: {e}")

        # å‹å®‰å…¨ãªæ¬ æå€¤å‡¦ç†
        from type_safe_validator import TypeSafeValidator

        validator = TypeSafeValidator(strict_mode=True)

        # ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼
        validation_result = validator.validate_numeric_columns(
            df, ["Open", "High", "Low", "Close", "Volume"]
        )
        if not validation_result["is_valid"]:
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            for error in validation_result["errors"]:
                logger.error(f"  - {error}")
            raise ValueError("ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼ã‚¨ãƒ©ãƒ¼")

        # å®‰å…¨ãªæ¬ æå€¤å‡¦ç†
        df = validator.safe_nan_handling(df, strategy="forward_fill")

        missing_count = df.isnull().sum().sum()
        if missing_count > 0:
            logger.warning(f"âš ï¸ {missing_count}ä»¶ã®NaNå€¤ãŒæ®‹ã£ã¦ã„ã¾ã™")
            # æ®‹ã‚Šã®NaNå€¤ã¯è¡Œå‰Šé™¤ã§å‡¦ç†
            df = df.dropna()
            logger.info(f"âœ… æ®‹ã‚Šã®NaNå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤: {missing_count}è¡Œ")

        # é‡è¤‡è¡Œã®å‰Šé™¤
        initial_rows = len(df)
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            df = df.drop_duplicates()
            logger.info(f"ğŸ—‘ï¸ é‡è¤‡è¡Œã‚’å‰Šé™¤: {duplicates}è¡Œ ({initial_rows} -> {len(df)})")

        # ç•°å¸¸å€¤ã®æ¤œå‡ºã¨å‡¦ç†
        numeric_df = df.select_dtypes(include=[np.number])
        for col in numeric_df.columns:
            if col in ["Open", "High", "Low", "Close", "Volume"]:
                # è² ã®å€¤ã®ãƒã‚§ãƒƒã‚¯
                negative_count = (df[col] < 0).sum()
                if negative_count > 0:
                    logger.warning(f"âš ï¸ {col}ã«è² ã®å€¤ãŒ{negative_count}ä»¶ã‚ã‚Šã¾ã™")

                # ç•°å¸¸ã«å¤§ããªå€¤ã®ãƒã‚§ãƒƒã‚¯
                q99 = df[col].quantile(0.99)
                outliers = (df[col] > q99 * 10).sum()
                if outliers > 0:
                    logger.warning(f"âš ï¸ {col}ã«ç•°å¸¸å€¤ãŒ{outliers}ä»¶ã‚ã‚Šã¾ã™")

        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆ
        logger.info(f"ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        logger.info(f"ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['Date'].min()} ï½ {df['Date'].max()}")

        return df

    except FileNotFoundError as e:
        error_handler.handle_file_error(e, input_file, "read")
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        raise

    except PermissionError as e:
        error_handler.handle_file_error(e, input_file, "read")
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    except ValueError as e:
        error_handler.log_error(
            e,
            "ãƒ‡ãƒ¼ã‚¿å€¤ã‚¨ãƒ©ãƒ¼",
            {
                "file_path": input_file,
                "data_shape": df.shape if "df" in locals() and df is not None else None,
            },
        )
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å€¤ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    except Exception as e:
        error_handler.log_error(
            e,
            "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼",
            {
                "file_path": input_file,
                "data_shape": df.shape if "df" in locals() and df is not None else None,
                "successful_encoding": (
                    successful_encoding if "successful_encoding" in locals() else None
                ),
            },
        )
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise


def engineer_basic_features(df):
    """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
    logger.info("ğŸ”§ åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é–‹å§‹")

    # åŸºæœ¬çš„ãªç§»å‹•å¹³å‡ï¼ˆæŠ€è¡“æŒ‡æ¨™ã¨é‡è¤‡å›é¿ï¼‰
    basic_sma_windows = preprocessing_config.get("sma_windows", [5, 10, 25, 50])
    for window in basic_sma_windows:
        if f"SMA_{window}" not in df.columns:
            df[f"SMA_{window}"] = df["Close"].rolling(window=window).mean()

    # åŸºæœ¬çš„ãªãƒ©ã‚°ç‰¹å¾´é‡
    lag_days = preprocessing_config.get("lag_days", [1, 3, 5])
    for lag in lag_days:
        df[f"Close_lag_{lag}"] = df["Close"].shift(lag)

    logger.info("âœ… åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†")
    return df


def preprocess_data(df):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°ï¼‰"""
    logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é–‹å§‹")

    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    df = engineer_basic_features(df)

    # é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    df = engineer_advanced_features(df)

    return df


def engineer_advanced_features(df):
    """é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™ã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
    logger.info("ğŸš€ é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚’é–‹å§‹")

    # æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å™¨ã‚’åˆæœŸåŒ–
    calculator = TechnicalIndicators()

    # æŠ€è¡“æŒ‡æ¨™è¨­å®šã‚’å–å¾—
    technical_config = preprocessing_config.get(
        "technical_indicators", calculator._get_default_config()
    )

    try:
        # å…¨ã¦ã®æŠ€è¡“æŒ‡æ¨™ã‚’è¨ˆç®—
        enhanced_df = calculator.calculate_all_indicators(df, technical_config)

        # æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸæŒ‡æ¨™ã‚’ãƒ­ã‚°å‡ºåŠ›
        original_columns = set(df.columns)
        new_columns = [
            col for col in enhanced_df.columns if col not in original_columns
        ]

        logger.info(f"ğŸ“ˆ è¿½åŠ ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™: {len(new_columns)}å€‹")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æŒ‡æ¨™ã‚’ãƒ­ã‚°å‡ºåŠ›
        momentum_indicators = [
            col
            for col in new_columns
            if any(x in col for x in ["RSI", "MACD", "Stoch"])
        ]
        volatility_indicators = [
            col for col in new_columns if any(x in col for x in ["BB_", "ATR"])
        ]
        volume_indicators = [
            col
            for col in new_columns
            if any(x in col for x in ["Volume", "VWAP", "OBV", "PVT"])
        ]
        trend_indicators = [
            col for col in new_columns if any(x in col for x in ["ADX", "DI", "CCI"])
        ]

        logger.info(f"  ğŸ“Š ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç³»: {len(momentum_indicators)}å€‹")
        logger.info(f"  ğŸ“ˆ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»: {len(volatility_indicators)}å€‹")
        logger.info(f"  ğŸ”Š ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»: {len(volume_indicators)}å€‹")
        logger.info(f"  ğŸ“‰ ãƒˆãƒ¬ãƒ³ãƒ‰ç³»: {len(trend_indicators)}å€‹")

        return enhanced_df

    except Exception as e:
        error_handler = get_unified_error_handler("engineer_advanced_features")
        error_handler.handle_data_processing_error(e, "æŠ€è¡“æŒ‡æ¨™è¨ˆç®—", df.shape)
        logger.error(f"âŒ æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning("ğŸ”„ åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ã§ç¶šè¡Œã—ã¾ã™")
        return df


def feature_selection_and_validation(df):
    """ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼"""
    logger.info("ğŸ¯ ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼ã‚’é–‹å§‹")

    # åˆ©ç”¨å¯èƒ½ãªæ‹¡å¼µç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
    enhanced_features = get_enhanced_features_list()

    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
    available_features = [col for col in enhanced_features if col in df.columns]
    missing_features = [col for col in enhanced_features if col not in df.columns]

    logger.info(
        f"âœ… åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡: {len(available_features)}/{len(enhanced_features)}"
    )

    if missing_features:
        logger.warning(f"âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡: {len(missing_features)}å€‹")
        for feature in missing_features[:5]:  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
            logger.warning(f"  - {feature}")
        if len(missing_features) > 5:
            logger.warning(f"  ... ãã®ä»– {len(missing_features) - 5}å€‹")

    # å‹å®‰å…¨ãªç„¡é™å€¤ãƒ»ç•°å¸¸å€¤ã®ãƒã‚§ãƒƒã‚¯
    from type_safe_validator import TypeSafeValidator

    validator = TypeSafeValidator(strict_mode=True)

    # æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹å®‰å…¨æ€§æ¤œè¨¼
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    validation_result = validator.validate_numeric_columns(df, numeric_columns)

    if not validation_result["is_valid"]:
        logger.error("âŒ æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã«å¤±æ•—")
        for error in validation_result["errors"]:
            logger.error(f"  - {error}")
        raise ValueError("æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å‹å®‰å…¨æ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼")

    # ç„¡é™å€¤ã®å®‰å…¨ãªå‡¦ç†
    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()
    if inf_count > 0:
        logger.warning(f"âš ï¸ ç„¡é™å€¤ã‚’æ¤œå‡º: {inf_count}å€‹")
        # ç„¡é™å€¤ã‚’NaNã«ç½®æ›ã—ã¦ã‹ã‚‰å®‰å…¨ã«å‡¦ç†
        df = df.replace([np.inf, -np.inf], np.nan)
        df = validator.safe_nan_handling(df, strategy="forward_fill")

    # åˆ†æ•£ãŒ0ã®ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
    numeric_df = df.select_dtypes(include=[np.number])
    zero_variance_cols = [
        col for col in numeric_df.columns if numeric_df[col].var() == 0
    ]

    if zero_variance_cols:
        logger.warning(f"âš ï¸ åˆ†æ•£0ã®ç‰¹å¾´é‡ã‚’æ¤œå‡º: {len(zero_variance_cols)}å€‹")
        for col in zero_variance_cols:
            logger.warning(f"  - {col}")

    return df, available_features


def validate_processed_data(df: pd.DataFrame) -> bool:
    """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    logger.info("ğŸ” å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã‚’é–‹å§‹")

    try:
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
        validator = DataValidator()

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®å®Ÿè¡Œ
        validation_results = validator.validate_stock_data(df)

        # æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨è¡¨ç¤º
        report = validator.generate_validation_report(validation_results)
        logger.info(f"\n{report}")

        # æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = "data_validation_report.txt"
        try:
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            logger.info(f"ğŸ“„ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_file}")
        except Exception as e:
            error_handler = get_unified_error_handler("validate_processed_data")
            error_handler.handle_file_error(e, report_file, "write")
            logger.warning(f"âš ï¸ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ã«å¤±æ•—: {e}")

        if not validation_results["is_valid"]:
            error_handler = get_unified_error_handler("validate_processed_data")
            error_handler.log_error(
                ValueError("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«å¤±æ•—"),
                "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
                {
                    "data_shape": df.shape,
                    "validation_results": validation_results,
                    "quality_score": validation_results.get("quality_score", 0),
                },
            )
            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False

        logger.info("âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        return True

    except Exception as e:
        error_handler = get_unified_error_handler("validate_processed_data")
        error_handler.log_error(
            e,
            "ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼",
            {
                "data_shape": df.shape if df is not None else None,
                "data_empty": df.empty if df is not None else None,
            },
        )
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆå …ç‰¢æ€§å¼·åŒ–ç‰ˆï¼‰"""
    error_handler = get_unified_error_handler("main_preprocessing")
    # specific_error_handler = get_specific_error_handler("main_preprocessing")  # çµ±åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ä¸è¦

    input_file = preprocessing_config.get("input_file", "stock_data.csv")
    output_file = preprocessing_config.get("output_file", "processed_stock_data.csv")

    try:
        enhanced_logger.log_operation_start(
            "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†", input_file=input_file, output_file=output_file
        )
        logger.info(f"ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_file}")
        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")

        # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        logger.info("ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
        df = load_and_clean_data(input_file)

        # 2. åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        logger.info("ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        df = engineer_basic_features(df)

        # 3. é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™ã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        logger.info("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—3: é«˜åº¦ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—")
        df = engineer_advanced_features(df)

        # 4. ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼
        logger.info("ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—4: ç‰¹å¾´é‡é¸æŠã¨æ¤œè¨¼")
        df, available_features = feature_selection_and_validation(df)

        # 5. æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†
        logger.info("ğŸ§¹ ã‚¹ãƒ†ãƒƒãƒ—5: æ¬ æå€¤ã®æœ€çµ‚å‡¦ç†")
        initial_rows = len(df)
        df = df.dropna()
        final_rows = len(df)
        dropped_rows = initial_rows - final_rows

        if dropped_rows > 0:
            logger.info(
                f"ğŸ—‘ï¸ æ¬ æå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤: {initial_rows} -> {final_rows} è¡Œ ({dropped_rows} è¡Œå‰Šé™¤)"
            )

        # 6. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®å®Ÿè¡Œ
        logger.info("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼")
        if not validate_processed_data(df):
            logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")

        # 7. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        logger.info("ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—7: ãƒ‡ãƒ¼ã‚¿ä¿å­˜")
        try:
            df.to_csv(output_file, index=False)
            logger.info(f"âœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {output_file}")
        except Exception as e:
            error_handler.handle_file_error(e, output_file, "write")
            raise

        # 8. æœ€çµ‚çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        logger.info("ğŸ“Š æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        logger.info(f"  ğŸ“ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
        logger.info(f"  ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['Date'].min()} ï½ {df['Date'].max()}")
        logger.info(f"  ğŸ“ˆ ç‰¹å¾´é‡æ•°: {len(df.columns)}å€‹")
        logger.info(f"  ğŸ¯ æ¨å¥¨ç‰¹å¾´é‡: {len(available_features)}å€‹")

        # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜ï¼ˆå‚è€ƒç”¨ï¼‰
        feature_list_file = output_file.replace(".csv", "_features.txt")
        try:
            with open(feature_list_file, "w", encoding="utf-8") as f:
                f.write("# åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ãƒªã‚¹ãƒˆ\n")
                f.write(f"# ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now()}\n")
                f.write(f"# ç·ç‰¹å¾´é‡æ•°: {len(available_features)}\n\n")
                for i, feature in enumerate(available_features, 1):
                    f.write(f"{i:3d}. {feature}\n")

            logger.info(f"ğŸ“ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ä¿å­˜: {feature_list_file}")
        except Exception as e:
            error_handler.handle_file_error(e, feature_list_file, "write")
            logger.warning(f"âš ï¸ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®ä¿å­˜ã«å¤±æ•—: {e}")

        enhanced_logger.log_operation_end(
            "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†",
            success=True,
            final_shape=df.shape,
            features_count=len(df.columns),
        )

    except FileNotFoundError as e:
        error_handler.handle_file_error(e, input_file, "read")
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
        logger.error("ğŸ’¡ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        raise

    except PermissionError as e:
        error_handler.handle_file_error(e, input_file, "read")
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿å–ã‚Šæ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        raise

    except ValueError as e:
        error_handler.log_error(
            e,
            "ãƒ‡ãƒ¼ã‚¿å€¤ã‚¨ãƒ©ãƒ¼",
            {
                "input_file": input_file,
                "output_file": output_file,
                "data_shape": df.shape if "df" in locals() and df is not None else None,
            },
        )
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å€¤ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error("ğŸ’¡ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        raise

    except Exception as e:
        error_handler.log_error(
            e,
            "å‰å‡¦ç†äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼",
            {
                "input_file": input_file,
                "output_file": output_file,
                "data_shape": df.shape if "df" in locals() and df is not None else None,
                "available_features_count": (
                    len(available_features)
                    if "available_features" in locals()
                    else None
                ),
            },
        )
        logger.error(f"âŒ å‰å‡¦ç†ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        logger.error("ğŸ’¡ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        raise


if __name__ == "__main__":
    main()
