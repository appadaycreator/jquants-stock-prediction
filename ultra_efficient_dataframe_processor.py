#!/usr/bin/env python3
"""
è¶…åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚³ãƒ”ãƒ¼ã‚’æœ€å°é™ã«æŠ‘ãˆã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’æœ€å¤§é™æ´»ç”¨
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Union, Callable, Any, Tuple
from contextlib import contextmanager
import gc
import psutil
from functools import wraps
import time
from dataclasses import dataclass
from unified_system import UnifiedSystem
import weakref
from copy import deepcopy

logger = logging.getLogger(__name__)


@dataclass
class DataFrameOptimizationStats:
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æœ€é©åŒ–çµ±è¨ˆ"""

    original_memory_mb: float
    optimized_memory_mb: float
    memory_reduction_mb: float
    memory_reduction_pct: float
    copy_operations_saved: int
    inplace_operations: int
    processing_time_saved: float
    dtype_optimizations: int


class UltraEfficientDataFrameProcessor:
    """è¶…åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, track_operations: bool = True):
        self.track_operations = track_operations
        self.operation_count = 0
        self.copy_operations_saved = 0
        self.inplace_operations = 0
        self.memory_saved = 0.0
        self.processing_time_saved = 0.0
        self.dtype_optimizations = 0
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‚ç…§ã‚’è¿½è·¡
        self.dataframe_refs = weakref.WeakSet()

    def smart_copy(
        self, df: pd.DataFrame, operation_name: str = "", force_copy: bool = False
    ) -> pd.DataFrame:
        """å¿…è¦æ™‚ã®ã¿ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆè¶…åŠ¹ç‡ç‰ˆï¼‰"""
        if self.track_operations:
            self.operation_count += 1

        # å¼·åˆ¶ã‚³ãƒ”ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if force_copy:
            self.logger.debug(f"ğŸ“‹ å¼·åˆ¶ã‚³ãƒ”ãƒ¼ä½œæˆ: {operation_name}")
            return df.copy()

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚’è©³ç´°ãƒã‚§ãƒƒã‚¯
        if self._will_modify_dataframe(df, operation_name):
            self.logger.debug(f"ğŸ“‹ ã‚³ãƒ”ãƒ¼ä½œæˆ: {operation_name}")
            return df.copy()
        else:
            self.logger.debug(f"â™»ï¸ ã‚³ãƒ”ãƒ¼å›é¿: {operation_name}")
            self.copy_operations_saved += 1
            return df

    def _will_modify_dataframe(self, df: pd.DataFrame, operation: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå¤‰æ›´ã•ã‚Œã‚‹ã‹ã©ã†ã‹ã‚’è©³ç´°åˆ¤å®š"""
        # å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ã„æ“ä½œ
        modifying_operations = [
            "fillna",
            "dropna",
            "drop_duplicates",
            "sort_values",
            "reset_index",
            "astype",
            "assign",
            "eval",
            "query",
            "set_index",
            "rename",
            "reindex",
            "interpolate",
            "replace",
            "update",
            "insert",
            "append",
            "concat",
        ]

        # å¤‰æ›´ã•ã‚Œãªã„å¯èƒ½æ€§ãŒé«˜ã„æ“ä½œ
        non_modifying_operations = [
            "head",
            "tail",
            "sample",
            "nlargest",
            "nsmallest",
            "describe",
            "info",
            "shape",
            "dtypes",
            "columns",
            "index",
            "values",
            "iloc",
            "loc",
            "at",
            "iat",
            "get",
            "isin",
            "isna",
            "notna",
            "duplicated",
            "memory_usage",
            "size",
            "ndim",
            "empty",
        ]

        # éƒ¨åˆ†çš„ãªå¤‰æ›´æ“ä½œï¼ˆæ¡ä»¶ä»˜ãã‚³ãƒ”ãƒ¼ï¼‰
        partial_modifying_operations = [
            "groupby",
            "rolling",
            "expanding",
            "ewm",
            "transform",
            "apply",
            "map",
            "applymap",
        ]

        operation_lower = operation.lower()

        if any(op in operation_lower for op in modifying_operations):
            return True
        elif any(op in operation_lower for op in non_modifying_operations):
            return False
        elif any(op in operation_lower for op in partial_modifying_operations):
            # éƒ¨åˆ†çš„ãªå¤‰æ›´ã®å ´åˆã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
            return self._check_dataframe_mutation_risk(df, operation)
        else:
            # ä¸æ˜ãªå ´åˆã¯å®‰å…¨ã®ãŸã‚ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
            return True

    def _check_dataframe_mutation_risk(self, df: pd.DataFrame, operation: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¤‰æ›´ãƒªã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ—¢ã«ä»–ã®æ“ä½œã§å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚³ãƒ”ãƒ¼ãŒå¿…è¦
        if id(df) in [id(ref()) for ref in self.dataframe_refs if ref() is not None]:
            return True

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¤‡é›‘ãªå ´åˆã¯ã‚³ãƒ”ãƒ¼ãŒå¿…è¦
        if (
            not df.index.is_monotonic_increasing
            and not df.index.is_monotonic_decreasing
        ):
            return True

        # ã‚«ãƒ©ãƒ åãŒé‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã¯ã‚³ãƒ”ãƒ¼ãŒå¿…è¦
        if len(df.columns) != len(set(df.columns)):
            return True

        return False

    def optimize_dtypes_ultra(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®è¶…åŠ¹ç‡æœ€é©åŒ–"""
        self.logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‹ã®è¶…åŠ¹ç‡æœ€é©åŒ–é–‹å§‹")

        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        df_optimized = df.copy()

        for col in df_optimized.columns:
            col_type = df_optimized[col].dtype

            if col_type != "object":
                c_min = df_optimized[col].min()
                c_max = df_optimized[col].max()

                if str(col_type)[:3] == "int":
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df_optimized[col] = df_optimized[col].astype(np.int8)
                        self.dtype_optimizations += 1
                    elif (
                        c_min > np.iinfo(np.int16).min
                        and c_max < np.iinfo(np.int16).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.int16)
                        self.dtype_optimizations += 1
                    elif (
                        c_min > np.iinfo(np.int32).min
                        and c_max < np.iinfo(np.int32).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.int32)
                        self.dtype_optimizations += 1
                else:
                    if (
                        c_min > np.finfo(np.float16).min
                        and c_max < np.finfo(np.float16).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.float16)
                        self.dtype_optimizations += 1
                    elif (
                        c_min > np.finfo(np.float32).min
                        and c_max < np.finfo(np.float32).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.float32)
                        self.dtype_optimizations += 1

        optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024
        reduction = (original_memory - optimized_memory) / original_memory * 100

        self.logger.info(
            f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–å®Œäº†: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB ({reduction:.1f}%å‰Šæ¸›)"
        )

        return df_optimized

    def process_inplace(self, df: pd.DataFrame, operations: List[Dict]) -> pd.DataFrame:
        """ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã§ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†"""
        self.logger.info(f"ğŸš€ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹å‡¦ç†é–‹å§‹: {len(operations)}æ“ä½œ")

        result_df = df

        for operation in operations:
            op_type = operation.get("type")
            op_params = operation.get("params", {})

            try:
                if op_type == "fillna":
                    result_df.fillna(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "dropna":
                    result_df.dropna(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "drop_duplicates":
                    result_df.drop_duplicates(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "sort_values":
                    result_df.sort_values(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "reset_index":
                    result_df.reset_index(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "astype":
                    for col, dtype in op_params.items():
                        result_df[col] = result_df[col].astype(dtype)
                    self.inplace_operations += 1
                elif op_type == "rename":
                    result_df.rename(**op_params, inplace=True)
                    self.inplace_operations += 1
                elif op_type == "set_index":
                    result_df.set_index(**op_params, inplace=True)
                    self.inplace_operations += 1
                else:
                    self.logger.warning(f"âš ï¸ æœªå¯¾å¿œã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œ: {op_type}")

                self.logger.debug(f"âœ… ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œå®Ÿè¡Œ: {op_type}")

            except Exception as e:
                self.system.log_error(e, f"ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚¨ãƒ©ãƒ¼ ({op_type})")

        self.logger.info(f"âœ… ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹å‡¦ç†å®Œäº†: {self.inplace_operations}æ“ä½œ")
        return result_df

    def chunk_processing_ultra(
        self, df: pd.DataFrame, chunk_size: int, processing_func: Callable
    ) -> pd.DataFrame:
        """è¶…åŠ¹ç‡ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        self.logger.info(f"ğŸ“Š è¶…åŠ¹ç‡ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–‹å§‹: {len(df)}è¡Œ, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")

        if len(df) <= chunk_size:
            return processing_func(df)

        results = []
        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i : i + chunk_size]

            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self._check_memory_limit():
                self.logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
                gc.collect()

            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            processed_chunk = processing_func(chunk)
            results.append(processed_chunk)

            self.logger.debug(
                f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {i//chunk_size + 1}/{(len(df)-1)//chunk_size + 1}"
            )

        # çµæœã‚’çµåˆ
        final_result = pd.concat(results, ignore_index=True)
        self.logger.info(f"âœ… è¶…åŠ¹ç‡ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {len(final_result)}è¡Œ")

        return final_result

    def _check_memory_limit(self, limit_mb: int = 2048) -> bool:
        """ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        current_memory = psutil.Process().memory_info().rss / 1024 / 1024
        return current_memory < limit_mb

    def get_optimization_stats(self) -> DataFrameOptimizationStats:
        """æœ€é©åŒ–çµ±è¨ˆã‚’å–å¾—"""
        return DataFrameOptimizationStats(
            original_memory_mb=0.0,  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¿½è·¡
            optimized_memory_mb=0.0,
            memory_reduction_mb=self.memory_saved,
            memory_reduction_pct=0.0,
            copy_operations_saved=self.copy_operations_saved,
            inplace_operations=self.inplace_operations,
            processing_time_saved=self.processing_time_saved,
            dtype_optimizations=self.dtype_optimizations,
        )


class ViewBasedProcessor:
    """ãƒ“ãƒ¥ãƒ¼ãƒ™ãƒ¼ã‚¹å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

    def create_view(
        self, df: pd.DataFrame, columns: List[str] = None, rows: slice = None
    ) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆï¼ˆã‚³ãƒ”ãƒ¼ãªã—ï¼‰"""
        if columns is None and rows is None:
            return df

        if columns is not None:
            df_view = df[columns]
        else:
            df_view = df

        if rows is not None:
            df_view = df_view.iloc[rows]

        self.logger.debug(f"ğŸ‘ï¸ ãƒ“ãƒ¥ãƒ¼ä½œæˆ: {len(df_view)}è¡Œ, {len(df_view.columns)}åˆ—")
        return df_view

    def process_view(self, df: pd.DataFrame, processing_func: Callable) -> pd.DataFrame:
        """ãƒ“ãƒ¥ãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç›´æ¥æ“ä½œ
        result = processing_func(df)
        return result


class MemoryEfficientDataFrameProcessor:
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, chunk_size: int = 10000, memory_limit_mb: int = 2048):
        self.chunk_size = chunk_size
        self.memory_limit_mb = memory_limit_mb
        self.ultra_processor = UltraEfficientDataFrameProcessor()
        self.view_processor = ViewBasedProcessor()
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

    def process_dataframe_ultra_efficient(
        self, df: pd.DataFrame, operations: List[Dict]
    ) -> pd.DataFrame:
        """è¶…åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†"""
        self.logger.info(f"ğŸš€ è¶…åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†é–‹å§‹: {len(operations)}æ“ä½œ")

        result_df = df

        for operation in operations:
            op_type = operation.get("type")

            if op_type == "inplace":
                result_df = self.ultra_processor.process_inplace(result_df, [operation])
            elif op_type == "dtype_optimization":
                result_df = self.ultra_processor.optimize_dtypes_ultra(result_df)
            elif op_type == "chunk_processing":
                result_df = self.ultra_processor.chunk_processing_ultra(
                    result_df,
                    operation.get("chunk_size", self.chunk_size),
                    operation.get("processing_func"),
                )
            elif op_type == "view_processing":
                result_df = self.view_processor.process_view(
                    result_df, operation.get("processing_func")
                )
            else:
                self.logger.warning(f"âš ï¸ æœªå¯¾å¿œã®æ“ä½œã‚¿ã‚¤ãƒ—: {op_type}")

        # æœ€é©åŒ–çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
        self._log_optimization_stats()

        return result_df

    def _log_optimization_stats(self):
        """æœ€é©åŒ–çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›"""
        stats = self.ultra_processor.get_optimization_stats()

        self.logger.info("ğŸ“Š è¶…åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†çµ±è¨ˆ:")
        self.logger.info(f"  â™»ï¸ ã‚³ãƒ”ãƒ¼æ“ä½œå‰Šæ¸›: {stats.copy_operations_saved}å›")
        self.logger.info(f"  ğŸ”§ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œ: {stats.inplace_operations}å›")
        self.logger.info(f"  ğŸ’¾ ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–: {stats.dtype_optimizations}åˆ—")
        self.logger.info(f"  â±ï¸ å‡¦ç†æ™‚é–“å‰Šæ¸›: {stats.processing_time_saved:.2f}ç§’")


def create_ultra_efficient_processor(
    chunk_size: int = 10000, memory_limit_mb: int = 2048
) -> MemoryEfficientDataFrameProcessor:
    """è¶…åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""
    return MemoryEfficientDataFrameProcessor(chunk_size, memory_limit_mb)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    import pandas as pd
    import numpy as np

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n_rows = 10000
    n_cols = 20

    data = {f"col_{i}": np.random.randn(n_rows) for i in range(n_cols)}

    sample_df = pd.DataFrame(data)

    # è¶…åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    processor = create_ultra_efficient_processor()

    operations = [
        {"type": "dtype_optimization", "params": {}},
        {
            "type": "inplace",
            "params": {"type": "fillna", "params": {"method": "ffill"}},
        },
        {"type": "inplace", "params": {"type": "dropna", "params": {}}},
    ]

    optimized_df = processor.process_dataframe_ultra_efficient(sample_df, operations)

    print(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {sample_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
    print(f"ğŸ“ˆ æœ€é©åŒ–å¾Œ: {optimized_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB")
    print(f"âœ… å‡¦ç†å®Œäº†: {len(optimized_df)}è¡Œ, {len(optimized_df.columns)}åˆ—")
