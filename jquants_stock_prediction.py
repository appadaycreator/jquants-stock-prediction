import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from font_config import setup_japanese_font
from sklearn.metrics import mean_absolute_error
from config_loader import get_config
from model_factory import ModelFactory, ModelEvaluator, ModelComparator

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
setup_japanese_font()

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
config = get_config()
prediction_config = config.get_prediction_config()

# ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
input_file = prediction_config.get("input_file", "processed_stock_data.csv")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_file}")
df = pd.read_csv(input_file)

# èª¬æ˜å¤‰æ•°ã¨ç›®çš„å¤‰æ•°
features = prediction_config.get(
    "features",
    ["SMA_5", "SMA_25", "SMA_50", "Close_1d_ago", "Close_5d_ago", "Close_25d_ago"],
)
target = prediction_config.get("target", "Close")

print(f"ä½¿ç”¨ç‰¹å¾´é‡: {features}")
print(f"ç›®çš„å¤‰æ•°: {target}")

X = df[features]
y = df[target]

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
test_size = prediction_config.get("test_size", 0.2)
random_state = prediction_config.get("random_state", 42)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=random_state
)
print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}è¡Œ, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}è¡Œ")

# ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
model_selection = prediction_config.get("model_selection", {})
compare_models = model_selection.get("compare_models", False)
primary_model = model_selection.get("primary_model", "random_forest")

# ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã¨è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
factory = ModelFactory()
evaluator = ModelEvaluator()

if compare_models:
    print("\nğŸ”„ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚’å®Ÿè¡Œä¸­...")

    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå™¨ã‚’ä½¿ç”¨
    comparator = ModelComparator()
    models_config = prediction_config.get("models", {})

    if not models_config:
        print("è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        from model_factory import get_default_models_config

        models_config = get_default_models_config()

    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿè¡Œ
    comparison_results = comparator.compare_models(
        models_config, X_train, X_test, y_train, y_test, features
    )

    if not comparison_results.empty:
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ:")
        print("=" * 80)
        for idx, row in comparison_results.iterrows():
            print(
                f"{row['model_name']:<15} | MAE: {row['mae']:.4f} | RMSE: {row['rmse']:.4f} | RÂ²: {row['r2']:.4f}"
            )

        # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        best_model_name = comparison_results.iloc[0]["model_name"]
        print(f"\nğŸ† æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«: {best_model_name}")

        # æ¯”è¼ƒçµæœã‚’CSVã«ä¿å­˜
        comparison_csv = prediction_config.get(
            "comparison_csv", "model_comparison_results.csv"
        )
        comparison_results.to_csv(comparison_csv, index=False)
        print(f"ğŸ“ æ¯”è¼ƒçµæœã‚’ '{comparison_csv}' ã«ä¿å­˜ã—ã¾ã—ãŸ")

        # æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«ã§å†å­¦ç¿’ã—ã¦è©³ç´°åˆ†æ
        best_config = models_config[best_model_name]
        model = factory.create_model(best_config["type"], best_config.get("params", {}))
    else:
        print(
            "âŒ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã§æœ‰åŠ¹ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        )
        model = factory.create_model("random_forest")
        best_model_name = "random_forest"

else:
    print(f"\nğŸ¯ å˜ä¸€ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ: {primary_model}")

    # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å–å¾—
    models_config = prediction_config.get("models", {})
    if primary_model in models_config:
        model_config = models_config[primary_model]
        model = factory.create_model(
            model_config["type"], model_config.get("params", {})
        )
    else:
        print(
            f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ« '{primary_model}' ã®è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
        )
        model = factory.create_model(primary_model)

    best_model_name = primary_model

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
print(f"\nğŸ“š ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­: {best_model_name}")
model.fit(X_train, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = model.predict(X_test)
metrics = evaluator.evaluate_model(model, X_test, y_test, y_pred)

print(f"\nğŸ“Š æœ€çµ‚è©•ä¾¡çµæœ:")
print(f"  MAE (å¹³å‡çµ¶å¯¾èª¤å·®): {metrics['mae']:.4f}")
print(f"  RMSE (å¹³å‡å¹³æ–¹æ ¹èª¤å·®): {metrics['rmse']:.4f}")
print(f"  RÂ² (æ±ºå®šä¿‚æ•°): {metrics['r2']:.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦ã‚’è¡¨ç¤º
feature_importance_df = evaluator.get_feature_importance(model, features)
if not feature_importance_df.empty:
    print("\nğŸ¯ ç‰¹å¾´é‡é‡è¦åº¦:")
    for idx, row in feature_importance_df.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")

# çµæœã®å¯è¦–åŒ–
output_image = prediction_config.get("output_image", "stock_prediction_result.png")
print(f"\nğŸ¨ çµæœã‚’ '{output_image}' ã«ä¿å­˜ä¸­...")

plt.figure(figsize=(15, 8))

# ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
plt.subplot(2, 2, 1)
plt.plot(y_test.values, label="å®Ÿéš›ã®æ ªä¾¡", color="blue", alpha=0.7, linewidth=2)
plt.plot(y_pred, label="äºˆæ¸¬æ ªä¾¡", color="red", alpha=0.7, linewidth=2)
plt.legend()
plt.title(f"æ ªä¾¡äºˆæ¸¬çµæœ ({best_model_name})")
plt.xlabel("ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
plt.ylabel("æ ªä¾¡")
plt.grid(True, alpha=0.3)

# æ•£å¸ƒå›³
plt.subplot(2, 2, 2)
plt.scatter(y_test, y_pred, alpha=0.6, color="green")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", lw=2)
plt.xlabel("å®Ÿéš›ã®æ ªä¾¡")
plt.ylabel("äºˆæ¸¬æ ªä¾¡")
plt.title("å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤")
plt.grid(True, alpha=0.3)

# æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
plt.subplot(2, 2, 3)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6, color="orange")
plt.axhline(y=0, color="r", linestyle="--")
plt.xlabel("äºˆæ¸¬æ ªä¾¡")
plt.ylabel("æ®‹å·®")
plt.title("æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ")
plt.grid(True, alpha=0.3)

# ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10å€‹ï¼‰
if not feature_importance_df.empty:
    plt.subplot(2, 2, 4)
    top_features = feature_importance_df.head(10)
    plt.barh(range(len(top_features)), top_features["importance"], color="skyblue")
    plt.yticks(range(len(top_features)), top_features["feature"])
    plt.xlabel("é‡è¦åº¦")
    plt.title("ç‰¹å¾´é‡é‡è¦åº¦ (Top 10)")
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(output_image, dpi=300, bbox_inches="tight")
plt.show()

print(f"\nâœ… äºˆæ¸¬å®Œäº†!")
print(f"   ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
print(f"   MAE: {metrics['mae']:.4f}")
print(f"   RÂ²: {metrics['r2']:.4f}")
print(f"   å‡ºåŠ›ç”»åƒ: {output_image}")
if compare_models:
    print(
        f"   æ¯”è¼ƒçµæœ: {prediction_config.get('comparison_csv', 'model_comparison_results.csv')}"
    )
