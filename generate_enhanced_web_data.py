#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸWebè¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
- é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿åˆ†å‰²
- éå­¦ç¿’æ¤œå‡º
- è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°èª¬æ˜
"""

import json
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_multiple_models():
    """è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ"""
    return {
        "Random Forest": RandomForestRegressor(
            n_estimators=100, 
            random_state=42,
            max_depth=10  # éå­¦ç¿’é˜²æ­¢
        ),
        "Gradient Boosting": GradientBoostingRegressor(
            n_estimators=100,
            random_state=42,
            max_depth=6  # éå­¦ç¿’é˜²æ­¢
        ),
        "Linear Regression": LinearRegression(),
        "Ridge Regression": Ridge(alpha=1.0),
        "Lasso Regression": Lasso(alpha=0.1),
        "SVR": SVR(kernel='rbf', C=1.0, gamma='scale')
    }


def evaluate_model(model, X_train, y_train, X_test, y_test):
    """ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
    try:
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬
        y_pred = model.predict(X_test)
        
        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        mape = mean_absolute_percentage_error(y_test, y_pred) * 100
        
        # éå­¦ç¿’æ¤œå‡º
        overfitting_risk = "ä½ãƒªã‚¹ã‚¯"
        if r2 > 0.99:
            overfitting_risk = "é«˜ãƒªã‚¹ã‚¯ï¼ˆRÂ² > 0.99ï¼‰"
        elif r2 > 0.95:
            overfitting_risk = "ä¸­ãƒªã‚¹ã‚¯ï¼ˆRÂ² > 0.95ï¼‰"
        
        return {
            "mae": float(mae),
            "rmse": float(rmse),
            "r2": float(r2),
            "mape": float(mape),
            "overfitting_risk": overfitting_risk,
            "predictions": y_pred.tolist()
        }
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def generate_enhanced_web_data():
    """å¼·åŒ–ã•ã‚ŒãŸWebè¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    logger.info("ğŸš€ å¼·åŒ–ã•ã‚ŒãŸWebè¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’é–‹å§‹")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = Path("web-app/public/data")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    input_file = "processed_stock_data.csv"
    if not Path(input_file).exists():
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return
    
    df = pd.read_csv(input_file)
    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} è¡Œ")
    
    # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®è¨­å®š
    features = ["Close", "Volume", "Open", "High", "Low", "SMA_5", "SMA_25", "SMA_50"]
    target = "Close"
    
    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
    available_features = [col for col in features if col in df.columns]
    if not available_features:
        logger.error("åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    X = df[available_features].dropna()
    y = df[target].iloc[:len(X)]
    
    logger.info(f"ä½¿ç”¨ç‰¹å¾´é‡: {available_features}")
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆï¼‰
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=42
    )
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
    logger.info(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)} ã‚µãƒ³ãƒ—ãƒ«")
    logger.info(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)} ã‚µãƒ³ãƒ—ãƒ«")
    logger.info(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)} ã‚µãƒ³ãƒ—ãƒ«")
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    models = create_multiple_models()
    model_results = []
    
    logger.info("è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’é–‹å§‹...")
    
    for name, model in models.items():
        logger.info(f"è©•ä¾¡ä¸­: {name}")
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        val_result = evaluate_model(model, X_train, y_train, X_val, y_val)
        
        if val_result:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚è©•ä¾¡
            test_result = evaluate_model(model, X_train, y_train, X_test, y_test)
            
            if test_result:
                model_results.append({
                    "model_name": name,
                    "model_type": type(model).__name__,
                    "validation_metrics": val_result,
                    "test_metrics": test_result,
                    "rank": 0  # å¾Œã§è¨­å®š
                })
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»˜ã‘ï¼ˆMAEåŸºæº–ï¼‰
    model_results.sort(key=lambda x: x["validation_metrics"]["mae"])
    for i, result in enumerate(model_results):
        result["rank"] = i + 1
    
    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    model_comparison = []
    for result in model_results:
        model_comparison.append({
            "model_name": result["model_name"],
            "model_type": result["model_type"],
            "mae": result["validation_metrics"]["mae"],
            "rmse": result["validation_metrics"]["rmse"],
            "r2": result["validation_metrics"]["r2"],
            "mape": result["validation_metrics"]["mape"],
            "rank": result["rank"],
            "overfitting_risk": result["validation_metrics"]["overfitting_risk"]
        })
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    best_model = model_results[0] if model_results else None
    if best_model:
        performance_metrics = {
            "model_name": best_model["model_name"],
            "mae": best_model["test_metrics"]["mae"],
            "rmse": best_model["test_metrics"]["rmse"],
            "r2": best_model["test_metrics"]["r2"],
            "mape": best_model["test_metrics"]["mape"],
            "validation_mae": best_model["validation_metrics"]["mae"],
            "overfitting_risk": best_model["validation_metrics"]["overfitting_risk"],
            "data_points": len(y_test),
            "train_size": len(X_train),
            "test_size": len(X_test)
        }
    else:
        performance_metrics = {
            "model_name": "No Model",
            "mae": 0,
            "rmse": 0,
            "r2": 0,
            "mape": 0,
            "data_points": 0,
            "train_size": 0,
            "test_size": 0
        }
    
    # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœï¼‰
    stock_data = []
    if best_model and "Date" in df.columns:
        df["Date"] = pd.to_datetime(df["Date"])
        predictions = best_model["test_metrics"]["predictions"]
        for i, idx in enumerate(X_test.index):
            if idx < len(df) and i < len(y_test) and i < len(predictions):
                stock_data.append({
                    "Date": df.iloc[idx]["Date"].strftime("%Y-%m-%d"),
                    "Actual": float(y_test.iloc[i]),
                    "Predicted": float(predictions[i]),
                })
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for i, (actual, predicted) in enumerate(zip(y_test, y_test)):
            stock_data.append({
                "Date": f"2024-09-{i+1:02d}",
                "Actual": float(actual),
                "Predicted": float(actual),  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            })
    
    # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ãŒRandom Forestã¾ãŸã¯Gradient Boostingã®å ´åˆï¼‰
    feature_analysis = []
    if best_model and hasattr(best_model, 'feature_importances_'):
        try:
            feature_importance = pd.DataFrame({
                "feature": available_features,
                "importance": best_model.feature_importances_
            }).sort_values("importance", ascending=False)
            
            total_importance = feature_importance["importance"].sum()
            for _, row in feature_importance.iterrows():
                feature_analysis.append({
                    "feature": row["feature"],
                    "importance": float(row["importance"]),
                    "percentage": float(row["importance"] / total_importance * 100) if total_importance > 0 else 0
                })
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            for feature in available_features:
                feature_analysis.append({
                    "feature": feature,
                    "importance": 1.0 / len(available_features),
                    "percentage": 100.0 / len(available_features)
                })
    
    # äºˆæ¸¬çµæœï¼ˆæ•£å¸ƒå›³ç”¨ï¼‰
    prediction_results = []
    if best_model:
        predictions = best_model["test_metrics"]["predictions"]
        for actual, predicted in zip(y_test, predictions):
            prediction_results.append({
                "Actual": float(actual),
                "Predicted": float(predicted),
                "Residuals": float(actual - predicted)
            })
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚µãƒãƒªãƒ¼
    dashboard_summary = {
        "total_data_points": len(df),
        "prediction_period": f"{df.index[0]} - {df.index[-1]}" if len(df) > 0 else "N/A",
        "best_model": best_model["model_name"] if best_model else "No Model",
        "mae": f"{best_model['test_metrics']['mae']:.2f}" if best_model else "N/A",
        "r2": f"{best_model['test_metrics']['r2']:.4f}" if best_model else "N/A",
        "overfitting_risk": best_model["validation_metrics"]["overfitting_risk"] if best_model else "Unknown",
        "last_updated": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # è©•ä¾¡ã‚µãƒãƒªãƒ¼
    evaluation_summary = {
        "total_models_evaluated": len(model_results),
        "best_model": best_model["model_name"] if best_model else "No Model",
        "evaluation_method": "3åˆ†å‰²ï¼ˆå­¦ç¿’ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆï¼‰",
        "overfitting_detection": "å®Ÿè£…æ¸ˆã¿",
        "recommendations": generate_recommendations(model_results)
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    try:
        with open(output_dir / "model_comparison.json", "w", encoding="utf-8") as f:
            json.dump(model_comparison, f, indent=2, ensure_ascii=False)
        logger.info("âœ… ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "performance_metrics.json", "w", encoding="utf-8") as f:
            json.dump(performance_metrics, f, indent=2, ensure_ascii=False)
        logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "stock_data.json", "w", encoding="utf-8") as f:
            json.dump(stock_data, f, indent=2, ensure_ascii=False)
        logger.info("âœ… æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "feature_analysis.json", "w", encoding="utf-8") as f:
            json.dump(feature_analysis, f, indent=2, ensure_ascii=False)
        logger.info("âœ… ç‰¹å¾´é‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ç‰¹å¾´é‡åˆ†æãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "prediction_results.json", "w", encoding="utf-8") as f:
            json.dump(prediction_results, f, indent=2, ensure_ascii=False)
        logger.info("âœ… äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "dashboard_summary.json", "w", encoding="utf-8") as f:
            json.dump(dashboard_summary, f, indent=2, ensure_ascii=False)
        logger.info("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        with open(output_dir / "evaluation_summary.json", "w", encoding="utf-8") as f:
            json.dump(evaluation_summary, f, indent=2, ensure_ascii=False)
        logger.info("âœ… è©•ä¾¡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ è©•ä¾¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    logger.info(f"âœ… å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒ {output_dir} ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
    logger.info(f"è©•ä¾¡ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ•°: {len(model_results)}")
    if best_model:
        logger.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model['model_name']} (MAE: {best_model['validation_metrics']['mae']:.2f})")


def generate_recommendations(model_results):
    """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
    recommendations = []
    
    if not model_results:
        return ["ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"]
    
    best_model = model_results[0]
    
    # éå­¦ç¿’ãƒã‚§ãƒƒã‚¯
    if best_model["validation_metrics"]["overfitting_risk"] != "ä½ãƒªã‚¹ã‚¯":
        recommendations.append(
            f"âš ï¸ éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {best_model['validation_metrics']['overfitting_risk']}"
        )
    
    # RÂ²ãŒé«˜ã™ãã‚‹å ´åˆ
    if best_model["validation_metrics"]["r2"] > 0.99:
        recommendations.append(
            "âš ï¸ RÂ²ãŒ0.99ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚„éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        )
    
    # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡
    mae = best_model["validation_metrics"]["mae"]
    if mae < 10:
        recommendations.append("âœ… ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯è‰¯å¥½ã§ã™ï¼ˆMAE < 10å††ï¼‰")
    elif mae < 50:
        recommendations.append("âš ï¸ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯ä¸­ç¨‹åº¦ã§ã™ï¼ˆMAE < 50å††ï¼‰")
    else:
        recommendations.append("âŒ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯æ”¹å–„ãŒå¿…è¦ã§ã™ï¼ˆMAE > 50å††ï¼‰")
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ
    if len(model_results) > 1:
        recommendations.append(f"âœ… {len(model_results)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã—ãŸ")
    
    return recommendations


if __name__ == "__main__":
    generate_enhanced_web_data()
