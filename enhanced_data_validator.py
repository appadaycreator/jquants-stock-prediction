#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ãƒ‡ãƒ¼ã‚¿å“è³ªã®å‘ä¸Šã¨ç•°å¸¸å€¤æ¤œå‡ºã®æ”¹å–„
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from enum import Enum
import warnings
from scipy import stats
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler


class ValidationLevel(Enum):
    """æ¤œè¨¼ãƒ¬ãƒ™ãƒ«ã®å®šç¾©"""

    BASIC = "basic"
    STANDARD = "standard"
    STRICT = "strict"
    CUSTOM = "custom"


@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    is_valid: bool
    score: float
    issues: List[str]
    warnings: List[str]
    recommendations: List[str]
    detailed_report: Dict[str, Any]


class EnhancedDataValidator:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, validation_level: ValidationLevel = ValidationLevel.STANDARD):
        self.validation_level = validation_level
        self.logger = logging.getLogger(__name__)

        # æ¤œè¨¼ãƒ«ãƒ¼ãƒ«ã®è¨­å®š
        self.validation_rules = self._setup_validation_rules()

    def _setup_validation_rules(self) -> Dict[str, Any]:
        """æ¤œè¨¼ãƒ«ãƒ¼ãƒ«ã®è¨­å®š"""
        rules = {
            "required_columns": ["Date", "Open", "High", "Low", "Close", "Volume"],
            "numeric_columns": ["Open", "High", "Low", "Close", "Volume"],
            "date_column": "Date",
            "ohlc_validation": True,
            "volume_validation": True,
            "outlier_detection": True,
            "missing_value_threshold": 0.1,  # 10%ä»¥ä¸Šæ¬ æå€¤ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
            "outlier_threshold": 3.0,  # 3Ïƒãƒ«ãƒ¼ãƒ«
            "volume_min": 0,  # ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®æœ€å°å€¤
            "price_min": 0.01,  # ä¾¡æ ¼ã®æœ€å°å€¤
            "price_max": 1000000,  # ä¾¡æ ¼ã®æœ€å¤§å€¤
        }

        if self.validation_level == ValidationLevel.STRICT:
            rules.update(
                {
                    "missing_value_threshold": 0.05,  # 5%ä»¥ä¸Šæ¬ æå€¤ãŒã‚ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                    "outlier_threshold": 2.5,  # 2.5Ïƒãƒ«ãƒ¼ãƒ«
                    "require_ohlc_consistency": True,
                    "require_volume_consistency": True,
                }
            )
        elif self.validation_level == ValidationLevel.BASIC:
            rules.update(
                {
                    "missing_value_threshold": 0.2,  # 20%ä»¥ä¸Šæ¬ æå€¤ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
                    "outlier_threshold": 4.0,  # 4Ïƒãƒ«ãƒ¼ãƒ«
                    "ohlc_validation": False,
                    "volume_validation": False,
                }
            )

        return rules

    def validate_data_quality(self, data: pd.DataFrame) -> ValidationResult:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        self.logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ¤œè¨¼ã‚’é–‹å§‹")

        issues = []
        warnings = []
        recommendations = []
        detailed_report = {}

        # 1. åŸºæœ¬æ§‹é€ ã®æ¤œè¨¼
        structure_issues = self._validate_data_structure(data)
        issues.extend(structure_issues)

        # 2. ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼
        type_issues = self._validate_data_types(data)
        issues.extend(type_issues)

        # 3. æ¬ æå€¤ã®æ¤œè¨¼
        missing_report = self._validate_missing_values(data)
        detailed_report["missing_values"] = missing_report
        if missing_report["total_missing"] > 0:
            warnings.append(f"æ¬ æå€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {missing_report['total_missing']}å€‹")

        # 4. ç•°å¸¸å€¤ã®æ¤œè¨¼
        outlier_report = self._validate_outliers(data)
        detailed_report["outliers"] = outlier_report
        if outlier_report["total_outliers"] > 0:
            warnings.append(f"ç•°å¸¸å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {outlier_report['total_outliers']}å€‹")

        # 5. OHLCãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§æ¤œè¨¼
        if self.validation_rules["ohlc_validation"]:
            ohlc_issues = self._validate_ohlc_consistency(data)
            issues.extend(ohlc_issues)

        # 6. ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if self.validation_rules["volume_validation"]:
            volume_issues = self._validate_volume_data(data)
            issues.extend(volume_issues)

        # 7. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        timeseries_issues = self._validate_timeseries_consistency(data)
        issues.extend(timeseries_issues)

        # 8. ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ¤œè¨¼
        distribution_report = self._validate_data_distribution(data)
        detailed_report["distribution"] = distribution_report

        # 9. æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = self._generate_recommendations(data, detailed_report)

        # 10. ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        score = self._calculate_quality_score(data, issues, warnings, detailed_report)

        is_valid = len(issues) == 0 and score >= 0.7

        result = ValidationResult(
            is_valid=is_valid,
            score=score,
            issues=issues,
            warnings=warnings,
            recommendations=recommendations,
            detailed_report=detailed_report,
        )

        self.logger.info(
            f"âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼å®Œäº†: ã‚¹ã‚³ã‚¢={score:.2f}, å•é¡Œ={len(issues)}å€‹, è­¦å‘Š={len(warnings)}å€‹"
        )

        return result

    def _validate_data_structure(self, data: pd.DataFrame) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æ¤œè¨¼"""
        issues = []

        # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèª
        required_columns = self.validation_rules["required_columns"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            issues.append(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")

        # ãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ç¢ºèª
        if len(data) == 0:
            issues.append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        elif len(data) < 10:
            issues.append("ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ï¼ˆ10è¡Œæœªæº€ï¼‰")

        return issues

    def _validate_data_types(self, data: pd.DataFrame) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®æ¤œè¨¼"""
        issues = []

        # æ•°å€¤ã‚«ãƒ©ãƒ ã®å‹ç¢ºèª
        numeric_columns = self.validation_rules["numeric_columns"]
        for col in numeric_columns:
            if col in data.columns:
                if not pd.api.types.is_numeric_dtype(data[col]):
                    issues.append(f"ã‚«ãƒ©ãƒ  '{col}' ãŒæ•°å€¤å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“")

        # æ—¥ä»˜ã‚«ãƒ©ãƒ ã®å‹ç¢ºèª
        date_column = self.validation_rules["date_column"]
        if date_column in data.columns:
            if not pd.api.types.is_datetime64_any_dtype(data[date_column]):
                issues.append(f"ã‚«ãƒ©ãƒ  '{date_column}' ãŒæ—¥ä»˜å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“")

        return issues

    def _validate_missing_values(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æ¬ æå€¤ã®æ¤œè¨¼"""
        missing_report = {
            "total_missing": 0,
            "missing_by_column": {},
            "missing_percentage": 0.0,
            "critical_missing": [],
        }

        total_cells = data.size
        missing_cells = data.isnull().sum().sum()
        missing_report["total_missing"] = missing_cells
        missing_report["missing_percentage"] = (
            missing_cells / total_cells if total_cells > 0 else 0
        )

        # ã‚«ãƒ©ãƒ åˆ¥ã®æ¬ æå€¤
        for col in data.columns:
            missing_count = data[col].isnull().sum()
            missing_report["missing_by_column"][col] = missing_count

            # é‡è¦ãªã‚«ãƒ©ãƒ ã®æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
            if col in self.validation_rules["required_columns"] and missing_count > 0:
                missing_report["critical_missing"].append(col)

        return missing_report

    def _validate_outliers(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ç•°å¸¸å€¤ã®æ¤œè¨¼"""
        outlier_report = {
            "total_outliers": 0,
            "outliers_by_column": {},
            "outlier_percentage": 0.0,
            "critical_outliers": [],
        }

        numeric_columns = [
            col
            for col in self.validation_rules["numeric_columns"]
            if col in data.columns
        ]

        for col in numeric_columns:
            if data[col].dtype in ["int64", "float64"]:
                # Z-scoreæ³•ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º
                z_scores = np.abs(stats.zscore(data[col].dropna()))
                outliers = z_scores > self.validation_rules["outlier_threshold"]
                outlier_count = outliers.sum()

                outlier_report["outliers_by_column"][col] = outlier_count
                outlier_report["total_outliers"] += outlier_count

                # é‡è¦ãªã‚«ãƒ©ãƒ ã®ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                if col in ["Close", "Volume"] and outlier_count > 0:
                    outlier_report["critical_outliers"].append(col)

        if outlier_report["total_outliers"] > 0:
            outlier_report["outlier_percentage"] = (
                outlier_report["total_outliers"] / data.size
            )

        return outlier_report

    def _validate_ohlc_consistency(self, data: pd.DataFrame) -> List[str]:
        """OHLCãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§æ¤œè¨¼"""
        issues = []

        ohlc_columns = ["Open", "High", "Low", "Close"]
        if all(col in data.columns for col in ohlc_columns):
            # High >= Low ã®ç¢ºèª
            invalid_high_low = data["High"] < data["Low"]
            if invalid_high_low.any():
                issues.append(f"High < Low ã®è¡ŒãŒ {invalid_high_low.sum()} è¡Œã‚ã‚Šã¾ã™")

            # High >= Open, Close ã®ç¢ºèª
            invalid_high_open = data["High"] < data["Open"]
            if invalid_high_open.any():
                issues.append(f"High < Open ã®è¡ŒãŒ {invalid_high_open.sum()} è¡Œã‚ã‚Šã¾ã™")

            invalid_high_close = data["High"] < data["Close"]
            if invalid_high_close.any():
                issues.append(f"High < Close ã®è¡ŒãŒ {invalid_high_close.sum()} è¡Œã‚ã‚Šã¾ã™")

            # Low <= Open, Close ã®ç¢ºèª
            invalid_low_open = data["Low"] > data["Open"]
            if invalid_low_open.any():
                issues.append(f"Low > Open ã®è¡ŒãŒ {invalid_low_open.sum()} è¡Œã‚ã‚Šã¾ã™")

            invalid_low_close = data["Low"] > data["Close"]
            if invalid_low_close.any():
                issues.append(f"Low > Close ã®è¡ŒãŒ {invalid_low_close.sum()} è¡Œã‚ã‚Šã¾ã™")

        return issues

    def _validate_volume_data(self, data: pd.DataFrame) -> List[str]:
        """ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        issues = []

        if "Volume" in data.columns:
            # è² ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ç¢ºèª
            negative_volume = data["Volume"] < 0
            if negative_volume.any():
                issues.append(f"è² ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ãŒ {negative_volume.sum()} è¡Œã‚ã‚Šã¾ã™")

            # ã‚¼ãƒ­ãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ç¢ºèª
            zero_volume = data["Volume"] == 0
            if zero_volume.any():
                issues.append(f"ã‚¼ãƒ­ãƒœãƒªãƒ¥ãƒ¼ãƒ ãŒ {zero_volume.sum()} è¡Œã‚ã‚Šã¾ã™")

            # ç•°å¸¸ã«å¤§ããªãƒœãƒªãƒ¥ãƒ¼ãƒ ã®ç¢ºèª
            volume_mean = data["Volume"].mean()
            volume_std = data["Volume"].std()
            if volume_std > 0:
                extreme_volume = data["Volume"] > volume_mean + 5 * volume_std
                if extreme_volume.any():
                    issues.append(f"ç•°å¸¸ã«å¤§ããªãƒœãƒªãƒ¥ãƒ¼ãƒ ãŒ {extreme_volume.sum()} è¡Œã‚ã‚Šã¾ã™")

        return issues

    def _validate_timeseries_consistency(self, data: pd.DataFrame) -> List[str]:
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§æ¤œè¨¼"""
        issues = []

        date_column = self.validation_rules["date_column"]
        if date_column in data.columns:
            # æ—¥ä»˜ã®é‡è¤‡ç¢ºèª
            duplicate_dates = data[date_column].duplicated()
            if duplicate_dates.any():
                issues.append(f"é‡è¤‡ã—ãŸæ—¥ä»˜ãŒ {duplicate_dates.sum()} è¡Œã‚ã‚Šã¾ã™")

            # æ—¥ä»˜ã®é †åºç¢ºèª
            if not data[date_column].is_monotonic_increasing:
                issues.append("æ—¥ä»˜ãŒæ™‚ç³»åˆ—é †ã«ãªã£ã¦ã„ã¾ã›ã‚“")

            # æ—¥ä»˜ã®é–“éš”ç¢ºèª
            if len(data) > 1:
                date_diffs = data[date_column].diff().dropna()
                if not date_diffs.empty:
                    # ç•°å¸¸ã«å¤§ããªé–“éš”ã®ç¢ºèª
                    max_expected_interval = pd.Timedelta(days=7)  # 1é€±é–“
                    large_gaps = date_diffs > max_expected_interval
                    if large_gaps.any():
                        issues.append(f"ç•°å¸¸ã«å¤§ããªæ—¥ä»˜é–“éš”ãŒ {large_gaps.sum()} ç®‡æ‰€ã‚ã‚Šã¾ã™")

        return issues

    def _validate_data_distribution(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ¤œè¨¼"""
        distribution_report = {
            "skewness": {},
            "kurtosis": {},
            "correlation": {},
            "stationarity": {},
        }

        numeric_columns = [
            col
            for col in self.validation_rules["numeric_columns"]
            if col in data.columns
        ]

        for col in numeric_columns:
            if data[col].dtype in ["int64", "float64"] and not data[col].isna().all():
                # æ­ªåº¦ã®è¨ˆç®—
                distribution_report["skewness"][col] = data[col].skew()

                # å°–åº¦ã®è¨ˆç®—
                distribution_report["kurtosis"][col] = data[col].kurtosis()

        # ç›¸é–¢ã®è¨ˆç®—
        if len(numeric_columns) > 1:
            correlation_matrix = data[numeric_columns].corr()
            distribution_report["correlation"] = correlation_matrix.to_dict()

        return distribution_report

    def _generate_recommendations(
        self, data: pd.DataFrame, detailed_report: Dict[str, Any]
    ) -> List[str]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        # æ¬ æå€¤ã®æ¨å¥¨äº‹é …
        missing_report = detailed_report.get("missing_values", {})
        if missing_report.get("missing_percentage", 0) > 0.05:
            recommendations.append("æ¬ æå€¤ã®å‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ï¼ˆå‰å€¤è£œå®Œã€ç·šå½¢è£œå®Œãªã©ï¼‰")

        # ç•°å¸¸å€¤ã®æ¨å¥¨äº‹é …
        outlier_report = detailed_report.get("outliers", {})
        if outlier_report.get("total_outliers", 0) > 0:
            recommendations.append("ç•°å¸¸å€¤ã®å‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ï¼ˆé™¤å¤–ã€ä¿®æ­£ã€å¤‰æ›ãªã©ï¼‰")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ¨å¥¨äº‹é …
        distribution_report = detailed_report.get("distribution", {})
        for col, skewness in distribution_report.get("skewness", {}).items():
            if abs(skewness) > 2:
                recommendations.append(f"ã‚«ãƒ©ãƒ  '{col}' ã®åˆ†å¸ƒãŒåã£ã¦ã„ã¾ã™ï¼ˆæ­ªåº¦: {skewness:.2f}ï¼‰")

        # ãƒ‡ãƒ¼ã‚¿é‡ã®æ¨å¥¨äº‹é …
        if len(data) < 100:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        return recommendations

    def _calculate_quality_score(
        self,
        data: pd.DataFrame,
        issues: List[str],
        warnings: List[str],
        detailed_report: Dict[str, Any],
    ) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        base_score = 1.0

        # å•é¡Œã«ã‚ˆã‚‹æ¸›ç‚¹
        issue_penalty = len(issues) * 0.1
        warning_penalty = len(warnings) * 0.05

        # æ¬ æå€¤ã«ã‚ˆã‚‹æ¸›ç‚¹
        missing_report = detailed_report.get("missing_values", {})
        missing_penalty = missing_report.get("missing_percentage", 0) * 0.3

        # ç•°å¸¸å€¤ã«ã‚ˆã‚‹æ¸›ç‚¹
        outlier_report = detailed_report.get("outliers", {})
        outlier_penalty = outlier_report.get("outlier_percentage", 0) * 0.2

        # æœ€çµ‚ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        final_score = max(
            0.0,
            base_score
            - issue_penalty
            - warning_penalty
            - missing_penalty
            - outlier_penalty,
        )

        return final_score

    def detect_anomalies(
        self, data: pd.DataFrame, method: str = "isolation_forest"
    ) -> Dict[str, Any]:
        """ç•°å¸¸å€¤æ¤œå‡ºã®å¼·åŒ–"""
        self.logger.info(f"ğŸ” ç•°å¸¸å€¤æ¤œå‡ºã‚’é–‹å§‹: æ–¹æ³•={method}")

        numeric_columns = [
            col
            for col in self.validation_rules["numeric_columns"]
            if col in data.columns
        ]

        if not numeric_columns:
            return {"anomalies": [], "anomaly_scores": [], "method": method}

        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        numeric_data = data[numeric_columns].dropna()

        if len(numeric_data) == 0:
            return {"anomalies": [], "anomaly_scores": [], "method": method}

        # ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åŒ–
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_data)

        anomalies = []
        anomaly_scores = []

        if method == "isolation_forest":
            # Isolation Forestæ³•
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(scaled_data)
            anomaly_scores = iso_forest.decision_function(scaled_data)

            anomalies = numeric_data[anomaly_labels == -1].index.tolist()

        elif method == "z_score":
            # Z-scoreæ³•
            z_scores = np.abs(stats.zscore(scaled_data, axis=0))
            max_z_scores = np.max(z_scores, axis=1)
            anomalies = numeric_data[
                max_z_scores > self.validation_rules["outlier_threshold"]
            ].index.tolist()
            anomaly_scores = max_z_scores.tolist()

        result = {
            "anomalies": anomalies,
            "anomaly_scores": anomaly_scores,
            "method": method,
            "total_anomalies": len(anomalies),
            "anomaly_percentage": (
                len(anomalies) / len(numeric_data) if len(numeric_data) > 0 else 0
            ),
        }

        self.logger.info(f"âœ… ç•°å¸¸å€¤æ¤œå‡ºå®Œäº†: {len(anomalies)}å€‹ã®ç•°å¸¸å€¤ã‚’æ¤œå‡º")

        return result

    def suggest_data_improvements(
        self, data: pd.DataFrame, validation_result: ValidationResult
    ) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿æ”¹å–„ã®ææ¡ˆ"""
        improvements = []

        # æ¬ æå€¤ã®æ”¹å–„ææ¡ˆ
        missing_report = validation_result.detailed_report.get("missing_values", {})
        if missing_report.get("total_missing", 0) > 0:
            improvements.append("æ¬ æå€¤ã®å‡¦ç†: å‰å€¤è£œå®Œã€ç·šå½¢è£œå®Œã€ã¾ãŸã¯æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹è£œå®Œã‚’æ¤œè¨")

        # ç•°å¸¸å€¤ã®æ”¹å–„ææ¡ˆ
        outlier_report = validation_result.detailed_report.get("outliers", {})
        if outlier_report.get("total_outliers", 0) > 0:
            improvements.append("ç•°å¸¸å€¤ã®å‡¦ç†: é™¤å¤–ã€ä¿®æ­£ã€ã¾ãŸã¯ãƒ­ãƒã‚¹ãƒˆçµ±è¨ˆé‡ã®ä½¿ç”¨ã‚’æ¤œè¨")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ”¹å–„ææ¡ˆ
        distribution_report = validation_result.detailed_report.get("distribution", {})
        for col, skewness in distribution_report.get("skewness", {}).items():
            if abs(skewness) > 1:
                improvements.append(f"ã‚«ãƒ©ãƒ  '{col}' ã®åˆ†å¸ƒæ”¹å–„: å¯¾æ•°å¤‰æ›ã€Box-Coxå¤‰æ›ã€ã¾ãŸã¯æ­£è¦åŒ–ã‚’æ¤œè¨")

        # ãƒ‡ãƒ¼ã‚¿é‡ã®æ”¹å–„ææ¡ˆ
        if len(data) < 100:
            improvements.append("ãƒ‡ãƒ¼ã‚¿é‡ã®å¢—åŠ : ã‚ˆã‚Šå¤šãã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’æ¤œè¨")

        return improvements

    def generate_quality_report(
        self, data: pd.DataFrame, validation_result: ValidationResult
    ) -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = []
        report.append("=" * 50)
        report.append("ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
        report.append("=" * 50)

        # åŸºæœ¬æƒ…å ±
        report.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data)} è¡Œ Ã— {len(data.columns)} åˆ—")
        report.append(f"å“è³ªã‚¹ã‚³ã‚¢: {validation_result.score:.2f}")
        report.append(f"æ¤œè¨¼çµæœ: {'åˆæ ¼' if validation_result.is_valid else 'ä¸åˆæ ¼'}")

        # å•é¡Œã®è©³ç´°
        if validation_result.issues:
            report.append("\nã€å•é¡Œã€‘")
            for i, issue in enumerate(validation_result.issues, 1):
                report.append(f"{i}. {issue}")

        # è­¦å‘Šã®è©³ç´°
        if validation_result.warnings:
            report.append("\nã€è­¦å‘Šã€‘")
            for i, warning in enumerate(validation_result.warnings, 1):
                report.append(f"{i}. {warning}")

        # æ¨å¥¨äº‹é …
        if validation_result.recommendations:
            report.append("\nã€æ¨å¥¨äº‹é …ã€‘")
            for i, recommendation in enumerate(validation_result.recommendations, 1):
                report.append(f"{i}. {recommendation}")

        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        report.append("\nã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã€‘")
        for key, value in validation_result.detailed_report.items():
            report.append(f"{key}: {value}")

        return "\n".join(report)


def validate_stock_data(
    data: pd.DataFrame, validation_level: ValidationLevel = ValidationLevel.STANDARD
) -> ValidationResult:
    """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    validator = EnhancedDataValidator(validation_level)
    return validator.validate_data_quality(data)


def detect_stock_anomalies(
    data: pd.DataFrame, method: str = "isolation_forest"
) -> Dict[str, Any]:
    """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸å€¤æ¤œå‡ºï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    validator = EnhancedDataValidator()
    return validator.detect_anomalies(data, method)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_data = pd.DataFrame(
        {
            "Date": pd.date_range("2023-01-01", periods=100, freq="D"),
            "Open": np.random.uniform(50, 200, 100),
            "High": np.random.uniform(50, 200, 100),
            "Low": np.random.uniform(50, 200, 100),
            "Close": np.random.uniform(50, 200, 100),
            "Volume": np.random.randint(1000, 10000, 100),
        }
    )

    # æ¤œè¨¼ã®å®Ÿè¡Œ
    validator = EnhancedDataValidator(ValidationLevel.STANDARD)
    result = validator.validate_data_quality(sample_data)

    print(f"æ¤œè¨¼çµæœ: {result.is_valid}")
    print(f"å“è³ªã‚¹ã‚³ã‚¢: {result.score:.2f}")
    print(f"å•é¡Œæ•°: {len(result.issues)}")
    print(f"è­¦å‘Šæ•°: {len(result.warnings)}")
