#!/usr/bin/env python3
"""
é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨å‡¦ç†æ™‚é–“ã‚’å¤§å¹…ã«æ”¹å–„
"""

import pandas as pd
import numpy as np
import psutil
import gc
import logging
from typing import Dict, List, Optional, Generator, Tuple, Any, Callable
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from functools import lru_cache
import tracemalloc
from contextlib import contextmanager
import time
import hashlib
import joblib
import os
from dataclasses import dataclass
from unified_system import UnifiedSystem

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    memory_usage_mb: float
    processing_time_seconds: float
    cpu_usage_percent: float
    memory_peak_mb: float
    operations_count: int
    cache_hits: int
    cache_misses: int


class AdvancedMemoryOptimizer:
    """é«˜åº¦ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, memory_limit_mb: int = 2048, chunk_size: int = 10000):
        self.memory_limit_mb = memory_limit_mb
        self.chunk_size = chunk_size
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€psutil.Process()ã®åˆæœŸåŒ–ã‚‚ç„¡åŠ¹åŒ–
        self.process = None
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

        # ãƒ¡ãƒ¢ãƒªç›£è¦–ã®é–‹å§‹
        tracemalloc.start()

    def get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        if self.process is None:
            # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return 0.0
        return self.process.memory_info().rss / 1024 / 1024

    def check_memory_limit(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯"""
        current_memory = self.get_memory_usage()
        return current_memory < self.memory_limit_mb

    def optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        self.logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–é–‹å§‹")

        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024

        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        df_optimized = df.copy()

        for col in df_optimized.columns:
            col_type = df_optimized[col].dtype

            if col_type != "object":
                c_min = df_optimized[col].min()
                c_max = df_optimized[col].max()

                if str(col_type)[:3] == "int":
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df_optimized[col] = df_optimized[col].astype(np.int8)
                    elif (
                        c_min > np.iinfo(np.int16).min
                        and c_max < np.iinfo(np.int16).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.int16)
                    elif (
                        c_min > np.iinfo(np.int32).min
                        and c_max < np.iinfo(np.int32).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.int32)
                else:
                    if (
                        c_min > np.finfo(np.float16).min
                        and c_max < np.finfo(np.float16).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.float16)
                    elif (
                        c_min > np.finfo(np.float32).min
                        and c_max < np.finfo(np.float32).max
                    ):
                        df_optimized[col] = df_optimized[col].astype(np.float32)

        optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024 / 1024
        reduction = (original_memory - optimized_memory) / original_memory * 100

        self.logger.info(
            f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Œäº†: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB ({reduction:.1f}%å‰Šæ¸›)"
        )

        return df_optimized

    def process_large_dataframe(
        self, df: pd.DataFrame, processing_func: Callable
    ) -> pd.DataFrame:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        self.logger.info(f"ğŸ“Š å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†é–‹å§‹: {len(df)}è¡Œ")

        if len(df) <= self.chunk_size:
            return processing_func(df)

        results = []
        for i in range(0, len(df), self.chunk_size):
            chunk = df.iloc[i : i + self.chunk_size]

            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if not self.check_memory_limit():
                self.logger.warning("âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
                gc.collect()

            # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            processed_chunk = processing_func(chunk)
            results.append(processed_chunk)

            self.logger.debug(
                f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†: {i//self.chunk_size + 1}/{(len(df)-1)//self.chunk_size + 1}"
            )

        # çµæœã‚’çµåˆ
        final_result = pd.concat(results, ignore_index=True)
        self.logger.info(f"âœ… å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†å®Œäº†: {len(final_result)}è¡Œ")

        return final_result


class AdvancedCacheManager:
    """é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(
        self, cache_dir: str = "advanced_cache", max_cache_size_mb: int = 1024
    ):
        self.cache_dir = cache_dir
        self.max_cache_size_mb = max_cache_size_mb
        self.cache_stats = {"hits": 0, "misses": 0}
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

        os.makedirs(cache_dir, exist_ok=True)

    def _generate_cache_key(self, data_hash: str, operation: str, params: Dict) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        key_string = f"{data_hash}_{operation}_{str(sorted(params.items()))}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def _generate_data_hash(self, data: Any) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        if isinstance(data, pd.DataFrame):
            return hashlib.md5(data.to_string().encode()).hexdigest()
        elif isinstance(data, np.ndarray):
            return hashlib.md5(data.tobytes()).hexdigest()
        else:
            return hashlib.md5(str(data).encode()).hexdigest()

    def get_cached_result(
        self, data: Any, operation: str, params: Dict, compute_func: Callable
    ) -> Any:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã¾ãŸã¯è¨ˆç®—"""
        data_hash = self._generate_data_hash(data)
        cache_key = self._generate_cache_key(data_hash, operation, params)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.joblib")

        if os.path.exists(cache_file):
            try:
                self.cache_stats["hits"] += 1
                self.logger.debug(f"ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {operation}")
                return joblib.load(cache_file)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - è¨ˆç®—å®Ÿè¡Œ
        self.cache_stats["misses"] += 1
        self.logger.debug(f"ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {operation}")

        result = compute_func(data, params)

        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        try:
            joblib.dump(result, cache_file)
            self.logger.debug(f"ğŸ’¾ çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {operation}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return result

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        try:
            for file in os.listdir(self.cache_dir):
                if file.endswith(".joblib"):
                    os.remove(os.path.join(self.cache_dir, file))
            self.logger.info("ğŸ§¹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

    def get_cache_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (
            (self.cache_stats["hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "hit_rate": hit_rate,
            "total_requests": total_requests,
        }


class OptimizedTechnicalIndicators:
    """æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        memory_optimizer: AdvancedMemoryOptimizer,
        cache_manager: AdvancedCacheManager,
    ):
        self.memory_optimizer = memory_optimizer
        self.cache_manager = cache_manager
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

    def calculate_indicators_optimized(
        self, df: pd.DataFrame, config: Dict = None
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—"""
        self.logger.info("ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—é–‹å§‹")

        if config is None:
            config = self._get_default_config()

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        df_optimized = self.memory_optimizer.optimize_dataframe_memory(df)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ãŸæŒ‡æ¨™è¨ˆç®—
        result = self.cache_manager.get_cached_result(
            df_optimized,
            "technical_indicators",
            config,
            self._calculate_indicators_internal,
        )

        return result

    def _calculate_indicators_internal(
        self, df: pd.DataFrame, config: Dict
    ) -> pd.DataFrame:
        """å†…éƒ¨çš„ãªæŒ‡æ¨™è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰"""
        result_df = df.copy()

        try:
            # ç§»å‹•å¹³å‡ç³»ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            result_df = self._calculate_moving_averages_optimized(result_df, config)

            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç³»æŒ‡æ¨™ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            result_df = self._calculate_momentum_optimized(result_df, config)

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»æŒ‡æ¨™ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            result_df = self._calculate_volatility_optimized(result_df, config)

            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»æŒ‡æ¨™ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            result_df = self._calculate_volume_optimized(result_df, config)

            self.logger.info("âœ… æœ€é©åŒ–ã•ã‚ŒãŸæŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œäº†")
            return result_df

        except Exception as e:
            self.system.log_error(e, "æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼")
            return df

    def _calculate_moving_averages_optimized(
        self, df: pd.DataFrame, config: Dict
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸç§»å‹•å¹³å‡è¨ˆç®—"""
        windows = config.get("sma_windows", [5, 10, 20, 50])

        for window in windows:
            # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            df[f"SMA_{window}"] = (
                df["Close"].rolling(window=window, min_periods=1).mean()
            )

        return df

    def _calculate_momentum_optimized(
        self, df: pd.DataFrame, config: Dict
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™è¨ˆç®—"""
        # RSIè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        rsi_period = config.get("rsi_period", 14)
        delta = df["Close"].diff()
        gain = (
            delta.where(delta > 0, 0).rolling(window=rsi_period, min_periods=1).mean()
        )
        loss = (
            (-delta.where(delta < 0, 0))
            .rolling(window=rsi_period, min_periods=1)
            .mean()
        )
        rs = gain / loss
        df["RSI"] = 100 - (100 / (1 + rs))

        # MACDè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        fast = config.get("macd_fast", 12)
        slow = config.get("macd_slow", 26)
        signal = config.get("macd_signal", 9)

        ema_fast = df["Close"].ewm(span=fast, min_periods=1).mean()
        ema_slow = df["Close"].ewm(span=slow, min_periods=1).mean()
        df["MACD"] = ema_fast - ema_slow
        df["MACD_Signal"] = df["MACD"].ewm(span=signal, min_periods=1).mean()
        df["MACD_Histogram"] = df["MACD"] - df["MACD_Signal"]

        return df

    def _calculate_volatility_optimized(
        self, df: pd.DataFrame, config: Dict
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™è¨ˆç®—"""
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        bb_period = config.get("bb_period", 20)
        bb_std = config.get("bb_std", 2)

        sma = df["Close"].rolling(window=bb_period, min_periods=1).mean()
        std = df["Close"].rolling(window=bb_period, min_periods=1).std()

        df["BB_Upper"] = sma + (std * bb_std)
        df["BB_Lower"] = sma - (std * bb_std)
        df["BB_Percent"] = (df["Close"] - df["BB_Lower"]) / (
            df["BB_Upper"] - df["BB_Lower"]
        )

        # ATRè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        atr_period = config.get("atr_period", 14)
        high_low = df["High"] - df["Low"]
        high_close_prev = np.abs(df["High"] - df["Close"].shift(1))
        low_close_prev = np.abs(df["Low"] - df["Close"].shift(1))
        true_range = np.maximum(high_low, np.maximum(high_close_prev, low_close_prev))
        df["ATR"] = true_range.rolling(window=atr_period, min_periods=1).mean()

        return df

    def _calculate_volume_optimized(
        self, df: pd.DataFrame, config: Dict
    ) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™è¨ˆç®—"""
        # VWAPè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        typical_price = (df["High"] + df["Low"] + df["Close"]) / 3
        df["VWAP"] = (typical_price * df["Volume"]).cumsum() / df["Volume"].cumsum()
        df["VWAP_Deviation"] = (df["Close"] - df["VWAP"]) / df["VWAP"] * 100

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç§»å‹•å¹³å‡
        volume_sma_period = config.get("volume_sma_period", 20)
        df["Volume_SMA"] = (
            df["Volume"].rolling(window=volume_sma_period, min_periods=1).mean()
        )
        df["Volume_Rate"] = df["Volume"] / df["Volume_SMA"]

        return df

    def _get_default_config(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
        return {
            "sma_windows": [5, 10, 20, 50],
            "rsi_period": 14,
            "macd_fast": 12,
            "macd_slow": 26,
            "macd_signal": 9,
            "bb_period": 20,
            "bb_std": 2,
            "atr_period": 14,
            "volume_sma_period": 20,
        }


class AdvancedPerformanceMonitor:
    """é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.metrics_history = []
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

    @contextmanager
    def monitor_performance(self, operation_name: str):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024

            metrics = PerformanceMetrics(
                memory_usage_mb=end_memory,
                processing_time_seconds=end_time - start_time,
                cpu_usage_percent=psutil.cpu_percent(),
                memory_peak_mb=end_memory,
                operations_count=1,
                cache_hits=0,
                cache_misses=0,
            )

            self.metrics_history.append(
                {
                    "operation": operation_name,
                    "metrics": metrics,
                    "timestamp": pd.Timestamp.now(),
                }
            )

            self.logger.info(f"ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {operation_name}")
            self.logger.info(f"  â±ï¸ å‡¦ç†æ™‚é–“: {metrics.processing_time_seconds:.2f}ç§’")
            self.logger.info(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {metrics.memory_usage_mb:.1f}MB")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.metrics_history:
            return {}

        total_time = sum(
            m["metrics"].processing_time_seconds for m in self.metrics_history
        )
        avg_memory = np.mean(
            [m["metrics"].memory_usage_mb for m in self.metrics_history]
        )
        peak_memory = max([m["metrics"].memory_peak_mb for m in self.metrics_history])

        return {
            "total_operations": len(self.metrics_history),
            "total_time_seconds": total_time,
            "average_memory_mb": avg_memory,
            "peak_memory_mb": peak_memory,
            "operations": [m["operation"] for m in self.metrics_history],
        }


class UnifiedPerformanceOptimizer:
    """çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, memory_limit_mb: int = 2048, chunk_size: int = 10000):
        self.memory_optimizer = AdvancedMemoryOptimizer(memory_limit_mb, chunk_size)
        self.cache_manager = AdvancedCacheManager()
        self.technical_indicators = OptimizedTechnicalIndicators(
            self.memory_optimizer, self.cache_manager
        )
        self.performance_monitor = AdvancedPerformanceMonitor()
        # å¾ªç’°å‚ç…§ã‚’å›é¿ã™ã‚‹ãŸã‚ã€UnifiedSystemã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–
        self.system = None
        self.logger = logging.getLogger(__name__)

    def optimize_data_processing(
        self, df: pd.DataFrame, operations: List[Dict]
    ) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–"""
        self.logger.info(f"ğŸš€ çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–é–‹å§‹: {len(operations)}æ“ä½œ")

        with self.performance_monitor.monitor_performance("data_processing"):
            result_df = df

            for operation in operations:
                op_type = operation.get("type")

                if op_type == "technical_indicators":
                    result_df = (
                        self.technical_indicators.calculate_indicators_optimized(
                            result_df, operation.get("config")
                        )
                    )
                elif op_type == "memory_optimization":
                    result_df = self.memory_optimizer.optimize_dataframe_memory(
                        result_df
                    )
                elif op_type == "chunk_processing":
                    result_df = self.memory_optimizer.process_large_dataframe(
                        result_df, operation.get("processing_func")
                    )
                else:
                    self.logger.warning(f"âš ï¸ æœªå¯¾å¿œã®æ“ä½œã‚¿ã‚¤ãƒ—: {op_type}")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›
            self._log_optimization_stats()

            return result_df

    def _log_optimization_stats(self):
        """æœ€é©åŒ–çµ±è¨ˆã‚’ãƒ­ã‚°å‡ºåŠ›"""
        cache_stats = self.cache_manager.get_cache_stats()
        performance_summary = self.performance_monitor.get_performance_summary()

        self.logger.info("ğŸ“Š æœ€é©åŒ–çµ±è¨ˆ:")
        self.logger.info(f"  ğŸ“‹ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_stats.get('hit_rate', 0):.1f}%")
        self.logger.info(
            f"  â±ï¸ ç·å‡¦ç†æ™‚é–“: {performance_summary.get('total_time_seconds', 0):.2f}ç§’"
        )
        self.logger.info(
            f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {performance_summary.get('average_memory_mb', 0):.1f}MB"
        )
        self.logger.info(
            f"  ğŸ“ˆ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {performance_summary.get('peak_memory_mb', 0):.1f}MB"
        )


def create_performance_optimizer(
    memory_limit_mb: int = 2048, chunk_size: int = 10000
) -> UnifiedPerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""
    return UnifiedPerformanceOptimizer(memory_limit_mb, chunk_size)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    import pandas as pd
    import numpy as np

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    dates = pd.date_range("2024-01-01", periods=1000, freq="D")
    np.random.seed(42)

    base_price = 1000 + np.cumsum(np.random.randn(1000) * 0.02) * 1000

    sample_data = pd.DataFrame(
        {
            "Date": dates,
            "Open": base_price,
            "High": base_price * (1 + np.random.uniform(0, 0.05, 1000)),
            "Low": base_price * (1 - np.random.uniform(0, 0.05, 1000)),
            "Close": base_price + np.random.uniform(-20, 20, 1000),
            "Volume": np.random.randint(1000000, 10000000, 1000),
        }
    )

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ
    optimizer = create_performance_optimizer()

    operations = [
        {"type": "technical_indicators", "config": {}},
        {"type": "memory_optimization", "config": {}},
    ]

    optimized_data = optimizer.optimize_data_processing(sample_data, operations)

    print(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(sample_data.columns)}åˆ—, {len(sample_data)}è¡Œ")
    print(f"ğŸ“ˆ æœ€é©åŒ–å¾Œ: {len(optimized_data.columns)}åˆ—, {len(optimized_data)}è¡Œ")
    print(f"â• è¿½åŠ æŒ‡æ¨™: {len(optimized_data.columns) - len(sample_data.columns)}å€‹")
