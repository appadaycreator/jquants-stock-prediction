#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿å–å¾—ä¿¡é ¼æ€§å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ 
APIæ¥ç¶šã®å®‰å®šæ€§ç¢ºä¿ã¨ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã‚’æä¾›
"""

import time
import logging
import requests
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import json
import hashlib

logger = logging.getLogger(__name__)


class DataQualityLevel(Enum):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒ™ãƒ«"""
    EXCELLENT = "excellent"  # 95%ä»¥ä¸Š
    GOOD = "good"          # 80-95%
    FAIR = "fair"          # 60-80%
    POOR = "poor"          # 40-60%
    CRITICAL = "critical"  # 40%æœªæº€


@dataclass
class DataQualityMetrics:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    completeness: float  # å®Œå…¨æ€§ï¼ˆæ¬ æå€¤ã®å°‘ãªã•ï¼‰
    accuracy: float       # ç²¾åº¦ï¼ˆç•°å¸¸å€¤ã®å°‘ãªã•ï¼‰
    consistency: float    # ä¸€è²«æ€§ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ï¼‰
    timeliness: float     # é®®åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æ–°ã—ã•ï¼‰
    overall_score: float  # ç·åˆã‚¹ã‚³ã‚¢
    quality_level: DataQualityLevel
    issues: List[str]     # ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ


class EnhancedDataReliabilitySystem:
    """ãƒ‡ãƒ¼ã‚¿å–å¾—ä¿¡é ¼æ€§å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆæœŸåŒ–"""
        self.config = config or {}
        self.session = requests.Session()
        self.quality_history = []
        self.connection_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "timeout_requests": 0,
            "connection_errors": 0,
            "data_quality_issues": 0
        }
        
        # è¨­å®šã®åˆæœŸåŒ–
        self.max_retries = self.config.get("max_retries", 5)
        self.base_retry_interval = self.config.get("base_retry_interval", 2)
        self.max_retry_interval = self.config.get("max_retry_interval", 60)
        self.backoff_multiplier = self.config.get("backoff_multiplier", 2)
        self.timeout = self.config.get("timeout", 30)
        self.min_quality_score = self.config.get("min_quality_score", 0.8)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session.headers.update({
            'User-Agent': 'J-Quants-Stock-Prediction/2.0',
            'Accept': 'application/json',
            'Connection': 'keep-alive'
        })
        
        logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¿¡é ¼æ€§å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def make_reliable_request(
        self, 
        method: str, 
        url: str, 
        **kwargs
    ) -> Tuple[requests.Response, Dict[str, Any]]:
        """
        ä¿¡é ¼æ€§ã®é«˜ã„APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
        Returns:
            Tuple[Response, Metadata]: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        start_time = time.time()
        self.connection_stats["total_requests"] += 1
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        max_retries = kwargs.pop('max_retries', self.max_retries)
        timeout = kwargs.pop('timeout', self.timeout)
        
        last_error = None
        response_data = None
        
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ğŸŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {method} {url}")
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
                response = self.session.request(
                    method, 
                    url, 
                    timeout=timeout,
                    **kwargs
                )
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®è¨˜éŒ²
                response_time = time.time() - start_time
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                if response.status_code == 200:
                    self.connection_stats["successful_requests"] += 1
                    logger.info(f"âœ… APIãƒªã‚¯ã‚¨ã‚¹ãƒˆæˆåŠŸ: {response.status_code} ({response_time:.2f}s)")
                    
                    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
                    quality_metrics = self._validate_response_data(response)
                    
                    if quality_metrics.overall_score >= self.min_quality_score:
                        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªè‰¯å¥½: {quality_metrics.overall_score:.2f}")
                        return response, {
                            "attempt": attempt + 1,
                            "response_time": response_time,
                            "quality_metrics": quality_metrics,
                            "success": True
                        }
                    else:
                        logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿å“è³ªãŒåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™: {quality_metrics.overall_score:.2f}")
                        self.connection_stats["data_quality_issues"] += 1
                        
                        if attempt < max_retries:
                            continue
                        else:
                            logger.error("âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªãŒåŸºæº–ã‚’æº€ãŸã—ã¾ã›ã‚“")
                            raise ValueError(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãŒåŸºæº–ã‚’æº€ãŸã—ã¾ã›ã‚“: {quality_metrics.overall_score:.2f}")
                else:
                    logger.warning(f"âš ï¸ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                    last_error = requests.exceptions.HTTPError(f"HTTP {response.status_code}: {response.text}")
                    
            except requests.exceptions.Timeout as e:
                self.connection_stats["timeout_requests"] += 1
                last_error = e
                logger.warning(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {e}")
                
            except requests.exceptions.ConnectionError as e:
                self.connection_stats["connection_errors"] += 1
                last_error = e
                logger.warning(f"ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {e}")
                
            except Exception as e:
                last_error = e
                logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {e}")
            
            # ãƒªãƒˆãƒ©ã‚¤å‰ã®å¾…æ©Ÿ
            if attempt < max_retries:
                retry_interval = min(
                    self.base_retry_interval * (self.backoff_multiplier ** attempt),
                    self.max_retry_interval
                )
                logger.info(f"â³ {retry_interval}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(retry_interval)
        
        # å…¨è©¦è¡Œå¤±æ•—
        self.connection_stats["failed_requests"] += 1
        logger.error(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ: {max_retries + 1}å›")
        
        if last_error:
            raise last_error
        else:
            raise Exception("APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")

    def _validate_response_data(self, response: requests.Response) -> DataQualityMetrics:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®å“è³ªæ¤œè¨¼"""
        try:
            # JSONãƒ‡ãƒ¼ã‚¿ã®è§£æ
            data = response.json()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            df = None
            if isinstance(data, dict) and 'data' in data:
                df = pd.DataFrame(data['data'])
            elif isinstance(data, list):
                df = pd.DataFrame(data)
            
            if df is not None and not df.empty:
                return self._calculate_data_quality(df)
            else:
                # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆ
                return DataQualityMetrics(
                    completeness=0.0,
                    accuracy=0.0,
                    consistency=0.0,
                    timeliness=0.0,
                    overall_score=0.0,
                    quality_level=DataQualityLevel.CRITICAL,
                    issues=["ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™"]
                )
                
        except json.JSONDecodeError:
            return DataQualityMetrics(
                completeness=0.0,
                accuracy=0.0,
                consistency=0.0,
                timeliness=0.0,
                overall_score=0.0,
                quality_level=DataQualityLevel.CRITICAL,
                issues=["JSONè§£æã‚¨ãƒ©ãƒ¼"]
            )
        except Exception as e:
            return DataQualityMetrics(
                completeness=0.0,
                accuracy=0.0,
                consistency=0.0,
                timeliness=0.0,
                overall_score=0.0,
                quality_level=DataQualityLevel.CRITICAL,
                issues=[f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"]
            )

    def _calculate_data_quality(self, df: pd.DataFrame) -> DataQualityMetrics:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã®è¨ˆç®—"""
        issues = []
        
        # 1. å®Œå…¨æ€§ï¼ˆæ¬ æå€¤ã®å°‘ãªã•ï¼‰
        total_cells = df.size
        missing_cells = df.isnull().sum().sum()
        completeness = 1.0 - (missing_cells / total_cells) if total_cells > 0 else 0.0
        
        if completeness < 0.9:
            issues.append(f"æ¬ æå€¤ãŒå¤šã™ãã¾ã™: {missing_cells}/{total_cells}")
        
        # 2. ç²¾åº¦ï¼ˆç•°å¸¸å€¤ã®å°‘ãªã•ï¼‰
        accuracy = 1.0
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outlier_ratio = len(outliers) / len(df) if len(df) > 0 else 0
                
                if outlier_ratio > 0.1:  # 10%ä»¥ä¸Šã®ç•°å¸¸å€¤
                    accuracy -= outlier_ratio * 0.5
                    issues.append(f"åˆ— '{col}' ã«ç•°å¸¸å€¤ãŒå¤šã™ãã¾ã™: {outlier_ratio:.1%}")
        
        accuracy = max(0.0, accuracy)
        
        # 3. ä¸€è²«æ€§ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ï¼‰
        consistency = 1.0
        
        # æ—¥ä»˜åˆ—ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        date_columns = ['date', 'Date', 'timestamp', 'Timestamp']
        for col in date_columns:
            if col in df.columns:
                try:
                    pd.to_datetime(df[col])
                except:
                    consistency -= 0.2
                    issues.append(f"æ—¥ä»˜åˆ— '{col}' ã®å½¢å¼ãŒä¸æ­£ã§ã™")
        
        # æ•°å€¤åˆ—ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        for col in numeric_columns:
            if col in df.columns:
                if df[col].dtype == 'object':
                    consistency -= 0.1
                    issues.append(f"æ•°å€¤åˆ— '{col}' ã«æ–‡å­—åˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        
        consistency = max(0.0, consistency)
        
        # 4. é®®åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æ–°ã—ã•ï¼‰
        timeliness = 1.0
        if 'date' in df.columns or 'Date' in df.columns:
            date_col = 'date' if 'date' in df.columns else 'Date'
            try:
                dates = pd.to_datetime(df[date_col])
                latest_date = dates.max()
                days_old = (datetime.now() - latest_date).days
                
                if days_old > 7:
                    timeliness = max(0.0, 1.0 - (days_old - 7) / 30)
                    issues.append(f"ãƒ‡ãƒ¼ã‚¿ãŒå¤ã™ãã¾ã™: {days_old}æ—¥å‰")
            except:
                timeliness = 0.5
                issues.append("æ—¥ä»˜ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        overall_score = (completeness * 0.3 + accuracy * 0.3 + 
                        consistency * 0.2 + timeliness * 0.2)
        
        # å“è³ªãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
        if overall_score >= 0.95:
            quality_level = DataQualityLevel.EXCELLENT
        elif overall_score >= 0.8:
            quality_level = DataQualityLevel.GOOD
        elif overall_score >= 0.6:
            quality_level = DataQualityLevel.FAIR
        elif overall_score >= 0.4:
            quality_level = DataQualityLevel.POOR
        else:
            quality_level = DataQualityLevel.CRITICAL
        
        return DataQualityMetrics(
            completeness=completeness,
            accuracy=accuracy,
            consistency=consistency,
            timeliness=timeliness,
            overall_score=overall_score,
            quality_level=quality_level,
            issues=issues
        )

    def get_connection_statistics(self) -> Dict[str, Any]:
        """æ¥ç¶šçµ±è¨ˆã®å–å¾—"""
        total = self.connection_stats["total_requests"]
        if total == 0:
            return self.connection_stats
        
        success_rate = self.connection_stats["successful_requests"] / total
        failure_rate = self.connection_stats["failed_requests"] / total
        timeout_rate = self.connection_stats["timeout_requests"] / total
        connection_error_rate = self.connection_stats["connection_errors"] / total
        quality_issue_rate = self.connection_stats["data_quality_issues"] / total
        
        return {
            **self.connection_stats,
            "success_rate": success_rate,
            "failure_rate": failure_rate,
            "timeout_rate": timeout_rate,
            "connection_error_rate": connection_error_rate,
            "quality_issue_rate": quality_issue_rate
        }

    def get_quality_report(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        if not self.quality_history:
            return {"message": "å“è³ªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        recent_metrics = self.quality_history[-10:]  # ç›´è¿‘10å›
        
        avg_completeness = np.mean([m.completeness for m in recent_metrics])
        avg_accuracy = np.mean([m.accuracy for m in recent_metrics])
        avg_consistency = np.mean([m.consistency for m in recent_metrics])
        avg_timeliness = np.mean([m.timeliness for m in recent_metrics])
        avg_overall = np.mean([m.overall_score for m in recent_metrics])
        
        # å“è³ªãƒ¬ãƒ™ãƒ«ã®åˆ†å¸ƒ
        level_counts = {}
        for level in DataQualityLevel:
            level_counts[level.value] = sum(1 for m in recent_metrics if m.quality_level == level)
        
        return {
            "average_metrics": {
                "completeness": avg_completeness,
                "accuracy": avg_accuracy,
                "consistency": avg_consistency,
                "timeliness": avg_timeliness,
                "overall_score": avg_overall
            },
            "quality_level_distribution": level_counts,
            "total_samples": len(self.quality_history),
            "recent_samples": len(recent_metrics)
        }

    def add_fallback_data_source(self, source_name: str, source_config: Dict[str, Any]):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¿½åŠ """
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚½ãƒ¼ã‚¹ã®å®Ÿè£…
        logger.info(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ : {source_name}")

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.session.close()
        logger.info("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¿¡é ¼æ€§å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
