#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å®Ÿè£…æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
æœ€é©åŒ–åŠ¹æœã‚’æ¸¬å®šã—ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®šã™ã‚‹åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
import logging
import time
import psutil
import gc
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from unified_system import UnifiedSystem
from unified_performance_optimizer import create_unified_performance_optimizer
from advanced_performance_optimizer import create_performance_optimizer
from enhanced_model_comparator import create_enhanced_model_comparator
from ultra_efficient_dataframe_processor import create_ultra_efficient_processor

logger = logging.getLogger(__name__)


@dataclass
class PerformanceTestResult:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ"""

    test_name: str
    baseline_time: float
    optimized_time: float
    time_improvement: float
    baseline_memory: float
    optimized_memory: float
    memory_improvement: float
    success: bool
    error_message: str = ""


class PerformanceOptimizationTester:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.system = UnifiedSystem("PerformanceOptimizationTester")
        self.logger = logging.getLogger(__name__)
        self.test_results = []

    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")

        test_results = {}

        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_results["dataframe_processing"] = self._test_dataframe_processing()

        # 2. æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_results["technical_indicators"] = self._test_technical_indicators()

        # 3. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_results["model_comparison"] = self._test_model_comparison()

        # 4. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_results["memory_optimization"] = self._test_memory_optimization()

        # 5. ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_results["parallel_processing"] = self._test_parallel_processing()

        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        comprehensive_report = self._generate_comprehensive_report(test_results)

        self.logger.info("âœ… åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")
        return comprehensive_report

    def _test_dataframe_processing(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        self.logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_rows = 10000
        n_cols = 20

        data = {f"col_{i}": np.random.randn(n_rows) for i in range(n_cols)}
        df = pd.DataFrame(data)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        baseline_df = df.copy()
        baseline_df = baseline_df.fillna(method="ffill")
        baseline_df = baseline_df.dropna()
        baseline_df = baseline_df.astype(
            {f"col_{i}": np.float32 for i in range(n_cols)}
        )

        baseline_time = time.time() - start_time
        baseline_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        # æœ€é©åŒ–å‡¦ç†
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        processor = create_ultra_efficient_processor()
        operations = [
            {"type": "dtype_optimization", "params": {}},
            {
                "type": "inplace",
                "params": {"type": "fillna", "params": {"method": "ffill"}},
            },
            {"type": "inplace", "params": {"type": "dropna", "params": {}}},
        ]

        optimized_df = processor.process_dataframe_ultra_efficient(df, operations)

        optimized_time = time.time() - start_time
        optimized_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        # çµæœã‚’è¿”ã™
        return {
            "baseline_time": baseline_time,
            "optimized_time": optimized_time,
            "time_improvement": (baseline_time - optimized_time) / baseline_time * 100,
            "baseline_memory": baseline_memory,
            "optimized_memory": optimized_memory,
            "memory_improvement": (baseline_memory - optimized_memory)
            / baseline_memory
            * 100,
            "success": True,
        }

    def _test_technical_indicators(self) -> Dict[str, Any]:
        """æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        self.logger.info("ğŸ“ˆ æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        dates = pd.date_range("2024-01-01", periods=1000, freq="D")
        base_price = 1000 + np.cumsum(np.random.randn(1000) * 0.02) * 1000

        df = pd.DataFrame(
            {
                "Date": dates,
                "Open": base_price,
                "High": base_price * (1 + np.random.uniform(0, 0.05, 1000)),
                "Low": base_price * (1 - np.random.uniform(0, 0.05, 1000)),
                "Close": base_price + np.random.uniform(-20, 20, 1000),
                "Volume": np.random.randint(1000000, 10000000, 1000),
            }
        )

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        baseline_df = df.copy()
        # ç°¡å˜ãªæŠ€è¡“æŒ‡æ¨™è¨ˆç®—
        baseline_df["SMA_20"] = baseline_df["Close"].rolling(window=20).mean()
        baseline_df["RSI"] = 100 - (
            100 / (1 + baseline_df["Close"].diff().rolling(window=14).mean())
        )

        baseline_time = time.time() - start_time
        baseline_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        # æœ€é©åŒ–å‡¦ç†
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        optimizer = create_performance_optimizer()
        optimized_df = optimizer.technical_indicators.calculate_indicators_optimized(df)

        optimized_time = time.time() - start_time
        optimized_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        return {
            "baseline_time": baseline_time,
            "optimized_time": optimized_time,
            "time_improvement": (baseline_time - optimized_time) / baseline_time * 100,
            "baseline_memory": baseline_memory,
            "optimized_memory": optimized_memory,
            "memory_improvement": (baseline_memory - optimized_memory)
            / baseline_memory
            * 100,
            "success": True,
        }

    def _test_model_comparison(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        self.logger.info("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_samples = 1000
        n_features = 10

        X = np.random.randn(n_samples, n_features)
        y = np.random.randn(n_samples)

        from sklearn.model_selection import train_test_split

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        models_config = {
            "random_forest": {
                "type": "random_forest",
                "params": {"n_estimators": 50, "random_state": 42},
            },
            "linear_regression": {"type": "linear_regression", "params": {}},
            "ridge": {"type": "ridge", "params": {"alpha": 1.0}},
        }

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LinearRegression, Ridge
        from sklearn.metrics import mean_absolute_error

        baseline_results = []
        for model_name, config in models_config.items():
            if config["type"] == "random_forest":
                model = RandomForestRegressor(**config["params"])
            elif config["type"] == "linear_regression":
                model = LinearRegression(**config["params"])
            elif config["type"] == "ridge":
                model = Ridge(**config["params"])

            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            mae = mean_absolute_error(y_test, y_pred)
            baseline_results.append({"model": model_name, "mae": mae})

        baseline_time = time.time() - start_time
        baseline_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        # æœ€é©åŒ–å‡¦ç†
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        comparator = create_enhanced_model_comparator(use_cache=True, use_parallel=True)
        optimized_results = comparator.compare_models_enhanced(
            models_config, X_train, X_test, y_train, y_test
        )

        optimized_time = time.time() - start_time
        optimized_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        return {
            "baseline_time": baseline_time,
            "optimized_time": optimized_time,
            "time_improvement": (baseline_time - optimized_time) / baseline_time * 100,
            "baseline_memory": baseline_memory,
            "optimized_memory": optimized_memory,
            "memory_improvement": (baseline_memory - optimized_memory)
            / baseline_memory
            * 100,
            "success": True,
        }

    def _test_memory_optimization(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        self.logger.info("ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_rows = 50000
        n_cols = 50

        data = {f"col_{i}": np.random.randn(n_rows) for i in range(n_cols)}
        df = pd.DataFrame(data)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        baseline_df = df.copy()
        baseline_df = baseline_df.fillna(method="ffill")
        baseline_df = baseline_df.dropna()

        baseline_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        # æœ€é©åŒ–å‡¦ç†
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        optimizer = create_performance_optimizer()
        optimized_df = optimizer.memory_optimizer.optimize_dataframe_memory(df)
        optimized_df = optimized_df.fillna(method="ffill")
        optimized_df = optimized_df.dropna()

        optimized_memory = (
            psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
        )

        return {
            "baseline_memory": baseline_memory,
            "optimized_memory": optimized_memory,
            "memory_improvement": (baseline_memory - optimized_memory)
            / baseline_memory
            * 100,
            "success": True,
        }

    def _test_parallel_processing(self) -> Dict[str, Any]:
        """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        self.logger.info("ğŸš€ ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_samples = 2000
        n_features = 15

        X = np.random.randn(n_samples, n_features)
        y = np.random.randn(n_samples)

        from sklearn.model_selection import train_test_split

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        models_config = {
            f"model_{i}": {
                "type": "random_forest",
                "params": {"n_estimators": 30, "random_state": 42},
            }
            for i in range(5)
        }

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‡¦ç†ï¼ˆé€æ¬¡å‡¦ç†ï¼‰
        start_time = time.time()

        comparator_sequential = create_enhanced_model_comparator(use_parallel=False)
        sequential_results = comparator_sequential.compare_models_enhanced(
            models_config, X_train, X_test, y_train, y_test
        )

        sequential_time = time.time() - start_time

        # æœ€é©åŒ–å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
        start_time = time.time()

        comparator_parallel = create_enhanced_model_comparator(use_parallel=True)
        parallel_results = comparator_parallel.compare_models_enhanced(
            models_config, X_train, X_test, y_train, y_test
        )

        parallel_time = time.time() - start_time

        return {
            "sequential_time": sequential_time,
            "parallel_time": parallel_time,
            "speedup": sequential_time / parallel_time,
            "success": True,
        }

    def _generate_comprehensive_report(
        self, test_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = {
            "test_summary": {
                "total_tests": len(test_results),
                "successful_tests": sum(
                    1
                    for result in test_results.values()
                    if result.get("success", False)
                ),
                "failed_tests": sum(
                    1
                    for result in test_results.values()
                    if not result.get("success", False)
                ),
            },
            "performance_improvements": {},
            "recommendations": [],
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®è¨ˆç®—
        for test_name, result in test_results.items():
            if result.get("success", False):
                if "time_improvement" in result:
                    report["performance_improvements"][
                        f"{test_name}_time_improvement"
                    ] = result["time_improvement"]
                if "memory_improvement" in result:
                    report["performance_improvements"][
                        f"{test_name}_memory_improvement"
                    ] = result["memory_improvement"]
                if "speedup" in result:
                    report["performance_improvements"][f"{test_name}_speedup"] = result[
                        "speedup"
                    ]

        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        if test_results.get("dataframe_processing", {}).get("time_improvement", 0) > 0:
            report["recommendations"].append("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã®æœ€é©åŒ–ãŒåŠ¹æœçš„ã§ã™")

        if test_results.get("model_comparison", {}).get("time_improvement", 0) > 0:
            report["recommendations"].append("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã®æœ€é©åŒ–ãŒåŠ¹æœçš„ã§ã™")

        if test_results.get("memory_optimization", {}).get("memory_improvement", 0) > 0:
            report["recommendations"].append("ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒåŠ¹æœçš„ã§ã™")

        if test_results.get("parallel_processing", {}).get("speedup", 1) > 1:
            report["recommendations"].append("ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨ãŒåŠ¹æœçš„ã§ã™")

        return report

    def log_test_results(self, report: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ­ã‚°å‡ºåŠ›"""
        self.logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆçµæœ:")

        summary = report["test_summary"]
        self.logger.info(f"  ğŸ“ˆ ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}")
        self.logger.info(f"  âœ… æˆåŠŸ: {summary['successful_tests']}")
        self.logger.info(f"  âŒ å¤±æ•—: {summary['failed_tests']}")

        improvements = report["performance_improvements"]
        for metric, value in improvements.items():
            if "time_improvement" in metric:
                self.logger.info(f"  â±ï¸ {metric}: {value:.1f}%æ”¹å–„")
            elif "memory_improvement" in metric:
                self.logger.info(f"  ğŸ’¾ {metric}: {value:.1f}%æ”¹å–„")
            elif "speedup" in metric:
                self.logger.info(f"  ğŸš€ {metric}: {value:.1f}å€é«˜é€ŸåŒ–")

        recommendations = report["recommendations"]
        if recommendations:
            self.logger.info("  ğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in recommendations:
                self.logger.info(f"    - {rec}")


def run_performance_optimization_tests() -> Dict[str, Any]:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    tester = PerformanceOptimizationTester()
    report = tester.run_comprehensive_tests()
    tester.log_test_results(report)
    return report


if __name__ == "__main__":
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    test_report = run_performance_optimization_tests()

    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆå®Œäº†")
    print(
        f"âœ… æˆåŠŸ: {test_report['test_summary']['successful_tests']}/{test_report['test_summary']['total_tests']}"
    )

    if test_report["recommendations"]:
        print("ğŸ’¡ æ¨å¥¨äº‹é …:")
        for rec in test_report["recommendations"]:
            print(f"  - {rec}")
